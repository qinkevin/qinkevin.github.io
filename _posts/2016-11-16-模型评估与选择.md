---
layout: post
title: "模型评估与选择"       # Title of the post
subtitle:  
date:       2016-11-19 12:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 机器学习
---

# 模型评估与选择

## 1.性能度量
- 通常将学习方法对未知数据的预测能力称为**泛化能力**。
- **损失函数**度量模型一次预测的好坏，**风险函数**（也称**期望损失**）度量平均意义下模型预测的好坏。学习的目的就是选择期望损失最小的模型。
- 模型关于训练数据集的平均损失称为**经验风险**或经验损失。**经验风险最小化**认为经验风险最小的模型就是最优模型。
- **结构风险最小化**等价于正则化。**结构风险**在经验风险上加上表示模型复杂度的正则化项。结构风险最小化认为结构风险最小的模型才是最优模型。
- **正则化项**一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大
- **奥卡姆剃刀原理：**在所有可能选择的模型中，能够很好地解释已知数据并且十分简单才是最好的模型
- **生成方法：**由数据学习联合概率分布，然后求出条件概率分布作为预测模型。典型的生成模型有朴素贝叶斯法和隐马尔科夫模型
- **判别方法：**由数据直接学习决策函数或者条件概率分布作为预测模型。典型的判别模型有k近邻法、感知机、决策树、逻辑回归、最大熵模型、支持向量机、提升方法和条件随机场
- **TP:**真正例，将正类预测为正类数
- **TN:**真负例，将负类预测为负类数
- **FP:**假正例，将负类预测为正类数
- **FN:**假负例，将正类预测为负类数
- **confusion matrix:**以上四个组成的矩阵
- **accuracy:**正确率，表示被正确划分的比例，包括正例和反例，当类别均衡时，正确率最有效。
$$accuracy  = \frac{TP+TN}{P+N}$$
- **precision:**查准率，用P表示，表示被检测出来的信息当中正确的的信息中所占的比率$$ precision = \dfrac {TP}{TP+FP}$$
- **recall:**查全率，用R表示，表示所有正确的信息被检测出来的比率$$recall=\dfrac{TP}{TP+FN}$$
>查准率和查全率是一对矛盾的度量。一般来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。常用来解决类别不平衡问题

- **F1:**是基于查准率和查全率的调和平均,有相同的权重$$F1 = \dfrac{2  P R }{P+R}$$
- 提升度指的是分类模型的预测精度相比随机猜测模型的提升幅度

## 2.评估方法

- hold-out:留出法，直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T

> 训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外的偏差而对最终结果产生影响

- Random subsampling:单次使用留出法得到的估计结果往往不够稳定可靠，一般要采用若干次随机划分、重复进行试验评估后取平均值作为留出法的评估结果
- Cross-Validation:交叉验证法，也称k-fold cross-validation.将数据集划分为k个大小相似的互斥子集，每个子集要尽可能保持数据分布的一致性，所以一般通过分层采样得到.每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，因此可以做k次训练和测试，然后取均值。通常k取10。
- leave-one-out:留一法，为交叉验证法的特例，k取样本数
- Boottrap:自助法，以自助取样法（又称有放回抽样）为基础。给定包含m个样本的数据集D，有放回抽样m次，将形成的数据集作为训练集，未抽中的作为测试集。显然D中有一部分样本会在训练集中多次出现，而另一部分样本不出现。

>自助法在数据集较小、难以有效划分训练/测试集时很有用；此外，自助法能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处。然而，自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差。因此在初始数据量足够时，留出法和交叉验证法更常用一些

- 调参与最终模型:调参时，常常对每个参数选定一个范围和变化步长。在研究对比不同算法的泛化性能时，我们用测试集上的判别效果来估计模型在实际使用时的泛化能力，而把训练数据另外划分为训练集和验证集，基于验证集上的性能来进行模型选择和调参
>当比较不同算法的泛化性能时，需要用假设检验检测统计显著性，因为差别可能是因为误差导致的
- costs and benefits:之前的度量都隐含假设了均等代价。但FP和FN的代价可能是不同的，非均等代价下，要最小化总体代价，而非简单地最小化错误次数。
- TPR : TPR is the proportion of positive tuples that are correctly labeled by the model$$TPR = \dfrac{TP}{TP+FN}$$
- FPR : FPR is the proportion of negative tuples that are mislabeled as positive. $$ FPR = \dfrac{FP}{TN+FP} $$
- ROC: 根据学习器的预测结果对样例进行排序，按此顺序逐个把样本作为正例进行预测，每次计算TPR和FPR，以TPR为纵轴，FPR为横轴

>An ROC curve for a given model shows the trade-off between the true positive rate(TPR)  and the false positive rate(FPR).

- AUC: AUC为ROC曲线下的面积，面积越大模型越准确
- feature scaling: 特征缩放，Feature scaling is a method used to standardize the range of independent variables or features of data.也称normalization
- 偏差与方差：泛化误差就是所学习到的模型的期望风险。泛化误差可以分解为**偏差**、**方差**与**噪声**之和。偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力；方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响；噪声则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度
- 偏差-方差窘境：给定学习任务，假定我们能控制学习算法的训练程度，则在训练不足时，学习器的拟合能力不够强，训练数据的扰动不足以使学习器发生显著变化，此时偏差主导了泛化错误率（高偏差代表欠拟合）；随着训练程度的加深，学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能被学习器学到，方差逐渐主导了泛化错误率；在训练程度充足后，学习器的拟合能力已非常强，训练数据发生的轻微扰动都会导致学习器发生显著变化，若训练数据自身的、非全局的特性被学习器学到了，则将发生过拟合（高方差代表过拟合）。
- 类别不平衡问题: 是指分类任务中不同类别的训练样例数目差别很大的情况.undersampling：去除一些反例使得正反例数目接近，然后再进行学习；oversampling:即增加一些正例使得正反例数目接近，然后再进行学习；threshold-moving:直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，选择移动阈值来分类

## 3.参考文献
- [机器学习](https://book.douban.com/subject/26708119/)
- [Data Mining](https://book.douban.com/subject/6533777/)
- [统计学习方法](https://book.douban.com/subject/10590856/)
