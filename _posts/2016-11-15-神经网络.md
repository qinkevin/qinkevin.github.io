---
layout: post
title: "神经网络"       # Title of the post
subtitle:  
date:       2016-11-19 12:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 机器学习
---

# 神经网络

## 1.神经元模型
- 在M-P神经元模型中，神经元接收到来自n个其他神经元传递来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将与神经元的阀值进行比较，然后通过“激活函数”处理以产生神经元的输出。激活函数通常使用sigmod函数

## 2.感知机

>感知机是神经网络和支持向量机的基础

- 感知机是二类分类的线性分类模型，由两层神经元组成。输入层接收外界输入信号后传递给输出层，输出层是M-P神经元。
- 感知机模型的假设空间是定义在特征空间中的所有线性分类器。感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此，导入基于误分类的损失函数，利用梯度下降法对损失函数极小化。损失函数对应于误分类点到分类超平面的总距离
- 感知机学习算法是误分类驱动的，具体采用随机梯度下降。首先，任意选取一个超平面w0,b0,然后用梯度下降法不断地极小化目标函数。极小化过程中不是一次使M中所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。所以当采用不同的初值或选取不同的误分类点，解可以不同
- 感知机学习算法的直观解释是：当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整w,b的值，使分类超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类

## 3.多层前馈神经网络
多层前馈神经网络：输入层神经元接收外界输入，隐层与输出层神经元对信号进行加工，最终结果由输出层神经元输出；换言之，输入层神经元仅是接受输入，不进行函数处理，隐层与输出层包含功能神经元。
- 神经网络的学习过程，就是根据训练数据来调整神经元之间的“连接权”以及每个功能神经元的阀值。换言之，神经网络“学”到的东西，蕴含在连接权与阈值中

>Multilayer feed-forward networks, given enough hidden units and enough training samples, can closely approximate any function。然而，如何设置隐层神经元的个数仍是个未决问题，实际应用中通常靠“试错法”

## 4.BP算法
- BP算法基于梯度下降策略，以目标的负梯度方向对参数进行调整，学习率一般设置为0.1，太大容易震荡，太小则收敛速度又会过慢
- 对每个训练样例，BP算法执行以下操作：先将输入示例提供给输入层神经元，然后逐层将信号前传，直到产生输出层的结果，然后计算输出层的误差，再将误差逆向传播至隐层神经元，最后根据隐层神经元的误差来对连接权和阈值进行调整。该迭代过程循环进行，直到达到某些停止条件为止
- BP算法的目标是要最小化训练集D上的累积误差。
- 一般来说，标准BP算法每次更新只针对单个样例，参数更新得非常频繁，而且对不同样例进行更新得效果可能出现“抵消”现象。因此，为了达到同样的累积误差极小点，标准BP算法往往需进行更多次数的迭代。累积BP算法直接针对累积误差最小化，它在读取整个训练集D一遍后才对参数进行更新，其参数更新的频率低得多。但在很多任务中，累积误差下降到一定程度之后，进一步下降会非常缓慢，这时标准BP往往会更快获得较好的解，尤其是在训练集D非常大时更明显
- BP神经网络经常遭遇过拟合，其训练误差持续降低，但测试误差却可能上升。一种是早停(early stopping)，一种是正则化。

## 5.全局最小与局部极小
基于梯度的搜索是使用最为广泛的参数寻优方法。在此类方法中，我们从某些初始解出发，迭代寻找最优参数值。每次迭代中，我们先计算误差函数在当前点的梯度，然后根据梯度确定搜索方向。例如，由于负梯度方向是函数值下降最快的方向，因此梯度下降法就是沿着负梯度方向搜索最优解。若误差函数在当前点的梯度为零，则已达到局部极小，更新量将为零，这意味着参数的迭代更新将在此停止。显然，如果误差函数仅有一个局部极小，那么此时找到的局部极小就是全局最小；然后，如果误差函数具有多个局部极小，则不能保证找到的解是全局最小
以下策略经常用来“跳出”局部极小：
- 以多组不同参数值初始化多个神经网络，按标准方法训练后，取其中误差最小的解作为最终参数，这相当于从多个不同的初始点开始搜索，这样就可能陷入不同的局部极小，从中进行选择有可能获得更接近全局最小的结果
- 使用“模拟退火”技术。模拟退火在每一步都以一定的概率接受比当前解更差的结果，从而有助于“跳出”局部极小。在每步迭代过程中，接受“次优解”的概率要随着时间的推移而逐渐降低，从而保证算法稳定
- 使用随机梯度下降。与标准梯度下降法精确计算梯度不同，随机梯度下降法在计算梯度时加入了随机因素。于是，即便陷入局部极小点，它计算出的梯度仍可能不为零，这样就有机会跳出局部极小继续搜索

## 6.深度学习
通过多层处理，逐渐将初始的“低层”特征表示转化为“高层”特征表示后，用“简单模型”即可完成复杂的分类等学习任务。即通过机器学习技术自身来产生好特征

## 7.参考文献
- [机器学习](https://book.douban.com/subject/26708119/)
- [Data Mining](https://book.douban.com/subject/6533777/)
- [统计学习方法](https://book.douban.com/subject/10590856/)
