---
layout: post
title: "我是高频交易工程师"       # Title of the post
subtitle:
date:       2016-11-19 12:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 读书笔记
---

# 我是高频交易工程师：知乎董可人自选集

- 偏执的科技至上理念，极其复杂的技术难题，疯狂追求机器极致的性能，以及高强度的同业竞争
- 至于赚钱，建议你把它当作承担压力、努力工作之后的回报，而不要幻想无意间被葵花宝典砸到学得惊世神功，从此纵横股市点石成金；这种神话至少在这个行业是不存在的
- 被动做市（Passive Market Making）。这种策略是在交易所挂限价单进行双边交易以提供流动性。所谓双边交易，是指做市商手中持有一定存货，然后同时进行买和卖两方交易。这种策略的收入包括买卖价差（spread）和交易所提供
- 套利（Arbitrage）。这个策略应该是大家比较熟悉的，就是看两种高相关性的产品之间的价差
- 简单说就是利用技术手段，比如高速连接和下单，来探测其他较慢的市场参与者的交易意图并且抢在他们之前进行交易，将利润建立在他人的损失上。
- Structural，简单说就是利用技术手段，比如高速连接和下单，来探测其他较慢的市场参与者的交易意图并且抢在他们之前进行交易，将利润建立在他人的损失上。
- Directional，就是预测一定时间内的价格走势，顺势而为
- 做市商策略是事实上最好的业务模式，具备可持续发展的性质。做市商在市场上充当流动性提供者，通俗地说，就是有任何人想买一个东西（比如股票、期货），你要保证能卖给他；有任何人想卖一个东西，你要保证从他那买。「保证」的意思就是如果市场上没有别人出头，做市商就必须出来——隐含的意思就是，做市商是所有人的对手盘。
- 做市商的策略，本质上是认为市场价格在短期内具有波动性，涨上去的价格会落下来，反之亦然。所以他可以选择承担一定的风险，暂时从你手里把东西买过来，过一段时间价格变得有利时再卖掉。注意这里的风险是真实存在的，没有什么保证价格一定会向着做市商有利的方向变化。时间跨度越大，这种风险也越大。做市商承担了这种风险，买过来的东西需要持有一定时间作为库存，来赚取因为波动性而产生的一点点价差（通常是一分、两分）。也有更稳妥一些的做法，是通过其他高相关性的产品做对冲，比如买进一只股票的同时卖出它的期货，这个模型更复杂一些，对算法和性能的要求也更高
- 这种生意的本质决定了必须要能大量买卖，才能积少成多，形成效益。
- 它会付钱给做市商，可能还减免手续费什么的给点小福利。这样一来，这种做市商即使生意做得不好赔点钱，可算上交易所的报酬，也还是能盈利。这样的人，我们叫做contractual market maker。就是说，他会和交易所签订一个合同，承诺提供多少流动性；交易所也相应地给一些报酬和福利。
- 就是如何做到不赔钱？一个是按照上面说过的，做好对冲；另一个就是发现形势不利的时候要能及时撤单——这个是最考验低延迟的地方，速度慢就会发生来不及撤单而遭受损失的情况。
- 所以，做市这种行为的特点和市场上的需求，决定了HFT是最适合做这件事的。这也是为什么我们经常说HFT给市场提供流动性。
- 另外一种套利策略则是指，找到两种强相关性的证券，通过消除两者间不合理的价差来盈利。一个极端的例子是，ETF（Exchange Traded Funds）和组成ETF的那些股票。如果你知道ETF的计算方式，就可以用同样的方式通过那些股票的价格来计算一个ETF的期望价格。有的时候，由于种种原因，你发现这个价格和你在市场上看到的ETF价格不一样，你就知道显然是市场发生了一些混乱，早晚这个价格会变回来。这时你就可以买入（卖出）ETF，卖出（买入）那些股票，坐等价格回归，可以稳赚不赔。
- 这个策略听起来很美，可实际上竞争非常激烈，因为任何人都可以做这件事。参与的人多了，市场就会少犯错误，同时每个人的利润空间也变小了。当你的套利收入不足以支撑HFT的研发维护成本的时候，离关门也就不远了。
- HFT的竞争对手一定是另一个HFT。HFT的速度优势是指，当交易所完成一笔交易，在通知所有交易者的时候，HFT因为在通信线路的上游，所以会比别人先看到这条交易确认信息。这个时候他可能会根据自己对这条信息的理解以及业务需求，增加或者撤掉自己的一些单子，但这些都是完全正常的交易操作，不存在任何恶意。并且由于这条信息是发送给所有人的，任何人都会对此作出解读和反应。但只有那些在同一个时间粒度上工作的交易者之间才可能存在竞争（之后再解释这个问题）。而对于最初这条交易的发起者来说，由于交易已经完成，所以不存在任何被攻击的可能。
- 高频交易的第一推动力——追逐流动性的交易所们
- 高频交易者依靠技术手段和高速连接，利用不同交易所同Tape上价格发布的时间差来探测其他低速交易者的交易意图，抢在他们之前进行交易来盈利。
- 高频做市策略的动机和驱动力：在市场碎片化的大背景下，各家交易所为提高自身竞争力从而吸引更多交易量，必须聘请高水平做市商入驻，帮助改进流动性质量。
- 高频做市商所做交易的行为本质：这种交易只求参与其中，作为买卖双方的中转即可，并不需要追求买卖价差的绝对数值。极端情况下，参与程度（Participation Rate）本身即为所出售的产品，而具体的某笔交易并不必以盈利为目的。
- 这也是HFT系统和互联网系统最大的区别所在：HFT系统的精髓在于把单机的软硬件系统的性能发挥到极致，而不是像互联网那样强调高负载和延展性，动辄用几千台机器搭集群的做法在这里是不适用的
- 数据要分两种，一种是从交易所发过来的市场数据，流量很大；另一种是系统向交易所发出的交易指令，相比前者流量很小，这两种数据需要在不同的TCP/IP连接里传输。
- 因为是自动化交易系统，人工干预的部分肯定比较小，所以图形界面不是重点。而为了性能考虑，图形界面需要和后台分开部署在不同的机器上，通过网络交互，以免任何图形界面上的问题导致后台系统故障或者被抢占资源。这样又要在后台增加新的TCP/IP连接。
- 简而言之，就是用修改函数指针的方式把系统调用入口换成自己的程序，比如用共享内存来实现端口通讯的API。
- 常见的时间序列（Time Series）方法，一般要求数据的时间点是等间距分布的，但是交易数据有时并不能满足这个条件，我们可能会发现在一段时间内出现频繁交易，在另一段时间内市场又趋于平静。针对这个问题，对交易数据建模就需要进行一些特别的处理，这里提供两种思路。
第一种是对数据进行Down Sample，原始的高频数据虽然是非等间距的，你可以自己定义一个时间间隔重新抽样。这个时候需要注意的，是在抽样的时候需要仔细设计方法来计算抽样点的各项数值（价格、成交量等等）。简单的做法是用原始数据fit出一条曲线，然后按照新的时间点取曲线上对应的值，或者算抽样时间间隔内数据点的加权平均，权值可以是数据点之间的时间差，抑或成交量之类的。这种方法做完之后就可以用常用的时序模型的技术了。
第二种就是所谓的Point Process，直接对时间点进行建模（而非像时间序列模型是对抽样点的数据进行建模）。最简单的Point Process是一般概率统计课上都会教的Poisson Process，它的特点是每次事件发生都是独立的，时间差复合指数分布。其他答案中提到的ACD是一种升级版，对事件间的时间差进行建模，刻画了事件之间会彼此影响的情况。这个模型技术上跟ARMA是一套，只不过把时间序列换成了事件序列的时间差而已。
- 另外有一种Point Process族是对单位时间内事件发生的概率密度进行建模的。学术界研究比较多的一种叫Hawkes Process，以前主要用来对地震的余震进行建模，后来大家觉得地震其实跟金融交易里的震荡也挺像，有一批人就尝试过往上套。
- Hawkes Process的思路是说历史上发生的事情对未来的概率密度函数有影响，只是随着时间流逝，这种影响会逐渐减弱（Decay）。技术上就是做一个Kernel，所有的历史事件都输入这个Kernel来计算对未来的影响，Kernel本身做成Decay的形式就可以模拟衰减了。
- Point Process还有一个高级点的变种叫做Masked Point Process，思路是把每个事件的相关数据作为标记（Mask）附加到时间点上，这样一来，模型在对时间点建模的同时也兼顾了数据。而且这种标记可以是多个，对交易数据来说，价格、成交量、波动性等都可以作为标记。这样可以极大地扩充模型的描述能力。
- 这两种模型各有特点，但也存在一个共有的问题，就是其底层技术设计可以非常漂亮，各种数学性质都很完备，但是你把它往数据上一套，就会发现问题重重，很多时候连基本的统计检验都通不过。其中一个原因，是这些模型本身的思路出发点非常好，可问题是数据质量很难保证，要是你拿到的是不知经过了几层处理的N手数据，就更是很难说清问题究竟出在哪（具体例子点击查看）。要知道高精度的Timestamped交易数据是相当难求的，但如果时间都不准的话又怎么上模型呢？
- 虽然真正的Order Book只存在于交易所内部，所有交易都在交易所内完成，但是交易所会把每笔报价和市价单都转发给所有人，因此所有的买家和卖家都可以自己维护一个同样的数据结构，相当于交易所Order Book的镜像。通过跟踪分析自己手里这份的镜像变化，来制定交易策略，是高频交易算法的核心思想。
- 为了解决这个问题，交易所提供了一种针对性的工具，就是所谓的「冰山订单」（Iceberg Order）。这种订单可以很大，但只有一小部分是公开的，大部分则隐藏起来，除了交易所和发单者本人谁也看不到，真的像一座「冰山」一样
- 需要强调的是，使用冰山订单并不是没有代价的，因为你隐藏了真实的需求，在屏蔽掉潜在的攻击者的同时，也屏蔽掉了真正的交易者！而且会使得成交时间显著增加－－因为没人知道你想买／卖这么多，你只能慢慢等待对手盘的出现。所以当有人下决定发出冰山订单的时候，也会有对市场情况的考虑，只有等到合适的时机才会做这种选择。
- 因为使用的信息太模糊了，而且说到底建模的对象只是一种相关性，没有什么能保证冰山订单的发送者一定是按照这个逻辑出牌的。
- 高频世界里，有一条永恒的建模准则值得铭记：先看数据再建模。如果你看了上面的介绍就开始天马行空地思考数学模型，那基本上是死路一条。我见过很多年轻人，特别有热情，一上来就开始做数学定义，然后推导偏微分方程，数学公式写满一摞纸，最后一接触数据才发现模型根本行不通。这是非常遗憾的。
- 由于有「冰山订单」这样一个从经济学基本的供需关系出发的真实需求，通过分析实际数据找到一丝线索，最后通过数学模型刻画出定量的策略，这才是漂亮的策略研发。
- 首先，CSV格式是一个很差的存储格式。CSV按行存储，每读入一行都要按照分隔符进行拆分。
- 数据全部以文本形式存储，意味着每读入一个浮点数（比如1.234）就要进行字符串到数字的转换。这些都是完全不必要的，会大大增加任务的处理时间。
- 其次，有人建议把数据导入到SQL数据库，这也是个坏主意。SQL数据库的确可以提供一些便利的查询和索引，但是每次访问数据库都需要进行进程间通信。如果你需要逐条访问，这个通信时间也会非常漫长，远远超出真正计算所需要的时间。
- 简言之，以上两种方案里你的大部分时间都浪费在无意义的I/O上，计算机的使用效率非常低。下面给出几种方案建议，可以根据个人情况选择：a. 使用内存数据库（如KDB），百万条数据导入内存数据库也不过100MB左右的内存占用，毫无压力。在此基础上使用系统支持的语言（比如KDB中的Q）进行算法开发。这是Quant界比较常见的一种方案，应该有成熟社区，容易找到帮助。如果你所在的机构正好有这方面的技术积累，就更方便了。但是缺点在于，数学计算比较受限于系统，如果你用到一些系统没提供的算法，开发成本比较大。KDB的语言本身需要陡峭的学习曲线，甚至很多方面可能非常晦涩，比如一个！就可以根据上下文不同表示多种不同的操作符，从取模到生成列表，对于新手来说入门可能有些困难。
- Python神器：Python是非常适合做Quant类工作的语言，本身就是科学计算方面的统治级语言，现在加入了IPython，Pandas等重量级神器，为Quant类工作量身定做，而且仍在飞速发展中，以后会越来越重要。IPython提供一整套的类似Matlab的计算环境，包含了丰富的数学函数和画图工具，示例可参见点击查看。Pandas则是专门针对金融类计算设计开发的计算库（Python Data Analysis Library），底层使用HDF5格式（一种高效的数据存储格式，对于金融时间序列来说远远优于CSV），上层提供很多强大的内建函数（统计类的mean，stddev，类SQL的group by，order by，等等），而且本身工作在Python环境里，可以直接使用Python做编程方面的工作。更值得一提的是HDF5也是Matlab支持的存储格式，所以有必要的时候可以无缝迁移到Matlab（如果正好有某个算法是Matlab提供而Python没有的）。需要强调的是Python在科学计算方面有巨大的优势，是统治级的语言，像并行计算，GPU之类都有很好的支持，不用担心要自己搭框架。
- 关于其他语言，我自己很喜欢一个比较小众的组合：Mathematica+Java/Scala。Mathematica的优点在于，它本身提供函数式的编程语言，表达能力非常强大。比如Map/Reduce是标配，很多时候不需要去做烦人的for循环或下标控制，排版经常可以直接照数学公式原样输入，即直观又不容易写错；代码和输出混排的排版方式使得建模时的演算和推理过程非常流畅，甚至还可以直接生成动画，对于找直观理解非常有帮助（这几点分别被IPython和R偷师了一部分）。Mathematica的缺点在于对金融类的时间序列数据没有很好的内建支持，使得存储和计算都会比较低效，因此需要用内嵌Java的方式来补足，对于数据格式或性能敏感的操作都可以用Java/Scala实现。在我心目中，这个组合无出其右，不论是快速建模还是建模转生产，都远远领先于其他选择。但Mathematica的商用授权很贵，如果公司本身不认可的话很难得到支持，这是最致命的缺陷。另外随着Python系的逐渐成熟，领先优势在逐渐缩小，长远来看Python的势头更好一些。
- 所以如果你是一个有着数理金融等背景的新人打算开始Quant生涯，在决定是否要投资到这项重量级技术上时需要慎重，即便它目前的市场定价可能仍在峰值。相比之下，我认为Python会是更理想的选择，既能很好地完成建模工作，也可以训练一定的编程技巧，使你在必要时也能胜任一些简单的C++工作。
- 对于一个Quant来说，当他受制于人的那一刻，就已经注定了是一场悲剧。
- 事实上，很多时候，秘密就隐藏在这些最原始的数据里。
- 《Introduction to Algorithms》和《Computer Systems - A Programmer's Perspective
- Q是指风险中性测度。风险中性的意思主要是说历史数据不能帮助你预测未来的走势，所以你的决策是没有风险补偿的。这当然是一个非常虚幻的假设，但是由此而得的模型可以给出漂亮的数学性质，而且可以在缺乏数据的情况下得到一些结论，所以有一定的实际意义。涉及的数学技术主要是随机过程、偏微分方程之类。在数学派系里，这些显得相对高端，一般人概念里都是那些思维不同于常人的人捣鼓的玩意。P是指真实概率测度。所谓真实，主要是说模型依赖的概率分布是从历史数据上估算出来的。严格来讲，我个人不认为这种东西叫做「真实」，最多只能说是从真实数据上估算出来的，显然没有什么东西保证历史一定会重演（比如黑天鹅）。但是这个是目前大家公认的说法，所以咱们不较真。从定义可以看出这套方法主要依赖数据，数据量越大估算的效果越好。涉及的技术主要是时间序列（ARIMA，GARCH之类），Bayesian，以及现在流行的机器学习等方法。不难看出，为了倒腾数据，这套方法练到上层就要开始刷装备。在电子化时代这最终演化为拼机房的「军备竞赛」。两者对比可以看出：Q重模型而轻数据，P则重数据而轻模型。当然两者也都要即有模型也有数据，但从应用上来讲，Q者是模型固定，用数据来精化模型的参数（calibration）；P者则可以有若干备选模型，由数据的计算结果来选择最佳的模型（estimation）。
- 在新的时代，程序代码已经开始成为继数学之后的新生一代科学普适性语言。
- 入行以后你会发现，和所有其他行业一样，大部分时间里你的工作内容可能都是琐碎和枯燥的，包括一些拼体力的编程，调试，清洗数据，分析日志。而实际上外界所传的Quant种种美好的数学建模和理论分析，能够在其中占据一成就已经很不错。所以，当你下决心要成为一名职业Quant时，首先要做好心理准备，经得起这些枯燥和琐碎，才能最终品尝那一点点甘醴。
- 团队里最受欢迎的是一种T字型人才。即对自己擅长的领域有深入的理解，同时对其他相关的领域也保持好奇心和基本的了解。这样的人更有可能作为角色成员，有机地成为团队的一部分。唯有如此，别人才愿与你同舟共济。
