---
layout: post
title: "集成学习"       # Title of the post
subtitle:   
date:       2016-11-19 12:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 机器学习
---
# 集成学习

- 集成学习通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性能。这对“弱学习器”尤为明显

>弱学习器常指泛化性能略优于随机猜测的学习器

- 要获得好的集成，个体学习器应“好而不同”，即个体学习器要有一定的“准确性”，即学习器不能太坏，并且要有“多样性”，即学习器间具有差异，但两者往往存在冲突。
- 根据个体学习器的生成方式，集成学习方法大致可分为两大类，即个体学习器间存在强依赖关系、必须串行生成的序列化方法，以及个体学习器间不存在强依赖关系、可同时生成的并行化方法；前者的代表是Boosting,后者的代表是Bagging 和 Random Forests

## 1.Boosting and AdaBoost
- Boosting 的工作机制是：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合

> - 和Bagging 不同，Boosting 是基于分类器的表现投票，而不是每个分类器的权重都相同
> - Boosting容易过拟合
> - bagging的有放回抽样是为了保证每次相互独立

## 2.Bagging
- Bagging 基于 bootstrap samling(有放回抽样)采样出T个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合。在对预测输出进行结合时，Bagging 通常对分类任务使用简单投票法，对回归任务使用简单平均法

## 3.Random Forests
- 随机森林是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一颗决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类
- 随机森林对Bagging只做了小改动，但是与Bagging中基学习器的“多样性”仅通过样本扰动（通过对初始训练集采样）而来不同，随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动（从初始属性集中抽取出若干个属性子集，再基于每个属性子集训练一个基学习器），这使得最终集成的泛化性能可通过个体学习器之间差异度的增加而进一步提升。
-随机森林有两个参数，一个是森林中决策树的个数，一个是每棵树上使用的（随机选取的）特征变量个数

>随机森林不需要剪枝，也不会出现过拟合。因为随机森林每次划分只需要考虑更少的属性，所以在大型数据中非常有效
>随机森林是一类专门为决策树分类器设计的组合方法。它组合多棵决策树作出的预测，其中每棵树都是基于随机变量的一个独立集合的值产生的。随机化有助于减少决策树之间的相关性，从而改善组合分类器的泛化误差
>每一个决策树都可以进行“事后剪枝”以避免过拟合，但对于随机森林模型来说，这完全没有必要。随机森林的一大特性就是对过拟合的免疫性：模型本身就吸收了数据中的特质方差，因此即使不做单个决策树的“事后剪枝”，随机森林模型也不会过拟合

## 4.Bagging, boosting and stacking in machine learning

All three are so-called "meta-algorithms": approaches to combine several machine learning techniques into one predictive model in order to decrease the variance (bagging), bias (boosting) or improving the predictive force (stacking alias ensemble)

Every algorithm consists of two steps:
- Producing a distribution of simple ML models on subsets of the original data.
- Combining the distribution into one "aggregated" model.

Here is a short description of all three methods:
- Bagging (stands for Bootstrap Aggregation) is the way decrease the variance of your prediction by generating additional data for training from your original dataset using combinations with repetitions to produce multisets of the same cardinality/size as your original data. By increasing the size of your training set you can't improve the model predictive force, but just decrease the variance, narrowly tuning the prediction to expected outcome.
- Boosting is a two-step approach, where one first uses subsets of the original data to produce a series of averagely performing models and then "boosts" their performance by combining them together using a particular cost function (=majority vote). Unlike bagging, in the classical boosting the subset creation is not random and depends upon the performance of the previous models: every new subsets contains the elements that were (likely to be) misclassified by previous models.
- Stacking is a similar to boosting: you also apply several models to your original data. The difference here is, however, that you don't have just an empirical formula for your weight function, rather you introduce a meta-level and use another model/approach to estimate the input together with outputs of every model to estimate the weights or, in other words, to determine what models perform well and what badly given these input data.

Here is a comparison table:

![comparison table](img/RFfqb.png)

## 5.参考文献
- [机器学习](https://book.douban.com/subject/26708119/)
- [Data Mining](https://book.douban.com/subject/6533777/)
- [统计学习方法](https://book.douban.com/subject/10590856/)
