---
layout: post
title: "聚类"       # Title of the post
subtitle:  
date:       2016-11-19 12:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 机器学习
---

# 聚类

>聚类过程仅能自动形成簇结构，簇所对应的概念语义需由使用者来把握和命名


## 1.原型聚类

### 1.1 k-means
First, it randomly selects k of the objects in D, each of which initially represents a cluster mean or center. For each of the remaining objects, an object is assigned to the cluster to which it is the most similar, based on the Euclidean distance between the object and the cluster mean. The k-means algorithm then iteratively improves the within-cluster variation. For each cluster, it computes the new mean using the objects assigned to the cluster in the previous iteration. All the objects are then reassigned using the updated means as the new cluster centers. The iterations continue until the assignment is stable, that is, the clusters formed in the current round are the same as those formed in the previous round

> - 对于名义变量，可以使用众数代替平均值
> - 由于使用平均值，所以易受到噪声和异常值的影响
> - 可能会到达局部最优，所以可以多次初始化尝试
> - 做聚类不要随便标准化，因为可能量纲是很重要的
> - k-means最大的缺点在于可解释性，但是逻辑简单，运行速度快

### 1.2 k-medoids
由于k-means算法中心点的选取易收到异常值的影响，所以k-medoids限制中心点必须是真实点，而不是计算出来的位置，然后计算其他点与该点的距离最小

### 1.3 k-means 与 k-medoids 比较
The k-medoids method is more robust than k-means in the presence of noise and outliers because a medoid is less influenced by outliers or other extreme values than a mean. However, the complexity of each iteration in the k-medoids algorithm is O(k(n − k)2). For large values of n and k, such computation becomes very costly, and much more costly than the k-means method. Both methods require the user to specify k, the number of clusters.

## 2.层次聚类
层次聚类试图在不同层次对数据集进行划分，从而形成树形的聚类结构

### 2.1 Agglomerative Hierarchical Clustering
是一种自底向上聚合策略的层次聚类算法。它先将数据集中的每个样本看做一个初始聚类簇，然后在算法运行的每一步中找出距离最近的两个聚类簇进行合并，该过程不断重复，直至达到预设的聚类簇个数

>也可以自顶向下分拆，但是由于分拆的复杂度太高，一般用自底向上聚合策略

### 2.2BIRCH
BIRCH是一种聚类算法，它最大的特点是能利用有限的内存资源完成对大数据集的高质量的聚类，同时通过单遍扫描数据集最小化I/O代价
具体可参考[博客](http://blog.csdn.net/qll125596718/article/details/6895291)

## 3.密度聚类
此类算法假定聚类结构能通过样本分布的紧密程度确定。通常情形下，密度聚类算法从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇以获得最终的聚类结果

### 3.1DBSCAN
Initially, all objects in a given data set D are marked as “unvisited.” DBSCAN randomly selects an unvisited object p, marks p as “visited,” and checks whether the ε-neighborhood of p contains at least MinPts objects. If not, p is marked as a noise point. Otherwise, a new cluster C is created for p, and all the objects in the ε-neighborhood of p are added to a candidate set, N. DBSCAN iteratively adds to C those objects in N that do not belong to any cluster. In this process, for an object p′ in N that carries the label “unvisited,” DBSCAN marks it as “visited” and checks its ε-neighborhood. If the ε-neighborhood of p′ has at least MinPts objects, those objects in the ε-neighborhood of p′ are added to N. DBSCAN continues adding objects to C until C can no longer be expanded, that is, N is empty. At this time, cluster C is completed, and thus is output.
To find the next cluster, DBSCAN randomly selects an unvisited object from the remaining ones. The clustering process continues until all objects are visited

## 4.性能度量
希望聚类结果的“簇内相似度”高且“簇间相似度”低
聚类性能度量大致有两类。一类是将聚类结果与某个“参考模型”进行比较，称为“外部指标”；另一类是直接考察聚类结果而不利用任何参考模型，称为“内部指标”

## 5.参考文献
- [机器学习](https://book.douban.com/subject/26708119/)
- [Data Mining](https://book.douban.com/subject/6533777/)
