---
layout: post
title: "决策树"       # Title of the post
subtitle:  
date:       2016-11-19 12:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 机器学习
---
---
# 决策树

## 1.基本流程
>在决策树基本算法中，有三种情形会导致递归返回：（1）当前结点包含的样本全属于同一类别，无需划分；（2）当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；（3）当前结点包含的样本集合为空，不能划分

**C3、C4.5 、CART都是基于内存的决策树算法，适用于小数据**

## 2.划分选择
>即如何选择最优划分属性。划分的目的是希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”(purity)越来越高

### 2.1信息增益
 - 使用信息熵来度量样本集合纯度。熵越大，随机变量的不确定性就越大
 - 信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度
 - 一般而言，信息增益越大，则意味着使用该属性来进行划分所获得的“纯度提升”越大
 - ID3决策树学习算法使用信息增益为准则来选择划分属性
 - **信息增益准则对可取值数目较多的属性有所偏好**

### 2.2增益率
- C4.5使用增益率来选择最优划分属性
- 增益率对可取值数目较少的属性有所偏好，因此，C4.5并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的

### 2.3基尼指数
- CART使用基尼指数来划分最优划分属性
- 使用基尼值来度量数据集的纯度
- **基尼指数只能用于二分划分**
- 选择基尼指数最小的属性作为最优划分属性

### 2.4连续值处理
- 上述主要讨论基于离散属性生成决策树，当遇到连续属性时需要使用连续属性离散化技术。最简单的策略就是采用二分法对连续属性进行处理，把中位点作为候选划分点
>需要注意的是，与离散属性不同，若当前节点划分属性为连续属性，该属性还可作为其后代节点的划分属性

### 2.5多变量决策树
- 多变量决策树的非叶节点不再是仅对某个属性，而是对**属性的线性组合**进行测试
>决策树的测试条件每次可以涉及多个属性，斜决策树可以使用多个属性。构造归纳则通过由原始属性创建复合特征，简化了学习复杂的划分函数的任务

## 3.剪枝处理

### 3.1预剪枝
>预剪枝是指在决策树生成过程中，对每个节点在划分前先进行估计，当当前节点的划分不能带来决策树泛化性能提升，则停止划分并将当前节点标记为叶节点

- 预剪枝使得决策树的很多分支都没有“展开”，这不仅降低了过拟合的风险，还显著减少了决策树的训练时间开销和测试时间开销。
- 但另一方面，有些分支的当前划分虽不能提升泛化性能、甚至可能导致泛化性能暂时下降，但在其基础上进行的后续划分却有可能导致性能显著提高
- 预剪枝基于“贪心”本质禁止这些分支展开，给预剪枝决策树带来了**欠拟合**的风险

### 3.2后剪枝
>后剪枝则是先从训练集生成一颗完整的决策树，然后自底向上地对非叶节点进行考察，若将该节点对应的子树替换为叶节点能带来决策树泛化性能提升，则将该子树替换为叶节点

- 一般情形下，后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树
- 但后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中的所有非叶结点进行逐一考察，因此**其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多**

## 4.优缺点
>其主要优点是模型具有可读性，分类速度快

## 5.参考文献
- [机器学习](https://book.douban.com/subject/26708119/)
- [Data Mining](https://book.douban.com/subject/6533777/)
- [统计学习方法](https://book.douban.com/subject/10590856/)
