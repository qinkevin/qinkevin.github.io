---
layout: post
title: "深度学习"       # Title of the post
subtitle:
date:       2017-05-29 12:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 机器学习
---

# 深度学习
- When training neural networks, it is common to use "weight decay," where after each update, the weights are multiplied by a factor slightly less than 1. This prevents the weights from growing too large, and can be seen as gradient descent on a quadratic regularization term
- At the beginning, we are far from the destination, so we use larger learning rate；After several epochs, we are close to the destination, so we reduce the learning rate
- Why CNN for Image：Some patterns are much smaller than the whole image；The same patterns appear in different regions；Subsampling the pixels will not change the object
- 在RNN中，The same network is used again and again.
- Difficulties of Reinforcement Learning：It may be better to sacrifice immediate reward to gain more long-term reward；Agent’s actions affect the subsequent data it receives
- The quintessential example of a representation learning algorithm is the au-toencoder. An autoencoder is the combination of an encoder function that convertsthe input data into a different representation, and a decoder function that convertsthe new representation back into the original format. Autoencoders are trained topreserve as much information as possible when an input is run through the encoderand then the decoder, but are also trained to make the new representation havevarious nice properties.
- In some cases we will need an array with more than two axes. Inthe general case, an array of numbers arranged on a regular grid with avariable number of axes is known as a tensor.
- The L1 norm is commonly used in machine learning when the difference betweenzero and nonzero elements is very important
- One other norm that commonly arises in machine learning is the L∞ norm,also known as the max norm. This norm simplifies to the absolute value of theelement with the largest magnitude in the vector
- Diagonal matrices are of interest in part because multiplying by a diagonal matrixis very computationally efficient.
- In many cases, it is more practical to use a simple but uncertain rule ratherthan a complex but certain one, even if the true rule is deterministic and ourmodeling system has the fidelity to accommodate a complex rule.
- Independenceis a stronger requirement than zero covariance, because independence also excludesnonlinear relationships.
- In the absenceof prior knowledge about what form a distribution over the real numbers should take, the normal distribution is a good default choice for two major reasons
- A mixture distribution is made up of severalcomponent distributions. On each trial, the choice of which component distributiongenerates the sample is determined by sampling a component identity from amultinoulli distribution
- A very powerful and common type of mixture model is the Gaussian mixturemodel, in which the components p(x | c = i) are Gaussians. Each component hasa separately parametrized mean μ(i) and covariance Σ(i).
- The basic intuition behind information theory is that learning that an unlikelyevent has occurred is more informative than learning that a likely event hasoccurred
- If we have two separate probability distributions P (x) and Q(x) over the samerandom variable x, we can measure how different these two distributions are usingthe Kullback-Leibler (KL) divergence
- Conditioning refers to how rapidly a function changes with respect to small changesin its inputs. Functions that change rapidly when their inputs are perturbed slightlycan be problematic for scientific computation because rounding errors in the inputscan result in large changes in the output
- The matrix containing all such partial derivatives isknown as a Jacobian matrix
- When our function has multiple input dimensions, there are many secondderivatives. These derivatives can be collected together into a matrix called theHessian matrix.
- Because the Hessian matrix is real and symmetric,we can decompose it into a set of real eigenvalues and an orthogonal basis of eigenvectors.When the Hessian has a poor condition number, gradient descent performspoorly. This is because in one direction, the derivative increases rapidly, while inanother direction, it increases slowly. Gradient descent is unaware of this changein the derivative so it does not know that it needs to explore preferentially inthe direction where the derivative remains negative for longer.
- The simplest method for doing so is known as Newton’s method.Newton’s method is based on using a second-order Taylor series expansion toapproximate f(x) near some point x(0)
- Newton’s method is only appropriate when the nearbycritical point is a minimum (all the eigenvalues of the Hessian are positive), whereasgradient descent is not attracted to saddle points unless the gradient points toward them.
- Convex optimization algorithms are applicableonly to convex functions—functions for which the Hessian is positive semidefiniteeverywhere. Such functions are well-behaved because they lack saddle points andall of their local minima are necessarily global minima.
- One way to interpret maximum likelihood estimation is to view it as minimizingthe dissimilarity between the empirical distribution pˆdata defined by the trainingset and the model distribution, with the degree of dissimilarity between the twomeasured by the KL divergence
- Bayesian methods typically generalize much better when limited training datais available, but typically suffer from high computational cost when the number oftraining examples is large
- For feedforward neural networks, it isimportant to initialize all weights to small random values. The biases may beinitialized to zero or to small positive values.
- An advantage of this approach of deriving the cost function from maximumlikelihood is that it removes the burden of designing cost functions for each model.Specifying a model p(y | x) automatically determines a cost function log p(y | x).
- Functions that saturate (become very flat) underminethis objective because they make the gradient become very small. In many casesthis happens because the activation functions used to produce the output of thehidden units or the output units saturate. The negative log-likelihood helps toavoid this problem for many models. Many output units involve an exp functionthat can saturate when its argument is very negative. The log function in thenegative log-likelihood cost function undoes the exp of some output units
- Unfortunately, mean squared error and mean absolute error often lead to poorresults when used with gradient-based optimization. Some output units thatsaturate produce very small gradients when combined with these cost functions.This is one reason that the cross-entropy cost function is more popular than meansquared error or mean absolute error, even when it is not necessary to estimate anentire distribution p(y | x).
- Most hidden units aredistinguished from each other only by the choice of the form of the activationfunction g(z).
- One drawback to rectified linear units is that they cannot learn via gradient-based methods on examples for which their activation is zero.
- Maxout units generalize rectified linear units further.Instead of applying an element-wise function g(z), maxout units divide z intogroups of k values. Each maxout unit then outputs the maximum element of one of these groups
- A Maxout unit can learn a piecewise linear, convex function with up to k pieces(分段线性函数).Maxout units can thus be seen as learning the activation function itself ratherthan just the relationship between units.
- tanh(z) = 2σ(2z) − 1. σ为sigmod函数
- The widespread saturation ofsigmoidal units can make gradient-based learning very difficult. For this reason,their use as hidden units in feedforward networks is now discouraged. Their useas output units is compatible with the use of gradient-based learning when anappropriate cost function can undo the saturation of the sigmoid in the outputlayer.
- When a sigmoidal activation function must be used, the hyperbolic tangentactivation function(tanh) typically performs better than the logistic sigmoid
- In summary, a feedforward network with a single layer is sufficient to representany function, but the layer may be infeasibly large and may fail to learn andgeneralize correctly. In many circumstances, using deeper models can reduce thenumber of units required to represent the desired function and can reduce theamount of generalization error
- 激活函数：这个函数一般是非线性函数，也就是每个神经元通过这个函数将原有的来自其他神经元的输入做一个非线性变化，输出给下一层神经元。激活函数实现的非线性能力是前向传播很重要的一部分
- 成本函数：用来定量评估在特定输入值下，计算出来的输出结果距离这个输入值的真实值有多远，然后不断调整每一层的权重参数，使最后的损失值最小。这就是完成了一次反向传播。损失值越小，结果就越可靠
- 符号式计算一般是先定义各种变量，然后建立一个数据流图，在数据流图中规定各个变量之间的计算关系，最后需要对数据流图进行编译，但此时的数据流图还是一个空壳儿，里面没有任何实际数据，只有把需要运算的输入放进去后，才能在整个模型中形成数据流，从而形成输出值
- TensorFlow中涉及的运算都要放在图中，而图的允许只发生在会话(session)中。开启会话后，就可以用数据去填充节点，进行运算；关闭会话后，就不能进行计算了。因此，会话提供了操作运行和tensor求值的环境
- TensorFlow的边有两种连接关系：数据依赖和控制依赖。其中，实线边表示数据依赖，代表数据，即张量。任意维度的数据统称为张量。还有一种特殊边，一般画为虚线边，称为依赖控制，可以用于控制操作的运行，这被用来确保happens-before关系，这类边上没有数据流过，但源节点必须在目的节点开始执行前完成执行
- 图中的节点又称为算子，它代表一个操作（OP），一般用来表示施加的数据运算，也可以表示数据输入的起点以及输出的终点，或者是读取/写入持久变量的终点
- 启动图的第一步是创建一个Session对象。会话(session)提供在图中执行操作的一些方法。一般的模式是，建立会话，此时会生成一张空图，在会话中添加节点和边，形成一张图，然后执行
- 在调用Session对象的run()方法来执行图时，传入一些Tensor，这个过程叫填充(feed)；返回的结果类型根据输入的类型而定，这个过程叫取回(fetch)
- 会话是图交互的一个桥梁，一个会话可以有多个图，会话可以修改图的结构，也可以往图中注入数据进行计算。因此，会话主要有两个API接口:Extend和Run。Extend操作是在Graph中添加节点和边，Run操作是输入计算的节点和填充必要的数据后，进行运算，并输出运算结果
- 变量是一种特殊的数据，它在图中有固定的位置，不像普通张量那样可以流动。TensorFlow还提供了填充机制，可以在构建图时使用tf.placeholder()临时替代任意操作的张量，在调用Session对象的run()方法去执行图时，使用填充数据作为调用的参数，调用结束后，填充数据就消失
- 操作(operation)是对抽象操作的一个统称，而内核(kernel)则是能够运行在特定设备上的一种对操作的实现。因此，同一个操作可能会对应多个内核
- 批标准化(batch normalization)是为了克服神经网络层数加深导致难以训练而诞生的。深度神经网络随着网络深度加深，训练起来会越来越困难，收敛速度会很慢，常常会导致 vanishing gradient problem
- Covariate Shift 是指训练集的样本数据和目标样本集分布不一致时，训练得到的模型无法很好地泛化。它是分布不一致假设之下的一个分支问题，也就是指源域和目标域的条件概率是一致的，但是其边缘概率不同。的确，对于神经网络的各层输出，在经过了层内操作后，各层输出分布就会与对应的输入信号分布不同，而且差异会随着网络深度增大而加大，但是每一层所指向的样本标记仍然是不变的。
- 解决思路一般是根据训练样本和目标样本的比例对训练样本做一个矫正。因此，通过引入批标准化来规范化某些层或者所有层的输入，从而固定每层输入信号的均值与方差
- 批标准化一般用在非线性映射（激活函数）之前，对x=Wu+b做规范化，使结果（输出信息各个维度）的均值为0，方差为1.让每一层的输入有一个稳定的分布会有利于网络的训练
- 批标准化通过规范化让激活函数分布在线性区间，结果就是加大了梯度，让模型更加大胆地进行梯度下降，于是有如下优点：加大探索的步长，加快收敛的速度；更容易跳出局部最小值；破坏原来的数据分布，一定程度上缓解过拟合。因此，在遇到神经网络收敛速度很慢或梯度爆炸(gradient explode)等无法训练的情况下，都可以尝试用批标准化来解决
- 梯度消失是指在更新模型时采用链式求导法则反向求导，越往前梯度越小。最终的结果是到达一定深度后梯度对模型的更新就没有任何贡献了
- 梯度爆炸与梯度消失相反，如果是梯度非常大，链式求导后乘积就变得很大，使权重变得非常大，产生指数级爆炸
- 激活函数不会更改输入数据的维度，也就是输入和输出的维度是相同的
- sigmoid函数一旦输入落入饱和区，导数就会接近于0，很容易产生梯度消失
- tanh函数收敛速度比sigmoid要快
- relu函数定义为f(x)=max(x,0)。由于x>0时导数为1，所以，relu能够在x>0时保持梯度不衰减，从而缓解梯度消失问题，还能够更快地收敛，并提供了神经网络的稀疏表达能力。但是，随着训练的进行，部分输入会落入到硬饱和区(x<0)，导致对应的权重无法更新，称为"神经元死亡”
- dropout函数：一个神经元以概率keep_prob决定是否被抑制。如果被抑制，该神经元的输出就为0；如果不被抑制，那么该神经元的输出值将被放大到原来的1/keep_prob倍
- 在神经网络中，池化函数一般跟在卷积函数的下一层。池化操作是利用一个矩阵窗口在张量上进行扫描，将每个矩阵窗口的值通过取最大值或平均值来减少元素个数。每个池化操作的矩阵窗口大小是由ksize指定的，并且根据步长strides决定移动步长
- Adam法根据损失函数针对每个参数的梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率
- RMSProp法通过引入一个衰减系数，使每一回合都衰减一定比例。在实践中，对RNN效果很好
- CNN的权值共享(weight sharing)的网络结构显著降低了模型的复杂度，减少了权值的数量。CNN的特点在于隐藏层分为卷积层和池化层(pooling layer,又叫下采样层)。卷积层通过一块块卷积核（conventional kernel）在原始图像上平移来提取特征，每一个特征就是一个特征映射；而池化层通过汇聚特征后稀疏参数来减少要学习的参数，降低网络的复杂度，池化层最常见的包括最大值池化(max pooling)和平均值池化(average pooling)。
- 卷积核在提取特征映射时的动作称为padding,其有两种方式，即SAME和VALID.由于移动步长(stride)不一定能整除整张图的像素密度，我们把不越过边缘取样称为Valid Padding,取样的面积小于输入图像的像素宽度；越过边缘取样称为Same Padding,取样的面积和输入图像的像素宽度一致。
- 池化层主要是想降低网络训练参数及模型的过拟合程度
- 数据增强：增加训练数据是避免过拟合的好方法，并且能提升算法的准确率。当训练数据有限的时候，可以通过一些变换从已有的训练数据集中生成一些新数据，来扩大训练数据量
- RNN在网络中引入了定性循环，使信号从一个神经元传递到另一个神经元并不会马上消失，而是继续存活。
- RNN的解决方式是，隐藏层的输入不仅包括上一层的输出，还包括上一时刻该隐藏层的输出。
- RNN的特在在于它是按时间顺序展开的，下一步会受本步处理的影响
- BPTT算法是BP算法的扩展，可以将加载在网络上的时序信号按层展开，这样就使得前馈神经网络的静态网络转化为动态网络
- LSTM可以解决梯度消失的问题
- GRU有两个门，即重置门r和更新门z。重置门决定如何组合新输入和之前的记忆，更新门决定留下多少之前的记忆。如果把重置门都设为1，更新门都设为0，就得到普通的RNN模型
- CW-RNN是一种使用时钟频率驱动的RNN。它将隐藏层分为几个组，通过不同的隐藏层组工作在不同的时钟频率下来解决长时间依赖问题。每一组按照自己规定的时钟频率对输入进行处理。将时钟时间离散化，不同的时间点不同的隐藏层组工作，所有的隐藏层组在每一步不会都同时工作，这样就加快网络的训练。另外，时钟周期大的组的神经元速度慢，周期小的速度快，连接方向是周期大的连接到周期小的，周期小的不会连接到周期大的。
- 双向RNN假设当前（第t步）的输出不仅与前面的序列相关，而且与后面的序列有关。原始的双向RNN是一个相对较简单的RNN，由两个RNN上下叠加在一起组成。输出由这两个RNN的隐藏层的状态决定
-gcForest在只有少量数据的情况下也可以训练，并且超参数比深度神经网络少得多，对超参数设定来说性能健壮性也很高，所以使用gcForest训练起来很容易。此外，对于不同规模或者不同数据，也能使用默认设定取得很好的结果
- min-batch大小决定了权重的更新规则。
- 使用one-hot的原因是多类别的输出层是softmax层，它的输出时一个概率分布，从而要求输入的标记也以概率分布的形式出现，进而可以计算交叉熵
- 自编码网络的作用是将输入样本压缩到隐藏层，然后解压，在输出端重建样本。最终输出层神经元数量等于输入层神经元的数量。压缩依靠的是输入数据（图像、文字、声音）本身存在不同程度的冗余信息，自动编码网络通过学习去掉这些冗余信息，把有用的特征输入到隐藏层中。这里和PCA有些类似，要找到可以代表元数据的主要成分。其实，如果激活函数不适用sigmoid等非线性函数，而使用线性函数，就是PCA模型。可以想象，如果数据都是完全随机、相互独立、同分布的，自编码网络就很难学习到一个有效的压缩模型。
- 生成式对抗网络包含一个生成模型和一个判别模型。生成式对抗网络主要解决的问题是如何从训练样本中学习出新样本。生成模型就是负责训练出样本的分布，如果训练样本是图片就生成相似的图片，如果训练样本是文章句子就生成相似的文章句子。判别模型是一个二分类器，用来判断输入样本是真实数据还是训练生成的样本
- 生成式对抗网络的优化是一个二元极小极大博弈问题，它的目的是使生成模型的输出在输入给判别模型时，判别模型很难判断是真实数据还是虚假数据。训练好的生成模型，有能力把一个噪声向量转换成和训练集类似的样本
- 生成式对抗网络（GAN）在无监督学习上是非常有效的。但是，常规的生成式对抗网络的判别器使用的是sigmoid交叉熵损失函数，这在学习过程红可能导致梯度消失。生成式对抗网络的一个改进是Wassertein生成式对抗网络，它使用Wasserstein距离度量而不是Jensen-Shannon散度。生成式对抗网络的另一个改进是使用最小而成生成式对抗网络，它的判别模型采用最小平方损失函数
