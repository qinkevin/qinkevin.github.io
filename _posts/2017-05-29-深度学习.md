---
layout: post
title: "深度学习"       # Title of the post
subtitle:
date:       2017-05-29 12:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 机器学习
---

# 深度学习
- When training neural networks, it is common to use "weight decay," where after each update, the weights are multiplied by a factor slightly less than 1. This prevents the weights from growing too large, and can be seen as gradient descent on a quadratic regularization term
- At the beginning, we are far from the destination, so we use larger learning rate；After several epochs, we are close to the destination, so we reduce the learning rate
- Why CNN for Image：Some patterns are much smaller than the whole image；The same patterns appear in different regions；Subsampling the pixels will not change the object
- 在RNN中，The same network is used again and again.
- Difficulties of Reinforcement Learning：It may be better to sacrifice immediate reward to gain more long-term reward；Agent’s actions affect the subsequent data it receives
- The quintessential example of a representation learning algorithm is the autoencoder. An autoencoder is the combination of an encoder function that converts the input data into a different representation, and a decoder function that converts the new representation back into the original format. Autoencoders are trained to preserve as much information as possible when an input is run through the encoder and then the decoder, but are also trained to make the new representation have various nice properties.
- In some cases we will need an array with more than two axes. In the general case, an array of numbers arranged on a regular grid with avariable number of axes is known as a tensor.
- The L1 norm is commonly used in machine learning when the difference between zero and nonzero elements is very important
- One other norm that commonly arises in machine learning is the L∞ norm,also known as the max norm. This norm simplifies to the absolute value of the element with the largest magnitude in the vector
- Diagonal matrices are of interest in part because multiplying by a diagonal matrixis very computationally efficient.
- In many cases, it is more practical to use a simple but uncertain rule rather than a complex but certain one, even if the true rule is deterministic and our modeling system has the fidelity to accommodate a complex rule.
- Independence is a stronger requirement than zero covariance, because independence also excludes nonlinear relationships.
- In the absence of prior knowledge about what form a distribution over the real numbers should take, the normal distribution is a good default choice for two major reasons
- A mixture distribution is made up of several component distributions. On each trial, the choice of which component distribution generates the sample is determined by sampling a component identity from a multinoulli distribution
- A very powerful and common type of mixture model is the Gaussian mixture model, in which the components are Gaussians. Each component has a separately parametrized mean μ(i) and covariance Σ(i).
- The basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred
- If we have two separate probability distributions P (x) and Q(x) over the same random variable x, we can measure how different these two distributions are usingthe Kullback-Leibler (KL) divergence
- Conditioning refers to how rapidly a function changes with respect to small changes in its inputs. Functions that change rapidly when their inputs are perturbed slightly can be problematic for scientific computation because rounding errors in the inputs can result in large changes in the output
- The matrix containing all such partial derivatives is known as a Jacobian matrix
- When our function has multiple input dimensions, there are many second derivatives. These derivatives can be collected together into a matrix called the Hessian matrix.
- Because the Hessian matrix is real and symmetric, we can decompose it into a set of real eigenvalues and an orthogonal basis of eigenvectors.When the Hessian has a poor condition number, gradient descent performs poorly. This is because in one direction, the derivative increases rapidly, while in another direction, it increases slowly. Gradient descent is unaware of this change in the derivative so it does not know that it needs to explore preferentially in the direction where the derivative remains negative for longer.
- The simplest method for doing so is known as Newton’s method.Newton’s method is based on using a second-order Taylor series expansion to approximate f(x) near some point x(0)
- Newton’s method is only appropriate when the nearby critical point is a minimum (all the eigenvalues of the Hessian are positive), whereas gradient descent is not attracted to saddle points unless the gradient points toward them.
- Convex optimization algorithms are applicable only to convex functions—functions for which the Hessian is positive semidefinite everywhere. Such functions are well-behaved because they lack saddle points and all of their local minima are necessarily global minima.
- One way to interpret maximum likelihood estimation is to view it as minimizing the dissimilarity between the empirical distribution pˆdata defined by the training set and the model distribution, with the degree of dissimilarity between the two measured by the KL divergence
- Bayesian methods typically generalize much better when limited training data is available, but typically suffer from high computational cost when the number of training examples is large
- For feedforward neural networks, it is important to initialize all weights to small random values. The biases may be initialized to zero or to small positive values.
- An advantage of this approach of deriving the cost function from maximum likelihood is that it removes the burden of designing cost functions for each model.Specifying a model p(y|x) automatically determines a cost function log p(y|x).
- Functions that saturate (become very flat) undermine this objective because they make the gradient become very small. In many cases  this happens because the activation functions used to produce the output of the hidden units or the output units saturate. The negative log-likelihood helps to avoid this problem for many models. Many output units involve an exp function that can saturate when its argument is very negative. The log function in the negative log-likelihood cost function undoes the exp of some output units
- Unfortunately, mean squared error and mean absolute error often lead to poor results when used with gradient-based optimization. Some output units that saturate produce very small gradients when combined with these cost functions.This is one reason that the cross-entropy cost function is more popular than mean squared error or mean absolute error, even when it is not necessary to estimate an entire distribution p(y|x).
- Most hidden units are distinguished from each other only by the choice of the form of the activation function g(z).
- One drawback to rectified linear units is that they cannot learn via gradient-based methods on examples for which their activation is zero.
- Maxout units generalize rectified linear units further.Instead of applying an element-wise function g(z), maxout units divide z into groups of k values. Each maxout unit then outputs the maximum element of one of these groups
- A Maxout unit can learn a piecewise linear, convex function with up to k pieces(分段线性函数).Maxout units can thus be seen as learning the activation function itself rather than just the relationship between units.
- tanh(z) = 2σ(2z) − 1. σ为sigmod函数
- The widespread saturation of sigmoidal units can make gradient-based learning very difficult. For this reason,their use as hidden units in feedforward networks is now discouraged. Their use as output units is compatible with the use of gradient-based learning when anappropriate cost function can undo the saturation of the sigmoid in the output layer.
- When a sigmoidal activation function must be used, the hyperbolic tangent activation function(tanh) typically performs better than the logistic sigmoid
- In summary, a feedforward network with a single layer is sufficient to represent any function, but the layer may be infeasibly large and may fail to learn and generalize correctly. In many circumstances, using deeper models can reduce the number of units required to represent the desired function and can reduce the amount of generalization error
- 激活函数：这个函数一般是非线性函数，也就是每个神经元通过这个函数将原有的来自其他神经元的输入做一个非线性变化，输出给下一层神经元。激活函数实现的非线性能力是前向传播很重要的一部分
- 成本函数：用来定量评估在特定输入值下，计算出来的输出结果距离这个输入值的真实值有多远，然后不断调整每一层的权重参数，使最后的损失值最小。这就是完成了一次反向传播。损失值越小，结果就越可靠
- 符号式计算一般是先定义各种变量，然后建立一个数据流图，在数据流图中规定各个变量之间的计算关系，最后需要对数据流图进行编译，但此时的数据流图还是一个空壳儿，里面没有任何实际数据，只有把需要运算的输入放进去后，才能在整个模型中形成数据流，从而形成输出值
- TensorFlow中涉及的运算都要放在图中，而图的允许只发生在会话(session)中。开启会话后，就可以用数据去填充节点，进行运算；关闭会话后，就不能进行计算了。因此，会话提供了操作运行和tensor求值的环境
- TensorFlow的边有两种连接关系：数据依赖和控制依赖。其中，实线边表示数据依赖，代表数据，即张量。任意维度的数据统称为张量。还有一种特殊边，一般画为虚线边，称为依赖控制，可以用于控制操作的运行，这被用来确保happens-before关系，这类边上没有数据流过，但源节点必须在目的节点开始执行前完成执行
- 图中的节点又称为算子，它代表一个操作（OP），一般用来表示施加的数据运算，也可以表示数据输入的起点以及输出的终点，或者是读取/写入持久变量的终点
- 启动图的第一步是创建一个Session对象。会话(session)提供在图中执行操作的一些方法。一般的模式是，建立会话，此时会生成一张空图，在会话中添加节点和边，形成一张图，然后执行
- 在调用Session对象的run()方法来执行图时，传入一些Tensor，这个过程叫填充(feed)；返回的结果类型根据输入的类型而定，这个过程叫取回(fetch)
- 会话是图交互的一个桥梁，一个会话可以有多个图，会话可以修改图的结构，也可以往图中注入数据进行计算。因此，会话主要有两个API接口:Extend和Run。Extend操作是在Graph中添加节点和边，Run操作是输入计算的节点和填充必要的数据后，进行运算，并输出运算结果
- 变量是一种特殊的数据，它在图中有固定的位置，不像普通张量那样可以流动。TensorFlow还提供了填充机制，可以在构建图时使用tf.placeholder()临时替代任意操作的张量，在调用Session对象的run()方法去执行图时，使用填充数据作为调用的参数，调用结束后，填充数据就消失
- 操作(operation)是对抽象操作的一个统称，而内核(kernel)则是能够运行在特定设备上的一种对操作的实现。因此，同一个操作可能会对应多个内核
- 批标准化(batch normalization)是为了克服神经网络层数加深导致难以训练而诞生的。深度神经网络随着网络深度加深，训练起来会越来越困难，收敛速度会很慢，常常会导致 vanishing gradient problem
- Covariate Shift 是指训练集的样本数据和目标样本集分布不一致时，训练得到的模型无法很好地泛化。它是分布不一致假设之下的一个分支问题，也就是指源域和目标域的条件概率是一致的，但是其边缘概率不同。的确，对于神经网络的各层输出，在经过了层内操作后，各层输出分布就会与对应的输入信号分布不同，而且差异会随着网络深度增大而加大，但是每一层所指向的样本标记仍然是不变的。
- 解决思路一般是根据训练样本和目标样本的比例对训练样本做一个矫正。因此，通过引入批标准化来规范化某些层或者所有层的输入，从而固定每层输入信号的均值与方差
- 批标准化一般用在非线性映射（激活函数）之前，对x=Wu+b做规范化，使结果（输出信息各个维度）的均值为0，方差为1.让每一层的输入有一个稳定的分布会有利于网络的训练
- 批标准化通过规范化让激活函数分布在线性区间，结果就是加大了梯度，让模型更加大胆地进行梯度下降，于是有如下优点：加大探索的步长，加快收敛的速度；更容易跳出局部最小值；破坏原来的数据分布，一定程度上缓解过拟合。因此，在遇到神经网络收敛速度很慢或梯度爆炸(gradient explode)等无法训练的情况下，都可以尝试用批标准化来解决
- 梯度消失是指在更新模型时采用链式求导法则反向求导，越往前梯度越小。最终的结果是到达一定深度后梯度对模型的更新就没有任何贡献了
- 梯度爆炸与梯度消失相反，如果是梯度非常大，链式求导后乘积就变得很大，使权重变得非常大，产生指数级爆炸
- 激活函数不会更改输入数据的维度，也就是输入和输出的维度是相同的
- sigmoid函数一旦输入落入饱和区，导数就会接近于0，很容易产生梯度消失
- tanh函数收敛速度比sigmoid要快
- relu函数定义为f(x)=max(x,0)。由于x>0时导数为1，所以，relu能够在x>0时保持梯度不衰减，从而缓解梯度消失问题，还能够更快地收敛，并提供了神经网络的稀疏表达能力。但是，随着训练的进行，部分输入会落入到硬饱和区(x<0)，导致对应的权重无法更新，称为"神经元死亡”
- dropout函数：一个神经元以概率keep_prob决定是否被抑制。如果被抑制，该神经元的输出就为0；如果不被抑制，那么该神经元的输出值将被放大到原来的1/keep_prob倍
- 在神经网络中，池化函数一般跟在卷积函数的下一层。池化操作是利用一个矩阵窗口在张量上进行扫描，将每个矩阵窗口的值通过取最大值或平均值来减少元素个数。每个池化操作的矩阵窗口大小是由ksize指定的，并且根据步长strides决定移动步长
- Adam法根据损失函数针对每个参数的梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率
- RMSProp法通过引入一个衰减系数，使每一回合都衰减一定比例。在实践中，对RNN效果很好
- CNN的权值共享(weight sharing)的网络结构显著降低了模型的复杂度，减少了权值的数量。CNN的特点在于隐藏层分为卷积层和池化层(pooling layer,又叫下采样层)。卷积层通过一块块卷积核（conventional kernel）在原始图像上平移来提取特征，每一个特征就是一个特征映射；而池化层通过汇聚特征后稀疏参数来减少要学习的参数，降低网络的复杂度，池化层最常见的包括最大值池化(max pooling)和平均值池化(average pooling)。
- 卷积核在提取特征映射时的动作称为padding,其有两种方式，即SAME和VALID.由于移动步长(stride)不一定能整除整张图的像素密度，我们把不越过边缘取样称为Valid Padding,取样的面积小于输入图像的像素宽度；越过边缘取样称为Same Padding,取样的面积和输入图像的像素宽度一致。
- 池化层主要是想降低网络训练参数及模型的过拟合程度
- 数据增强：增加训练数据是避免过拟合的好方法，并且能提升算法的准确率。当训练数据有限的时候，可以通过一些变换从已有的训练数据集中生成一些新数据，来扩大训练数据量
- RNN在网络中引入了定性循环，使信号从一个神经元传递到另一个神经元并不会马上消失，而是继续存活。
- RNN的解决方式是，隐藏层的输入不仅包括上一层的输出，还包括上一时刻该隐藏层的输出。
- RNN的特点在于它是按时间顺序展开的，下一步会受本步处理的影响
- BPTT算法是BP算法的扩展，可以将加载在网络上的时序信号按层展开，这样就使得前馈神经网络的静态网络转化为动态网络
- LSTM可以解决梯度消失的问题,但不能解决梯度爆炸的问题。因为memory and input are added.The influence never disappears unless forget gate is closed
- GRU有两个门，即重置门r和更新门z。重置门决定如何组合新输入和之前的记忆，更新门决定留下多少之前的记忆。如果把重置门都设为1，更新门都设为0，就得到普通的RNN模型
- CW-RNN是一种使用时钟频率驱动的RNN。它将隐藏层分为几个组，通过不同的隐藏层组工作在不同的时钟频率下来解决长时间依赖问题。每一组按照自己规定的时钟频率对输入进行处理。将时钟时间离散化，不同的时间点不同的隐藏层组工作，所有的隐藏层组在每一步不会都同时工作，这样就加快网络的训练。另外，时钟周期大的组的神经元速度慢，周期小的速度快，连接方向是周期大的连接到周期小的，周期小的不会连接到周期大的。
- 双向RNN假设当前（第t步）的输出不仅与前面的序列相关，而且与后面的序列有关。原始的双向RNN是一个相对较简单的RNN，由两个RNN上下叠加在一起组成。输出由这两个RNN的隐藏层的状态决定
- gcForest在只有少量数据的情况下也可以训练，并且超参数比深度神经网络少得多，对超参数设定来说性能健壮性也很高，所以使用gcForest训练起来很容易。此外，对于不同规模或者不同数据，也能使用默认设定取得很好的结果
- min-batch大小决定了权重的更新规则。
- 使用one-hot的原因是多类别的输出层是softmax层，它的输出是一个概率分布，从而要求输入的标记也以概率分布的形式出现，进而可以计算交叉熵
- 自编码网络的作用是将输入样本压缩到隐藏层，然后解压，在输出端重建样本。最终输出层神经元数量等于输入层神经元的数量。压缩依靠的是输入数据（图像、文字、声音）本身存在不同程度的冗余信息，自动编码网络通过学习去掉这些冗余信息，把有用的特征输入到隐藏层中。这里和PCA有些类似，要找到可以代表元数据的主要成分。其实，如果激活函数不适用sigmoid等非线性函数，而使用线性函数，就是PCA模型。可以想象，如果数据都是完全随机、相互独立、同分布的，自编码网络就很难学习到一个有效的压缩模型。
- 生成式对抗网络包含一个生成模型和一个判别模型。生成式对抗网络主要解决的问题是如何从训练样本中学习出新样本。生成模型就是负责训练出样本的分布，如果训练样本是图片就生成相似的图片，如果训练样本是文章句子就生成相似的文章句子。判别模型是一个二分类器，用来判断输入样本是真实数据还是训练生成的样本
- 生成式对抗网络的优化是一个二元极小极大博弈问题，它的目的是使生成模型的输出在输入给判别模型时，判别模型很难判断是真实数据还是虚假数据。训练好的生成模型，有能力把一个噪声向量转换成和训练集类似的样本
- 生成式对抗网络（GAN）在无监督学习上是非常有效的。但是，常规的生成式对抗网络的判别器使用的是sigmoid交叉熵损失函数，这在学习过程红可能导致梯度消失。生成式对抗网络的一个改进是Wassertein生成式对抗网络，它使用Wasserstein距离度量而不是Jensen-Shannon散度。生成式对抗网络的另一个改进是使用最小而成生成式对抗网络，它的判别模型采用最小平方损失函数
- 在反向传播的过程中，Specifically, many subexpressions may be repeated several times within the overall expression for the gradient. Any procedure that computes the gradient will need to choose whether to store these subexpressions or to recompute them several times.
- the effect of L1 regularization is quite different from that of L2 regularization. Specifically, we can see that the regularization contribution to the gradient no longer scales linearly with each wi; instead it is a constant factor with a sign equal to sign(wi). One consequence of this form of the gradient is that we will not necessarily see clean algebraic solutions to quadratic approximations of J(X,y;w) as we did for L2 regularization.
- In comparison to L2 regularization, L1 regularization results in a solution that is more sparse. Sparsity in this context refers to the fact that some parameters have an optimal value of zero. The sparsity of L1 regularization is a qualitatively different behavior than arises with L2 regularization.
- A significant advantage of parameter sharing over regularizing the parameters to be close (via a norm penalty) is that only a subset of the parameters (the unique set) need to be stored in memory.
- Specifically, dropout trains the ensemble consisting of all sub-networks that can be formed by removing non-output units from an underlying base network
- Dropout training is not quite the same as bagging training. In the case of bagging, the models are all independent. In the case of dropout, the models share parameters, with each model inheriting a different subset of parameters from the parent neural network. This parameter sharing makes it possible to represent an exponential number of models with a tractable amount of memory. In the case of bagging, each model is trained to convergence on its respective training set. In the case of dropout, typically most models are not explicitly trained at all—usually, the model is large enough that it would be infeasible to sample all possible sub- networks within the lifetime of the universe. Instead, a tiny fraction of the possible sub-networks are each trained for a single step, and the parameter sharing causes the remaining sub-networks to arrive at good settings of the parameters
- A key insight (Hinton et al., 2012c) involved in dropout is that we can approximate p by evaluating p(y | x) in one model: the model with all units, but with the weights going out of unit i multiplied by the probability of including unit i. The motivation for this modification is to capture the right expected value of the output from that unit. We call this approach the weight scaling inference rule.
- One advantage of dropout is that it is very computationally cheap;Another significant advantage of dropout is that it does not significantly limit the type of model or training procedure that can be used.
- One of the key insights of dropout is that training a network with stochastic behavior and making predictions by averaging over multiple stochastic decisions implements a form of bagging with parameter sharing.
- Optimization algorithms that use the entire training set are called batch or deterministic gradient methods, because they process all of the training examples simultaneously in a large batch
- Optimization algorithms that use only a single example at a time are sometimes called stochastic or sometimes online methods.
- Ill-conditioning can manifest by causing SGD to get “stuck” in the sense that even very small steps increase the cost function.
- For many high-dimensional non-convex functions, local minima (and maxima) are in fact rare compared to another kind of point with zero gradient: a saddle point
- Neural networks with many layers often have extremely steep regions resembling cliffs. These result from the multiplication of several large weights together. On the face of an extremely steep cliff structure, the gradient update step can move the parameters extremely far, usually jumping off of the cliff structure altogether
- gradient clipping:When the traditional gradient descent algorithm proposes to make a very large step, the gradient clipping heuristic intervenes to reduce the step size to be small enough that it is less likely to go outside the region where the gradient indicates the direction of approximately steepest descent.
-  Cliff structures are most common in the cost functions for recurrent neural networks, because such models involve a multiplication of many factors, with one factor for each time step.
- vanishing and exploding gradient problem: Any eigenvalues λi that are not near an absolute value of 1 will either explode if they are greater than 1 in magnitude or vanish if they are less than 1 in magnitude.Vanishing gradients make it difficult to know which direction the parameters should move to improve the cost function, while exploding gradients can make learning unstable. The cliff structures described earlier that motivate gradient clipping are an example of the exploding gradient phenomenon.
- Recurrent networks use the same matrix W at each time step, but feedforward networks do not, so even very deep feedforward networks can largely avoid the vanishing and exploding gradient problem
- Regardless of which of these problems are most significant, all of them might be avoided if there exists a region of space connected reasonably directly to a solution by a path that local descent can follow, and if we are able to initialize learning within that well-behaved region. This last view suggests research into choosing good initial points for traditional optimization algorithms to use.
- Stochastic gradient descent (SGD) is possible to obtain an unbiased estimate of the gradient by taking the average gradient on a minibatch of m examples drawn i.i.d from the data generating distribution
- In practice, it is necessary to gradually decrease the learning rate over time。This is because the SGD gradient estimator introduces a source of noise (the random sampling of m training examples) that does not vanish even when we arrive at a minimum. By comparison, the true gradient of the total cost function becomes small and then 0 when we approach and reach a minimum using batch gradient descent, so batch gradient descent can use a fixed learning rate
- The most important property of SGD and related minibatch or online gradient-based optimization is that computation time per update does not grow with the number of training examples.
- The difference between Nesterov momentum and standard momentum is where the gradient is evaluated. With Nesterov momentum the gradient is evaluated after the current velocity is applied. Thus one can interpret Nesterov momentum
as attempting to add a correction factor to the standard method of momentum.
- We almost always initialize all the weights in the model to values drawn randomly from a Gaussian or uniform distribution
- To some extent, the exploding gradient problem can be mitigated by gradient clipping (thresholding the values of the gradients before performing a gradient descent step).
- The AdaGrad algorithm individually adapts the learning rates of all model parameters by scaling them inversely proportional to the square root of the sum of all of their historical squared values。The parameters with the largest partial derivative of the loss have a correspondingly rapid decrease in their learning rate, while parameters with small partial derivatives have a relatively small decrease in their learning rate. The net effect is greater progress in the more gently sloped directions of parameter space.
- The RMSProp algorithm modifies AdaGrad to perform better in the non-convex setting by changing the gradient accumulation into an exponentially weighted moving average.
- AdaGrad shrinks the learning rate according to the entire history of the squared gradient and may have made the learning rate too small before arriving at such a convex structure. RMSProp uses an exponentially decaying average to discard history from the extreme past so that it can converge rapidly after finding a convex bowl, as if it were an instance of the AdaGrad algorithm initialized within that bowl.
- Adam:In the context of the earlier algorithms, it is perhaps best seen as a variant on the combination of RMSProp and momentum with a few important distinctions.
- Adam is generally regarded as being fairly robust to the choice of hyperparameters, though the learning rate sometimes needs to be changed from the suggested default.
- Newton’s method is an optimization scheme based on using a second-order Tay- lor series expansion to approximate J(θ) near some point θ0, ignoring derivatives of higher order
- Newton’s method is appropriate only when the Hessian is positive definite
- Beyond the challenges created by certain features of the objective function, such as saddle points, the application of Newton’s method for training large neural networks is limited by the significant computational burden it imposes.since the parameters will change with every update, the inverse Hessian has to be computed at every training iteration。As a consequence, only networks a with very small number of parameters can be practically trained via Newton’s method
- Conjugate gradients is a method to efficiently avoid the calculation of the inverse Hessian by iteratively descending conjugate directions.
- The Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm attempts to bring some of the advantages of Newton’s method without the computational burden。The primary computational difficulty in applying Newton’s update is the calculation of the inverse Hessian H−1. The approach adopted by quasi-Newton methods (of which the BFGS algorithm is the most prominent) is to approximate the inverse with a matrix Mt that is iteratively refined by low rank updates to become a better approximation of H−1
- relative to conjugate gradients, BFGS has the advantage that it can spend less time refining each line search. On the other hand, the BFGS algorithm must store the inverse Hessian matrix, M, that requires O(n2) memory, making BFGS impractical for most modern deep learning models that typically have millions of parameters.
- In order to maintain the expressive power of the network, it is common to replace the batch of hidden unit activations H with γH0 +β rather than simply the normalized H0. The variables γ and β are learned parameters that allow the new variable to have any mean and standard deviation.
- In some cases, it may be possible to solve an optimization problem quickly by breaking it into separate pieces. If we minimize f (x) with respect to a single variable xi, then minimize it with respect to another variable xj and so on, repeatedly cycling through all variables, we are guaranteed to arrive at a (local) minimum. This practice is known as coordinate descent, because we optimize one coordinate at a time
- Coordinate descent makes the most sense when the different variables in the optimization problem can be clearly separated into groups that play relatively isolated roles, or when optimization with respect to one group of variables is significantly more efficient than optimization with respect to all of the variables.
- Coordinate descent is not a very good strategy when the value of one variable strongly influences the optimal value of another variable
- Polyak averaging：The basic idea is that the optimization algorithm may leap back and forth across a valley several times without ever visiting a point near the bottom of the valley. The average of all of the locations on either side should be close to the bottom of the valley though
- In practice, it is more important to choose a model family that is easy to optimize than to use a powerful optimization algorithm
- Specifically, modern neural networks reflect a design choice to use linear trans- formations between layers and activation functions that are differentiable almost everywhere and have significant slope in large portions of their domain.
- linear paths or skip connections between layers reduce the length of the shortest path from the lower layer’s parameters to the output, and thus mitigate the vanishing gradient problem
- Continuation methods are a family of strategies that can make optimization easier by choosing initial points to ensure that local optimization spends most of its time in well-behaved regions of space
- Continuation methods traditionally were mostly designed with the goal of overcoming the challenge of local minima. Specifically, they were designed to reach a global minimum despite the presence of many local minima
- Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers
- In its most general form, convolution is an operation on two functions of a real-valued argument
- In convolutional network terminology, the first argument (in this example, the function x) to the convolution is often referred to as the input and the second argument (in this example, the function w) as the kernel. The output is sometimes referred to as the feature map.
- Traditional neural network layers use matrix multiplication by a matrix of parameters with a separate parameter describing the interaction between each input unit and each output unit. This means every output unit interacts with every input unit. Convolutional networks, however, typically have sparse interactions(also referred to as sparse connectivity or sparse weights). This is accomplished by making the kernel smaller than the input。This means that we need to store fewer parameters, which both reduces the memory requirements of the model and improves its statistical efficiency。It also means that computing the output requires fewer operations
- The parameter sharing used by the convolution operation means that rather than learning a separate set of parameters for every location, we learn only one set
- A typical layer of a convolutional network consists of three stages. In the first stage, the layer performs several convolutions in parallel to produce a set of linear activations. In the second stage, each linear activation is run through an on linear activation function, such as the rectified linear activation function. This stage is sometimes called the detector stage. In the third stage, we use a pooling function to modify the output of the layer further.
- A pooling function replaces the output of the net at a certain location with a summary statistic of the nearby outputs
- In all cases, pooling helps to make the representation become approximately invariant to small translations of the input. Invariance to translation means that if we translate the input by a small amount, the values of most of the pooled outputs do not change.Invariance to local translation can be a very useful property if we care more about whether some feature is present than exactly where it is.
- For many tasks, pooling is essential for handling inputs of varying size. For example, if we want to classify images of variable size, the input to the classification layer must have a fixed size. This is usually accomplished by varying the size of an offset between pooling regions so that the classification layer always receives the same number of summary statistics regardless of the input size.
- One key insight is that convolution and pooling can cause underfitting. Like any prior, convolution and pooling are only useful when the assumptions made by the prior are reasonably accurate. If a task relies on preserving precise spatial information, then using pooling on all features can increase the training error.
- One essential feature of any convolutional network implementation is the ability to implicitly zero-pad the input V in order to make it wider.Zero padding the input allows us to control the kernel width and the size of the output independently.
- One is the extreme case in which no zero-padding is used whatsoever, and the convolution kernel is only allowed to visit positions where the entire kernel is contained entirely within the image. In MATLAB terminology, this is called valid convolution. In this case, all pixels in the output are a function of the same number of pixels in the input, so the behavior of an output pixel is somewhat more regular. However , the size of the output shrinks at each layer. If the input image has width m and the kernel has width k, the output will be of width m − k+ 1. The rate of this shrinkage can be dramatic if the kernels used are large. Since the shrinkage is greater than 0, it limits the number of convolutional layers that can be included in the network. As layers are added, the spatial dimension of the network will eventually drop to 1 × 1, at which point additional layers cannot meaningfully be considered convolutional. Another special case of the zero-padding setting is when just enough zero-padding is added to keep the size of the output equal to the size of the input. MATLAB calls this same convolution. In this case, the network can contain as many convolutional layers as the available hardware can support , since the operation of convolution does not modify the architectural possibilities available to the next layer. However, the input pixels near the border influence fewer output pixels than the input pixels near the center. This can make the border pixels somewhat underrepresented in the model.
- In some cases, we do not actually want to use convolution, but rather locally connected layers.In this case, the adjacency matrix in the graph of our MLP is the same, but every connection has its own weight.Locally connected layers are useful when we know that each feature should be a function of a small part of space, but there is no reason to think that the same feature should occur across all of space.
- It can also be useful to make versions of convolution or locally connected layers in which the connectivity is further restricted, for example to constrain that each output channel i be a function of only a subset of the input channels l. A common way to do this is to make the first m output channels connect to only the first n input channels, the second m output channels connect to only the second n input channels, and so on.
- Tiled convolution offers a compromise between a convolutional layer and a locally connected layer. Rather than learning a separate set of weights at every spatial location, we learn a set of kernels that we rotate through as we move through space.Each time we move one pixel to the right in the output, we move on to using a different kernel. This means that, like the locally connected layer, neighboring units in the output have different parameters.Unlike the locally connected layer, after we have gone through all t available kernels, we cycle back to the first kernel.Traditional convolution is equivalent to tiled convolution with t = 1
- One advantage to convolutional networks is that they can also process inputs with varying spatial extents. These kinds of input simply cannot be represented by traditional, matrix multiplication-based neural networks. This provides a compelling reason to use convolutional networks even when computational cost and overfitting are not significant issues
- Typically, the most expensive part of convolutional network training is learning the features.One way to reduce the cost of convolutional network training is to use features that are not trained in a supervised fashion
- Parameter sharing makes it possible to extend and apply the model to examples of different forms(different lengths, here) and generalize across them.Such sharing is particularly important when a specific piece of information can occur at multiple positions within the sequence.a recurrent neural network shares the same weights across several time steps
- The convolution operation allows a network to share parameters across time, but is shallow. The output of convolution is a sequence where each member of the output is a function of a small number of neighboring members of the input. The idea of parameter sharing manifests in the application of the same convolution kernel at each time step. Recurrent networks share parameters in a different way. Each member of the output is a function of the previous members of the output. Each member of the output is produced using the same update rule applied to the previous outputs.This recurrent formulation results in the sharing of parameters through a very deep computational graph.
- Many recurrent neural networks use h(t) =f(h(t−1),x(t);θ)or a similar equation to define the values of their hidden units.
- The unfolding process thus introduces two major advantages:1. Regardless of the sequence length, the learned model always has the same input size, because it is specified in terms of transition from one state to another state, rather than specified in terms of a variable-length history of states.2. It is possible to use the same transition function f with the same parameters at every time step.These two factors make it possible to learn a single model f that operates on all time steps and all sequence lengths, rather than needing to learn a separate model g(t) for all possible time steps.
- Teacher forcing is a procedure that emerges from the maximum likelihood criterion, in which during training the model receives the ground truth output y(t) as input at time t + 1.Teacher forcing is a training technique that is applicable to RNNs that have connections from their output to their hidden states at the next time step.
- The use of back-propagation on the unrolled graph is called the back-propagation through time (BPTT) algorithm
- The price recurrent networks pay for their reduced number of parameters is that optimizing the parameters may be difficult
- Computation of a typical bidirectional recurrent neural network, meant to learn to map input sequences x to target sequences y, with loss L(t) at each step t.The h recurrence propagates information forward in time (towards the right) while the g recurrence propagates information backward in time (towards the left). Thus at each point t, the output units o(t) can benefit from a relevant summary of the past in its h(t) input and from a relevant summary of the future in its g(t) input.
- the encoder-decoder or sequence-to-sequence architecture map a variable-length sequence to an-other variable-length sequence.The idea is very simple:(1) an encoder or reader or input RNN processes the input sequence. The encoder emits the context C, usually as a simple function of its final hidden state. (2) a decoder or writer or output RNN is conditioned on that fixed-length vector to generate the output sequenceY =(y(1),...,y(ny)). The innovation of this kind of architecture over those presented in earlier sections of this chapter is that the lengths nx and ny can vary from each other, while previous architectures constrained nx = ny = τ .In a sequence-to-sequence architecture, the two RNNs are trained jointly to maximize the average of logP(y(1),...,y(ny) |x(1),...,x(nx))over all the pairs of x and y sequences in the training set. The last state hnx of the encoder RNN is typically used as a representation C of the input sequence that is provided as input to the decoder RNN.
- Recursive neural networks represent yet another generalization of recurrent net-works, with a different kind of computational graph, which is structured as a deep tree, rather than the chain-like structure of RNNs.
- One clear advantage of recursive nets over recurrent nets is that for a sequence of the same length τ, the depth (measured as the number of compositions of nonlinear operations) can be drastically reduced from τ to O(logτ), which might help deal with long-term dependencies. An open question is how to best structure the tree. One option is to have a tree structure which does not depend on the data, such as a balanced binary tree
- A recursive network has a computational graph that generalizes that of the recurrent network from a chain to a tree.
- The mathematical challenge of learning long-term dependencies in recurrent networks:The basic problem is that gradients propagated over many stages tend to either vanish (most of the time) or explode (rarely, but with much damage to the optimization). Even if we assume that the parameters are such that the recurrent network is stable (can store memories, with gradients not exploding), the difficulty with long-term dependencies arises from the exponentially smaller weights given to long-term interactions (involving the multiplication of many Jacobians) compared to short-term ones
- Recurrent networks involve the composition of the same function multiple times, once per time step.In particular, the function composition employed by recurrent neural networks somewhat resembles matrix multiplication.
- Specifically, whenever the model is able to represent long term dependencies, the gradient of a long term interaction has exponentially smaller magnitude than the gradient of a short term interaction. It does not mean that it is impossible to learn, but that it might take a very long time to learn long-term dependencies, because the signal about these dependencies will tend to be hidden by the smallest fluctuations arising from short-term dependencies.
- One way to deal with long-term dependencies is to design a model that operates at multiple time scales, so that some parts of the model operate at fine-grained time scales and can handle small details, while other parts operate at coarse timescales and transfer information from the distant past to the present more efficiently.
- Adding Skip Connections through Time:One way to obtain coarse time scales is to add direct connections from variables in the distant past to variables in the present
- Leaky Units and a Spectrum of Different Time Scales:Another way to obtain paths on which the product of derivatives is close to one is to have units with linear self-connections and a weight near one on these connections.
- When we accumulate a running average μ(t) of some value v(t) by applying the update μ(t) ← αμ(t−1) + (1 − α)v(t) the α parameter is an example of a linear self-connection from μ(t−1) to μ(t). When α is near one, the running average remembers information about the past for a long time, and when α is near zero, information about the past is rapidly discarded. Hidden units with linear self-connections can behave similarly to such running averages. Such hidden units are called leaky units.
- Skip connections through d time steps are a way of ensuring that a unit can always learn to be influenced by a value from d time steps earlier. The use of a linear self-connection with a weight near one is a different way of ensuring that the unit can access values from the past. The linear self-connection approach allows this effect to be adapted more smoothly and flexibly by adjusting the real-valuedα rather than by adjusting the integer-valued skip length.
- Removing Connections:This idea differs from the skip connections through time discussed earlier because it involves actively removing length-one connections and replacing them with longer connections. Units modified in such a way are forced to operate on a long time scale. Skip connections through time add edges. Units receiving such new connections may learn to operate on a long time scale but may also choose to focus on their other short-term connections.
- Like leaky units, gated RNNs are based on the idea of creating paths through time that have derivatives that neither vanish nor explode. Leaky units did this with connection weights that were either manually chosen constants or were parameters. Gated RNNs generalize this to connection weights that may change at each time step.
- Leaky units allow the network to accumulate information (such as evidence for a particular feature or category) over a long duration. However, once that information has been used, it might be useful for the neural network to forget the old state.Instead of manually deciding when to clear the state, we want the neural network to learn to decide when to do it. This is what gated RNNs do.
- The clever idea of introducing self-loops to produce paths where the gradient can flow for long durations is a core contribution of the initial long short-term memory(LSTM) model。A crucial addition has been to make the weight on this self-loop conditioned on the context, rather than fixed。By making the weight of this self-loop gated (controlled by another hidden unit), the time scale of integration can be changed dynamically.
- LSTM：Cells are connected recurrently to each other, replacing the usual hidden units of ordinary recurrent networks.An input feature is computed with a regular artificial neuron unit. Its value can be accumulated into the state if the sigmoidal input gate allows it. The state unit has a linear self-loop whose weight is controlled by the forget gate. The output of the cell can be shut off by the output gate. All the gating units have a sigmoid nonlinearity, while the input unit can have any squashing nonlinearity. The state unit can also be used as an extra input to the gating units.
- Instead of a unit that simply applies an element-wise nonlinearity to the affine transformation of inputs and recurrent units, LSTM recurrent networks have “LSTM cells” that have an internal recurrence (a self-loop),in addition to the outer recurrence of the RNN. Each cell has the same inputs and outputs as an ordinary recurrent network, but has more parameters and a system of gating units that controls the flow of information
- GRU：The main difference with the LSTM is that a single gating unit simultaneously controls the forgetting factor and the decision to update the state unit.The reset and updates gates can individually “ignore” parts of the state vector.The update gates act like conditional leaky integrators that can linearly gate any dimension, thus choosing to copy it (at one extreme of the sigmoid) or completely ignore it (at the other extreme) by replacing it by the new “target state” value(towards which the leaky integrator wants to converge). The reset gates control which parts of the state get used to compute the next target state, introducing an additional nonlinear effect in the relationship between past state and future state.
- Clipping Gradients：The difficulty that arises is that when the parameter gradient is very large, a gradient descent parameter update could throw the parameters very far, into a region where the objective function is larger, undoing much of the work that had been done to reach the current solution. The gradient tells us the direction that corresponds to the steepest descent within an infinitesimal region surrounding the current parameters. Outside of this infinitesimal region, the cost function may begin to curve back upwards. The update must be chosen to be small enough to avoid traversing too much upward curvature. We typically use learning rates that decay slowly enough that consecutive steps have approximately the same learning rate. A step size that is appropriate for a relatively linear part of the landscape is often inappropriate and causes uphill motion if we enter a more curved part of the landscape on the next step.
- Gradient clipping can make gradient descent perform more reasonably in the vicinity of extremely steep cliffs.
- Gradient clipping helps to deal with exploding gradients, but it does not help with vanishing gradients. To address vanishing gradients and better capture long-term dependencies, we discussed the idea of creating paths in the computational graph of the unfolded recurrent architecture along which the product of gradients associated with arcs is near 1.One approach to achieve this is with LSTMs and other self-loops and gating mechanisms
- In some applications, it is possible for the machine learning system to refuse to make a decision. This is useful when the machine learning algorithm can estimate how confident it should be about a decision, especially if a wrong decision can be harmful and if a human operator is able to occasionally take over
- A natural performance metric to use in this situation is coverage. Coverage is the fraction of examples for which the machine learning system is able to produce a response
- How does one decide whether to gather more data? First, determine whether the performance on the training set is acceptable. If performance on the training set is poor, the learning algorithm is not using the training data that is already available, so there is no reason to gather more data. Instead, try increasing the size of the model by adding more layers or adding more hidden units to each layer. Also, try improving the learning algorithm, for example by tuning the learning rate hyperparameter. If large models and carefully tuned optimization algorithms do not work well, then the problem might be the quality of the training data. The data may be too noisy or may not include the right inputs needed to predict the desired outputs. This suggests starting over, collecting cleaner data or collecting a richer set of features.
- If the performance on the training set is acceptable, then measure the performance on a test set. If the performance on the test set is also acceptable, then there is nothing left to be done. If test set performance is much worse than training set performance, then gathering more data is one of the most effective solutions. The key considerations are the cost and feasibility of gathering more data, the cost and feasibility of reducing the test error by other means, and the amount of data that is expected to be necessary to improve test set performance significantly. At large internet companies with millions or billions of users, it is feasible to gather large datasets, and the expense of doing so can be considerably less than the other alternatives, so the answer is almost always to gather more training data。A simple alternative to gathering more data is to reduce the size of the model or improve regularization, by adjusting hyperparameters such as weight decay coefficients, or by adding regularization strategies such as dropout. If you find that the gap between train and test performance is still unacceptable even after tuning the regularization hyperparameters, then gathering more data is advisable
- The learning rate is perhaps the most important hyperparameter. If you have time to tune only one hyperparameter, tune the learning rate.
- Typically, a grid search involves picking values approximately on a logarithmic scale, e.g., a learning rate taken within the set {.1, .01, 10−3 , 10−4 , 10−5}, or a number of hidden units taken with the set {50, 100, 200, 500, 1000, 2000}.
- Grid search usually performs best when it is performed repeatedly. For example, suppose that we ran a grid search over a hyperparameter α using values of {−1, 0, 1}. If the best value found is 1, then we underestimated the range in which the best α lies and we should shift the grid and run another search with α in, for example, {1,2,3}. If we find that the best value of α is 0, then we may wish to refine our estimate by zooming in and running a grid search over {−.1, 0, .1}.
- A random search proceeds as follows. First we define a marginal distribution for each hyperparameter, e.g., a Bernoulli or multinoulli for binary or discrete hyperparameters, or a uniform distribution on a log-scale for positive real-valued hyperparameters。Unlike in the case of a grid search, one should not discretize or bin the values of the hyperparameters. This allows one to explore a larger set of values, and does not incur additional computational cost
- The main reason why random search finds good solutions faster than grid search is that the there are no wasted experimental runs, unlike in the case of grid search, when two values of a hyperparameter (given values of the other hyperparameters) would give the same result. In the case of grid search, the other hyperparameters would have the same values for these two runs, whereas with random search, they would usually have different values. Hence if the change between these two values does not marginally make much difference in terms of validation set error, grid search will unnecessarily repeat two equivalent experiments while random search will still give two independent explorations of the other hyperparameters.
- Most debugging strategies for neural nets are designed to get around one or both of these two difficulties. Either we design a case that is so simple that the correct behavior actually can be predicted, or we design a test that exercises one part of the neural net implementation in isolation
- In many cases, we choose to regard natural language as a sequence of words, rather than a sequence of individual characters or bytes
- Depending on how the model is designed, a token may be a word, a character, or even a byte.An n-gram is a sequence of n tokens.Models based on n-grams define the conditional probability of the nth token given the preceding n − 1 tokens. The model uses products of these conditional distributions to define the probability distribution over longer sequences
- Training n-gram models is straightforward because the maximum likelihood estimate can be computed simply by counting how many times each possible n gram occurs in the training set
- word embeddings：we view the raw symbols as points in a space of dimension equal to the vocabulary size. The word representations embed those points in a feature space of lower dimension。In the embedding space, words that frequently appear in similar contexts (or any pair of words sharing some “features” learned by the model) are close to each other. This often results in words with similar meanings being neighbors
- A classical approach to reducing the computational burden of high-dimensional output layers over large vocabulary sets V is to decompose probabilities hierarchically.One can think of this hierarchy as building categories of words, then categories of categories of words, then categories of categories of categories of words, etc. These nested categories form a tree, with words at the leaves. In a balanced tree, the tree has depth O(log |V|). The probability of a choosing a word is given by the product of the probabilities of choosing the branch leading to that word at every node on a path from the root of the tree to the leaf containing the word
- Reinforcement learning requires choosing a tradeoff between exploration and exploitation. Exploitation refers to taking actions that come from the current, best version of the learned policy—actions that we know will achieve a high reward. Exploration refers to taking actions specifically in order to obtain more training data。Many factors determine the extent to which we prefer exploration or exploitation. One of the most prominent factors is the time scale we are interested in. If the agent has only a short amount of time to accrue reward, then we prefer more exploitation. If the agent has a long time to accrue reward, then we begin with more exploration so that future actions can be planned more effectively with more knowledge. As time progresses and our learned policy improves, we move toward more exploitation
- 当使用softmax作为输出层时，使用cross-entropy作为损失函数
- 过拟合：early stopping、regularization、dropout、network structure
- dropout操作方法是, 首先设定一个 dropout ratio σ, σ 是超参数, 范围设置为 (0,1), 表示在 Forward 阶段需要随机断开的连接的比例. 每次 Forward 的时候都要随机的断开该比例的连接, 只更新剩下的 weight. 最后, 在 test/predict 的时候, 使用全部的连接, 不过, 这些 weights 全部都需要乘上 1−σ.Dropout 除了具有防止 overfitting 的作用之外, 还有 model ensemble 的作用.
- CNN之所以用于图片处理是因为图片的三个特点：Some patterns are much smaller than the whole image；The same patterns appear in different regions；Subsampling the pixels will not change the object bird（We can subsample the pixels to make image smaller，Less parameters for the network to process the image）。前两者主要是convolution,第三者是max pooling.所以CNN就是convolution max pooling 的重复进行，最后flatten然后再做一次fully connected feedforward network.filter中的参数就是要学习的目标，所以本质上是Shared weights的
- RNN是有记忆力的，会存储之前的信息，是共享权重的
- LSTM 有四个输入，一个输出：input gate（控制是否输入）,output gate（控制是否输出）,forget gate（控制是否忘记之前的记忆 ），激活函数通常是sigmod function,代表被打开的程度
- 将原来神经网络的节点换为LSTM的cell即可
- bag-of-word 会忽略词的顺序
- RNN中 the output of hidden layer are stored in the memory.memory can be considered as another input.The same network is used again and again
