---
layout:     post
title:      "Spark机器学习"
subtitle:
date:       2016-12-16 20:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 大数据
---

# 第一章 Spark的环境搭建与运行
- Spark从一开始便为应对迭代式应用的高性能需求而设计。在这类应用中，相同的数据会被多次访问。该设计主要靠利用数据集内存缓存以及启动任务时的低延迟和低系统开销来实现高性能
- 任何Spark程序的编写都是从SparkContext开始的
- 一个RDD代表一系列的“记录”（严格来说，某种类型的对象）。这些记录被分配或分区到一个集群的多个节点上（在本地模式下，可以类似地理解为单个进程里的多个线程上）。Spark中的RDD具备容错性，即当某个节点或任务失败时（因非用户代码错误的原因而引起，如硬件故障、网络不通等），RDD会在余下的节点上自动重建，以便任务能最终完成
- 在Spark编程模式下，所有的操作被分为转换(transformation)和执行(action)两种。一般来说，转换操作是对一个数据集里的所有记录执行某种函数，从而使记录发生改变；而执行通常是运行某些计算或聚合操作，并将结果返回运行SparkContext的那个驱动程序
- Spark程序中最常用的转换操作便是map操作。该操作对一个RDD里的每一条记录都执行某个函数，从而将输入映射称为新的输出
- 在Scala中=>表示匿名函数。语法line => line.size表示以=>操作符左边的部分作为输入，对其执行一个函数，并以=>操作符右边代码的执行结果为输出。
- 另一个常见的执行操作是count,来返回RDD中的记录数目
- Spark的大多数操作都会返回一个新RDD，但多数的执行操作则是返回计算的结果。这就意味着多个操作可以很自然地前后连接，从而让代码更为简洁明了
- 值得注意的一点是，Spark中的转换操作是延后的。也就是说，在RDD上调用一个转换操作并不会立即触发相应的计算。相反，这些转换操作会链接起来，并只在有执行操作被调用时才被高效地计算。这样，大部分操作可以在集群上并行执行，只有必要时才计算结果并将其返回给驱动程序，从而提高了Spark的效率。这就意味着，如果我们的Spark程序从未调用一个执行操作，就不会触发实际的计算，也不会得到任何结果
- Spark最为强大的功能之一便是能够把数据缓存在集群的内存里。这通过调用RDD的cache函数来实现。
- 广播变量(broadcast variable)为只读变量，它由运行SparkContext的驱动程序创建后发送给会参与计算的节点。对那些需要让各工作节点高效地访问相同数据的应用场景，比如机器学习，这非常有用。广播变量也可以被非驱动程序所在的节点（即工作节点）访问，访问的方法是调用该变量的value方法
- 通常只在需将结果返回到驱动程序所在节点以供本地处理时，才调用collect函数。注意，collect函数一般仅在的确需要将整个结果集返回驱动程序并进行后续处理时才有必要调用。如果在一个非常大的数据集上调用该函数，可能耗尽驱动程序的可用内存，进而导致程序崩溃。高负荷的处理应尽可能地在整个集群上进行，从而避免驱动程序称为系统瓶颈。然而在不少情况下，将结果收集到驱动程序的确是有必要的。很多机器学习算法的迭代过程便属于这类情况
- 累加器(accumulator)也是一种被广播到工作节点的变量。累加器与广播变量的关键不同，是后者只能读取而前者却可累加。但支持的累加操作有一定的限制。具体来说，这种累加必须是一种有关联的操作，即它得能保证在全局范围内累加起来的值能被正确地并行计算以及返回驱动程序。每一个工作节点只能访问和操作其自己本地的累加器，全局累加器则只允许驱动程序访问。累加器同样可以在Spark代码中通过value访问
- 在实际处理大量数据时，我们通常通过sortByKey这类操作来对其进行并行排序

# 第二章 设计机器学习系统
- 然而我们所处理的通常是大型数据集。这样，先在具有代表性的小样本数据集上进行初步的训练-测试回路，或是尽可能并行地选择模型，都会有所帮助
- 现实中应该同时监控模型准确度相关指标和业务指标
- 模型反馈(model feedback)，指通过用户的行为来对模型的预测进行反馈的过程。在现实系统中，模型的应用将影响用户的决策和潜在行为，从而反过来将从根本上改变模型自己将来的训练数据
- 好在我们可以借助一些机制来降低反馈回路的这种负面影响，比如提供一些无偏见的训练数据。这类数据来自那些没有被推荐的用户，又或者在一开始就考虑到这种平衡需求而划分出来的客户。这些机制有助于对数据的理解、探索以及利用已有的经验来提升系统的表现
- 在线学习在新数据到达时便能立即更新模型，从而使实时系统成为可能。这类方法的优势在于其系统将能对新的信息和底层行为（即输入数据的特征或是分布会随时间变化，现实中的绝大部分情况都会如此）作出快速的反应和调整
- 现实中的实时机器学习系统具有天生的复杂性，故实践中大部分的系统都以近实时性为设计目标。这是一种混合方法，它并不要求模型一定在数据到达时立即更新。相反，新的数据会被收集为小批量的训练数据，再输入给在线学习算法。大部分情况下，该方法会周期性地进行某种批处理。处理的内容可能包括在整个数据集上重新计算模型，或是更为复杂的某些数据处理以及模型的选择。这些能保证实时模型的表现不会随时间推移而变差。
-  另一种类似的方法是，在周期性批处理中进行重新计算时，若有新的数据到来则只对更复杂的模型进行近似更新。这样模型可从新的数据学习，但有短暂延迟。因为是近似更新，所以模型的准确度会随着时间推移而下降。但周期性地在所有数据上重新计算模型能弥补这一点

# 第三章 Spark上数据的获取、处理与准备
- Spark与ipython结合：cd spark  PYSPARK_DRIVER_PYTHON=ipython ./bin/pyspark
- Spark与ipython notebook结合：cd spark PYSPARK_DRIVER_PYTHON=ipython  PYSPARK_DRIVER_PYTHON_OPTS=notebook ./bin/pyspark
- collect():Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.
- count():统计RDD中元素的个数
- countByKey():与count类似，但是是以key为单位进行统计。注意：此函数返回的是一个map，不是int
- countByValue():统计一个RDD中各个value的出现次数。返回一个map，map的key是元素的值，value是出现的次数。
- first函数与collect函数类似，但前者只向驱动程序返回RDD的首个元素。我们也可以使用take(k)函数来只返回RDD的前k个元素到驱动程序
- matplotlib中hist函数中normed=True参数会正则化直方图，即表示百分比
- 对那些可能存在异常值或值域覆盖过大的特征，利用如对数或高斯核对其转换。这类转换有助于降低变量存在的值跳跃的影响，并将非线性关系变为线性的
- 数值特征到类别特征的转换也很常见，比如划分为区间特征。进行这类转换的变量常见的有年龄、地理特征和时间
- zipWithIndex函数将RDD中的元素和这个元素在RDD中的ID（索引号）组合成键/值对
- 正则化特征向量：通常是对数据中的某一行的所有特征进行转换，以让转换后的特征向量的长度标准化。也就是缩放向量中的各个特征以使得向量的范数为1

# 第四章 构建基于Spark的推荐引擎
- 基于用户与物品的方法的得分取决于若干用户或是物品之间依据相似度所构成的集合（即邻居），故它们也常被称为最近邻模型
- 推荐系统和协同过滤模型里常用的两个指标：均方误差(MSE)及K值平均准确率
- 均方误差的定义为各平方误差的和与总数目的商。其中平方误差是指预测到的评级与真实评级的差值的平方
- 均方根误差(RSME)只需在MSE上取平方根即可。
- K值平均准确率(MAPK)的意思是整个数据集上的K值平均准确率(APK)的均值。APK用于衡量针对某个查询所返回的“前K个”文档的平均相关性。对于每次查询，我们会将结果中的前K个与实际相关的文档进行比较
- 当用APK来做评估推荐模型时，每一个用户相当于一个查询，而每一个“前K个”推荐物组成的集合则相当于一个查到的文档结果集合。

# 第五章 Spark构建分类模型
- 对于二分类来说，逻辑回归的输出等价于模型预测某个数据点属于正类的概率估计
- 注意模型预测的值并不是恰好为1或0.预测的输出通常是实数，然后必须转换为预测类别。这是通过在分类器决策函数或打分函数中使用阈值来实现的
- 直觉上来讲，当阈值低于某个程度，模型的预测结果永远会是类别1.因此，模型的召回率为1，但是准确率很可能很低。相反，当阈值足够大，模型的预测结果永远会是类别0.此时，模型的召回率为0，但是因为模型不能预测任何真阳性的样本，很可能会有很多的假阴性样本。不仅如此，因为这种情况下真阳性和假阳性为0，所以无法定义模型的准确率。
- ROC曲线上每一个点代表分类器决策函数中不同的阈值
- 虽然原始特征是稀疏的（大部分维度是0），但对每个项减去均值之后，将得到一个非稀疏（稠密）的特征向量表示。数据规模比较小的时候，稀疏的特征不会产生问题，但实践中往往大规模数据是非常稀疏的。此时，不建议丢失数据的稀疏性，因为相应的稠密表示所需要的内存和计算量将爆炸性增长。此时标准化时只除以标准差，不会减去均值
- 在梯度下降中较大的步长收敛较快，但是步长太大可能导致收敛到局部最优解
- 注意决策树通常不需要特征的标准化和归一化，也不要求将类型特征进行二元编码
- 在实际中，会创建三个数据集：训练集、评估集（用于模型参数的调优）和测试集（用于估计模型在新数据中性能）
- 在交叉验证中，我们一般选择测试集中性能表现最好的参数设置（包括正则化以及步长等各种各样的参数）。然后用这些参数在所有的数据集上重新训练，最后用于新数据集的预测

# 第六章 Spark构建回归模型
- 类似线性回归模型需要使用对应损失函数，决策树在用于回归时也要使用对应的不纯度度量方法。这里的不纯度度量方法是方差，和最小二乘回归模型定义方差损失的方式一样
- MAE（平均绝对误差）是预测值和实际值的差的绝对值的平均值
- RMSLE（均方根对数误差）可以认为是对预测值和目标值进行对数变换后的RMSE。这个度量方法适用于目标变量值域很大，并且没有必要对预测值和目标值的误差进行惩罚的情况。另外，它也适用于计算误差的百分率而不是误差的绝对值
- R-平方系数，也称判定系数，用来评估模型拟合数据的好坏，常用于统计学中。R-平方系数具体测量目标变量的变异度，最终结果为0到1的一个值，1表示模型能够完美契合数据
- 许多机器学习模型都会假设输入数据和目标变量的分布，比如线性模型的假设为正态分布。但实际很多并不符合。一种解决方法是对目标变量进行变换，比如用目标值的对数代替原始数值，通常称为对数变换（这种变换也可以应用到特征值上）。另一种有用的变换是取平方根，适用于目标变量不为负数并且值域很大的情况
- 通常来讲，步长和迭代次数的设定需要权衡。较小的步长意味着收敛速度慢，需要较大的迭代次数。但是较大的迭代次数更加耗时，特别是在大数据上

# 第七章 Spark构建聚类模型
- 通常无监督学习会和监督模型相结合，比如使用无监督技术为监督模型生成输入数据
- 混合模型本质上是模糊K-均值的扩展，但是混合模型假设样本的数据是由某种概率分布产生的。类簇的分布是软分配，所以每个样本由K个概率分布的权重表示
- K-均值和最小方差回归一样使用方差函数作为优化目标，因此容易受到离群值和较大方差的特征影响。对于回归和分类问题来说，上述问题可以通过特征的归一化和标准化来解决，同时可能有助于提升性能。但是某些情况下我们可能不希望数据被标准化，比如根据某个特定的特征找到对应的类簇
- 检验分类结果的具体方法很多，可以通过重点解释每个类簇中靠近类中心的一些点，通常认为选择这些点对所分配的簇争议最小
- 聚类的评估通常分为两部分：内部评估和外部评估。内部评估表示评估过程使用训练模型时使用的训练数据，外部评估则使用训练数据之外的数据
- 通常的内部评价指标包括WCSS、Davies-Bouldin指数、Dunn指数和轮廓系数。所有这些度量指标都是使类簇内部的样本距离尽可能接近，不同类簇的样本相对较远
- 因为聚类被认为是无监督分类，如果有一些带标注的数据，便可以用这些标签来评估聚类模型。可以使用聚类模型预测类簇，使用分类模型中类似的方法评估预测值和真实标签的误差
