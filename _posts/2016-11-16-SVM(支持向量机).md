---
layout: post
title: "SVM（支持向量机）"       # Title of the post
description: SVM（支持向量机）       # Description of the post, used for Facebook Opengraph & Twitter
headline: SVM（支持向量机）     # Will appear in bold letters on top of the post
modified: 2016-11-16                 # Date
category: 机器学习
tags: [SVM]
image:
comments: true
mathjax:
---
# SVM(支持向量机)

当训练数据线性可分时，通过硬间隔最大化，学习一个线性的分类器，即线性可分支持向量机，又称为硬间隔支持向量机；当训练数据近似线性可分时，通过软间隔最大化，也学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机；当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机
> Binary classification schemes, such as support vector machines, can be adapted to handle multiclass classification. This involves constructing an ensemble of binary classifiers. Error-correcting codes can be used to in- crease the accuracy of the ensemble

## 1.线性可分支持向量机与硬间隔最大化
一般地，当训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开。感知机利用误分类最小的策略，求得分离超平面，不过这时的解有无穷多个。线性可分支持向量机利用间隔最大化求最优分离超平面，这时解是唯一的
- **函数间隔**：$$ y(w.x+b) $$
- **几何间隔**：对函数间隔进行规范化，在函数间隔上除以||w||
- 支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面
- 在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为**支持向量**
- **间隔（margin）**即支持向量所在的两条平行于分离超平面之间的距离，等于$$ \dfrac{2}{||w||}$$
> 在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用，而其他实例点并不起作用。如果移动支持向量将改变所求的解；但是如果在间隔边界以外移动其他实例点，甚至去掉这些点，则解是不会改变的。由于支持向量在确定分离超平面中起着决定性作用，所以将这种分类模型称为支持向量机。所以此种分类器的复杂性是由支持向量的数量而非数据的数量决定，所以支持向量机并不容易过拟合

## 2.线性支持向量机与软间隔最大化
- 通常情况下，样本点并非完全线性可分，可能存在一些异常点。
- 通过增加**松弛变量**解决
- 软间隔支持向量或者在间隔边界上，或者在间隔边界与分类超平面之间，或者在分离超平面误分一侧

## 3.非线性支持向量机与核函数
- 用线性分类方法求解非线性分类问题分为两步：首先使用一个变换将原空间的数据映射到新空间；然后在新空间里用线性分类学习方法从训练数据中学习分类模型
- 核技巧的想法是在学习与预测中只定义和函数，而不显式地定义映射函数
-  常用的核函数有线性核、多项式核函数、高斯核函数、Sigmoid核

## 4.SMO算法(待补充)

## 5.libSVM（待补充）

## 6.参考文献
- [机器学习](https://book.douban.com/subject/26708119/)
- [Data Mining](https://book.douban.com/subject/6533777/)
- [统计学习方法](https://book.douban.com/subject/10590856/)
