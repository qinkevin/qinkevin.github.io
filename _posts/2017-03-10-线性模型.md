---
layout: post
title: "线性模型"       # Title of the post
subtitle:
date:       2017-03-10 15:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 机器学习
---

# 线性回归
- 在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小
- 当特征数多于数据时，不可逆，所以不可以再用线性回归，此时可以得到多个解。选择哪一个解作为输出，将由学习算法的归纳偏好觉得，常见的做法是引入正则化。所以ridge regression就可以正则化

# 逻辑回归
- 问题变成了以对数似然函数为目标函数的最优化问题。逻辑回归学习中通常采用的方法是梯度下降及拟牛顿法
- 逻辑回归比较两个条件概率值的大小，将实例分到概率值较大的那一类
- 一个事件的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值
- 在逻辑回归中，输出Y=1的对数几率是输出x的线性函数
- sigmoid在逻辑回归中起到了两个作用，一是将线性函数的结果映射到了(0,1)，一是减少了离群点的影响
- 如果类别之间是否互斥（比如音乐只能属于古典音乐、乡村音乐、摇滚月的一种）就用softmax.否则类别之前有联系（比如一首歌曲可能有影视原声，也可能包含人声，或者是舞曲），这个时候使用k个LR更为合适

# 线性判别分析（LDA）
- LDA的思想非常朴素：给定训练样例集，设法将样例投影到一条直线上使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离；在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别
- 用于监督降维

# 多分类学习
- 有些二分类学习方法可以直接推广到多分类，但在更多情形下，我们是基于一些基本策略，利用二分类学习器来解决多分类问题
- 多分类学习的基本思路是“拆解法”，即将多分类任务拆为若干个二分类任务求解。具体来说，先对问题进行拆分，然后为拆出的每个二分类任务训练一个分类器；在测试时，对这些分类器的预测结果进行集成以获得最终的多分类结果.
- 逻辑回归可直接推广到多分类
- 最经典的拆分策略有三种：一对一(OvO)、一对其余(OvR)、多对多(MvM)。OvO将N个类别两两匹配，从而产生N（N-1）/2个二分类任务，最终结果通过投票产生。OvR则是每次将一个类的样例作为正例、所有其他类的样例作为反例来训练N个分类器。在测试时若仅有一个分类器预测为正类，则对应的类别标记作为最终分类结果

# 最大熵模型
- 最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型
- 直观的，最大熵原理认为要选择的概率模型首先必须满足已有的事实，即约束条件。在没有更多信息的情况下，那些不确定的部分都是“等可能的”。最大熵原理通过熵的最大化来表示等可能性。“等可能”不容易操作，而熵则是一个可优化的数值指标
- 满足约束条件的模型仍有无穷多个。学习的目的是在可能的模型集合中选择最优模型，而最大熵原理则给出最优模型选择的一个准则
- 最大熵模型也可以用于二类或多类分类
- 最大熵模型的优点有：
a) 最大熵统计模型获得的是所有满足约束条件的模型中信息熵极大的模型,作为经典的分类模型时准确率较高。
b) 可以灵活地设置约束条件，通过约束条件的多少可以调节模型对未知数据的适应度和对已知数据的拟合程度
- 最大熵模型的缺点有：
a) 由于约束函数数量和样本数目有关系，导致迭代过程计算量巨大，实际应用比较难。

# L1和L2正则化
- l1(lasso)会导致系数趋近于0，使得权重稀疏，所以可以用来做特征选择
- l2(ridge)会使得不可逆变得可逆，依然凸函数，可以达到全局最优

# 逻辑回归与最大熵模型
- 最大熵与逻辑回归模型有类似的形式，它们又称为对数线性模型。模型学习就是在给定的训练数据条件下对模型进行极大似然估计或正则化的极大似然估计
- 逻辑回归模型、最大熵模型学习归结为以似然函数为目标函数的最优化问题，通常通过迭代算法求解。从最优化的观点看，这时的目标函数具有很好的性质。它是光滑的凸函数，因此多种最优化的方法都适用，保证能找到全局最优解

# 逻辑回归于linear SVM对比
## 相同点:
- 都是分类算法
- 都是监督学习算法
- 都是判别模型
- 都能通过核函数方法针对非线性情况分类
- 目标都是找一个分类超平面
- 都能减少离群点的影响

## 不同点:
- 损失函数不同，逻辑回归是cross entropy loss，svm是hinge loss
- 逻辑回归在优化参数时所有样本点都参与了贡献，svm则只取离分离超平面最近的支持向量样本。这也是为什么逻辑回归不用核函数，它需要计算的样本太多。并且由于逻辑回归受所有样本的影响，当样本不均衡时需要平衡一下每一类的样本个数。
- 逻辑回归对概率建模，svm对分类超平面建模
- 逻辑回归是处理经验风险最小化，svm是结构风险最小化。这点体现在svm自带L2正则化项，逻辑回归并没有
- 逻辑回归通过非线性变换减弱分离平面较远的点的影响，svm则只取支持向量从而消去较远点的影响
- 逻辑回归是统计方法，svm是几何方法
