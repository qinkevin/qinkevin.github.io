---
layout: post
title: "KNN"       # Title of the post
subtitle:   
date:       2016-11-19 12:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 机器学习
---

# kNN（k近邻）

与前面介绍的学习方法相比，kNN并没有显式的训练过程，是“懒惰学习”的代表。“懒惰学习”在训练阶段仅仅是把样本保存起来，训练时间开销为零，待收到测试样本后再进行处理；相应的，那些在训练阶段就对样本进行学习处理的方法，称为“急切学习”

## 1.k近邻算法

k近邻算法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测
- 在应用中，k值一般取一个比较小的数值。通常采用交叉验证法来选取最优的k值
- 距离计算前要归一化
- 名义变量距离的计算一般比较是否相同，相同为0，不同为1
- 缺失值距离计算，一般假设为最大，即1
- kNN基于距离计算，可能会受到噪声或无关属性的影响
- KNN的k选取的时候，一般选为奇数，投票法才可以得出结论
- KNN算法如果采取多数表决方法，每个近邻对分类的影响都一样，这使得算法对K的选择很敏感。降低K的影响的一种途径就是根据每个最近邻距离的不同对其作用加权，结果使得远离的训练样例对分类的影响要比那些靠近的训练样例弱一些。

KNN的回归:在找到最近的k个实例之后，可以计算这k个实例的平均值作为预测值。或者还可以给这k个实例添加一个权重再求平均值，这个权重与度量距离成反比（越近权重越大）。

## 2.kd树（待补充）

k近邻最简单的实现方法是线性扫描。这时要计算输入实例与每一个训练实例的距离。当训练集很大时，计算非常耗时，这种方法是不可行的。
KD树是一个二叉树，表示对K维空间的一个划分，可以进行快速检索（那KNN计算的时候不需要对全样本进行距离的计算了）

### 2.1构造KD树
在k维的空间上循环找子区域的中位数进行划分的过程。
假设现在有K维空间的数据集T={x1,x2,x3,…xn},xi={a1,a2,a3..ak}
- 首先构造根节点，以坐标a1的中位数b为切分点，将根结点对应的矩形局域划分为两个区域，区域1中a1b
- 构造叶子节点，分别以上面两个区域中a2的中位数作为切分点，再次将他们两两划分，作为深度1的叶子节点，（如果a2=中位数，则a2的实例落在切分面）
- 不断重复2的操作，深度为j的叶子节点划分的时候，索取的ai 的i=j%k+1，直到两个子区域没有实例时停止

### 2.2KD树的搜索
- 首先从根节点开始递归往下找到包含x的叶子节点，每一层都是找对应的xi
- 将这个叶子节点认为是当前的“近似最近点”
- 递归向上回退，如果以x圆心，以“近似最近点”为半径的球与根节点的另一半子区域边界相交，则说明另一半子区域中存在与x更近的点，则进入另一个子区域中查找该点并且更新”近似最近点“
- 重复3的步骤，直到另一子区域与球体不相交或者退回根节点
- 最后更新的”近似最近点“与x真正的最近点


### 2.3KD树进行KNN查找
通过KD树的搜索找到与搜索目标最近的点，这样KNN的搜索就可以被限制在空间的局部区域上了，可以大大增加效率。

### 2.4KD树搜索的复杂度
当实例随机分布的时候，搜索的复杂度为log(N)，N为实例的个数，KD树更加适用于实例数量远大于空间维度的KNN搜索，如果实例的空间维度与实例个数差不多时，它的效率基于等于线性扫描。

## 3.优缺点
优点是简单容易理解，但存在维度诅咒和过拟合问题

## 3.参考文献
- [机器学习](https://book.douban.com/subject/26708119/)
- [Data Mining](https://book.douban.com/subject/6533777/)
- [统计学习方法](https://book.douban.com/subject/10590856/)
