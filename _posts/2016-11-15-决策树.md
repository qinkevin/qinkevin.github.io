---
layout: post
title: "决策树"       # Title of the post
subtitle:  
date:       2016-11-19 12:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 机器学习
---

# 决策树

## 1.基本流程
决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略是以损失函数为目标函数的最小化。决策树学习算法通常采用启发式方法，近似求解这一最优化问题。所以决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程

>在决策树基本算法中，有三种情形会导致递归返回：（1）当前结点包含的样本全属于同一类别，无需划分；（2）当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；（3）当前结点包含的样本集合为空，不能划分

**ID3、C4.5 、CART都是基于内存的决策树算法，适用于小数据**

## 2.划分选择
>即如何选择最优划分属性。划分的目的是希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”(purity)越来越高

### 2.1信息增益
 - 使用信息熵来度量样本集合纯度。熵越大，随机变量的不确定性就越大
 - 信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度
 - 一般而言，信息增益越大，则意味着使用该属性来进行划分所获得的“纯度提升”越大
 - ID3决策树学习算法使用信息增益为准则来选择划分属性
 - 当模型使用信息增益选择变量进行拆分时，已经自动地选择了对模型本身而言最为重要的变量。因此，变量的选择过程已经巧妙地嵌入了决策树模型的搭建过程

### 2.2信息增益率
- 信息增益准则对可取值数目较多的属性有所偏好，所以使用信息增益率来减少这种偏好带来的不利影响
- 信息增益率是该特征的信息增益除以该特征的IV值
- C4.5使用增益率来选择最优划分属性
- 增益率对可取值数目较少的属性有所偏好，因此，C4.5并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的

### 2.3基尼指数
- CART使用基尼指数来划分最优划分属性
- 使用基尼值来度量数据集的纯度
- **基尼指数只能用于二分划分**
- 在CART算法中,基尼不纯度表示一个随机选中的样本在子集中被分错的可能性。基尼不纯度为这个样本被选中的概率乘以它被分错的概率。当一个节点中所有样本都是一个类时，基尼不纯度为零。因此选择基尼指数最小的属性作为最优划分属性。

### 2.4连续值处理
- 上述主要讨论基于离散属性生成决策树，当遇到连续属性时需要使用连续属性离散化技术。最简单的策略就是采用二分法对连续属性进行处理，将所有取值从小到大排序，把两两的中位点作为候选划分点，即视为离散取值。然后我们就可像离散属性值一样来考察这些划分点，选取最优的划分点进行样本集合的划分
>需要注意的是，与离散属性不同，若当前节点划分属性为连续属性，该属性还可作为其后代节点的划分属性

### 2.5缺失值处理
- 在决策树中，如果我们缺失了某些数据，而这些数据是确定分支走向所必须的，那么实际上我们可以选择两个分支都走。不过，此处我们不是平均地统计各分支对应的结果值，而是对其进行加权统计。在一棵基本的决策树中，所有节点都隐含有一个值为1的权重，即观测数据对于数据项是否属于某个特定分类的概率具有百分之百的影响。而如果要走多个分支的话，那么我们可以给每个分支赋以一个权重，其值等于所有位于该分支的其他数据行所占的比重

### 2.6多变量决策树
- 多变量决策树的非叶节点不再是仅对某个属性，而是对**属性的线性组合**进行测试
- 在多变量决策树的学习过程中，不是为每个非叶结点寻找一个最优划分属性，而是试图建立一个合适的线性分类器
>决策树的测试条件每次可以涉及多个属性，斜决策树可以使用多个属性。构造归纳则通过由原始属性创建复合特征，简化了学习复杂的划分函数的任务

### 2.7回归树
- A linear model tree is a decision tree with a linear functional model in each leaf, whereas in classical regression tree it is the sample mean of the response for statistical units in each leaf(hence, a constant) tree is being considered.

### 2.8CART
- 对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则，进行特征选择

## 3.剪枝处理

### 3.1预剪枝
>预剪枝是指在决策树生成过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能提升，则停止划分并将当前节点标记为叶节点

- 预剪枝使得决策树的很多分支都没有“展开”，这不仅降低了过拟合的风险，还显著减少了决策树的训练时间开销和测试时间开销。
- 但另一方面，有些分支的当前划分虽不能提升泛化性能、甚至可能导致泛化性能暂时下降，但在其基础上进行的后续划分却有可能导致性能显著提高
- 预剪枝基于“贪心”本质禁止这些分支展开，给预剪枝决策树带来了**欠拟合**的风险

### 3.2后剪枝
>后剪枝则是先从训练集生成一颗完整的决策树，然后自底向上地对非叶节点进行考察，若将该节点对应的子树替换为叶节点能带来决策树泛化性能提升，则将该子树替换为叶节点

- 一般情形下，后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树
- 但后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中的所有非叶结点进行逐一考察，因此**其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多**

## 4.优缺点
- 其主要优点是模型具有可读性，分类速度快
- 最大的缺点是容易过拟合，且可能或陷于局部最小值中
- 与其他几种机器学习算法不同，决策树可以同时接受分类数据和数值数据作为输入
- 决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。决策树的生成只考虑局部最优，相对地，决策树的剪枝则考虑全局最优
- 总之，对于有大量数值型输入和输出的问题，决策树未必是一个好的选择；如果数值型输入之间存在许多错综复杂的关系，决策树同样也不一定是很好的选择。决策树最适合用来处理的，是那些带分界点的、由大量分类数据和数值数据共同组成的数据集。
- 决策树实际上是将空间用超平面进行划分的一种方法，每次分割的时候，都将当前的空间一分为二。这样使得每一个叶子节点都是在空间中的一个不相交的区域，在进行决策的时候，会根据输入样本每一维feature的值，一步一步往下，最后使得样本落入N个区域中的一个（假设有N个叶子节点）


## 5.参考文献
- [机器学习](https://book.douban.com/subject/26708119/)
- [Data Mining](https://book.douban.com/subject/6533777/)
- [统计学习方法](https://book.douban.com/subject/10590856/)
- [sklearn Decision Trees](http://scikit-learn.org/stable/modules/tree.html)
