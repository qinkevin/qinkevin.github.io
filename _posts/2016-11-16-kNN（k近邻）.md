---
layout: post
title: "KNN"       # Title of the post
description: KNN       # Description of the post, used for Facebook Opengraph & Twitter
headline: KNN      # Will appear in bold letters on top of the post
modified: 2016-11-16                 # Date
category: 机器学习
tags: [机器学习]
image:
comments: true
mathjax:
---

# kNN（k近邻）

与前面介绍的学习方法相比，kNN并没有显式的训练过程，是“懒惰学习”的代表。“懒惰学习”在训练阶段仅仅是把样本保存起来，训练时间开销为零，待收到测试样本后再进行处理；相应的，那些在训练阶段就对样本进行学习处理的方法，称为“急切学习”

## 1.k近邻算法

k近邻算法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测
- 在应用中，k值一般取一个比较小的数值。通常采用交叉验证法来选取最优的k值
- 距离计算前要归一化
- 名义变量距离的计算一般比较是否相同，相同为0，不同为1
- 缺失值距离计算，一般假设为最大，即1
- kNN基于距离计算，可能会受到噪声或无关属性的影响
- KNN的k选取的时候，一般选为奇数，投票法才可以得出结论
- KNN算法如果采取多数表决方法，每个近邻对分类的影响都一样，这使得算法对K的选择很敏感。降低K的影响的一种途径就是根据每个最近邻距离的不同对其作用加权，结果使得远离的训练样例对分类的影响要比那些靠近的训练样例弱一些。

## 2.kd树（待补充）

k近邻最简单的实现方法是线性扫描。这时要计算输入实例与每一个训练实例的距离。当训练集很大时，计算非常耗时，这种方法是不可行的。

## 3.参考文献
- [机器学习](https://book.douban.com/subject/26708119/)
- [Data Mining](https://book.douban.com/subject/6533777/)
- [统计学习方法](https://book.douban.com/subject/10590856/)
