---
layout: post
title: "hadoop权威指南"       # Title of the post
subtitle:
modified: 2016-11-22                 # Date
date:       2016-11-22 12:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 大数据
---

# chapter 0
[在Mac OSX Yosemite上安装Hadoop](http://www.jianshu.com/p/3aebdba32363)

# chapter 1 Meet Hadoop
- For all its strengths,MapReduce is fundamentally a batch processing system,and is not suitable for interactive analysis,so it's best for offline use
- HBase provides both online read/write access of individual rows and batch operations for reading and writing data in bulk,making it a good solution for building applications on
- YARN is a cluster resource management system,which allows any distributed program(not just MapReduce) to run on data in a Hadoop cluster
- MapReduce is a good fit for problems that need to analyze the whole dataset in a batch fashion,particularly for ad hoc analysis.An RDBMS is good for point queries or updates,where the dataset has been indexed to deliver low-latency retrieval and  update times of a relatively small amount of data.MapReduce suits applications where the data is written once and read many times,whereas a relational database is good for datasets that are continually updated.
- Hadoop works well on unstructured or semi-structured data because it is designed to interpret the data at processing time (so called schema-on-read). This provides flexibility and avoids the costly data loading phase of an RDBMS, since in Hadoop it is just a file copy

# chapter 2 MapReduce

- MapReduce is a programming model for data processing
- MapReduce works by breaking the processing into two phases: the map phase and the reduce phase.Each phase has key-value pairs as input and output,the types of which may be chosen by the programmer.The programmer also specifies two functions:the map function and the reduce function
- The output from the map function is processed by the MapReduce framework before being sent to the reduce function.This processing sorts and groups the key-value pairs by key
- The input types of the reduce function must match the output types of the map function
- Hadoop runs the job by dividing it into tasks, of which there are two types: map tasks and reduce tasks. The tasks are scheduled using YARN and run on nodes in the cluster. If a task fails, it will be automatically rescheduled to run on a different node
- Hadoop divides the input to a MapReduce job into fixed-size pieces called input splits, or just splits. Hadoop creates one map task for each split, which runs the user-defined map function for each record in the split.
- On the other hand, if splits are too small, the overhead of managing the splits and map task creation begins to dominate the total job execution time. For most jobs, a good split size tends to be the size of an HDFS block, which is 128 MB by default, although this can be changed for the cluster (for all newly created files) or specified when each file is created.
- Hadoop does its best to run the map task on a node where the input data resides in HDFS, because it doesn’t use valuable cluster bandwidth. This is called the data locality optimization
- It should now be clear why the optimal split size is the same as the block size: it is the largest size of input that can be guaranteed to be stored on a single node. If the split spanned two blocks, it would be unlikely that any HDFS node stored both blocks, so some of the split would have to be transferred across the network to the node running the map task, which is clearly less efficient than running the whole map task using local data.
- Map tasks write their output to the local disk, not to HDFS. Why is this? Map output is intermediate output: it’s processed by reduce tasks to produce the final output, and once the job is complete, the map output can be thrown away. So, storing it in HDFS with replication would be overkill.
- The output of the reduce is normally stored in HDFS for reliability
- Many MapReduce jobs are limited by the bandwidth available on the cluster, so it pays to minimize the data transferred between map and reduce tasks. Hadoop allows the user to specify a combiner function to be run on the map output, and the combiner function’s output forms the input to the reduce function
- The combiner function doesn’t replace the reduce function.But it can help cut down the amount of data shuffled between the mappers and the reducers, and for this reason alone it is always worth considering whether you can use a combiner function in your MapReduce job
- Hadoop Streaming uses Unix standard streams as the interface between Hadoop and your program, so you can use any language that can read standard input and write to standard output to write your MapReduce program

# chapter 3 HDFS

- Applications that require low-latency access to data, in the tens of milliseconds range, will not work well with HDFS. Remember, HDFS is optimized for delivering a high throughput of data, and this may be at the expense of latency.HBase is currently a better choice for low-latency access
- Files in HDFS may be written to by a single writer. Writes are always made at the end of the file, in append-only fashion. There is no support for multiple writers or for modifications at arbitrary offsets in the file
- HDFS blocks are large compared to disk blocks, and the reason is to minimize the cost of seeks. If the block is large enough, the time it takes to transfer the data from the disk can be significantly longer than the time to seek to the start of the block. Thus, trans‐ ferring a large file made of multiple blocks operates at the disk transfer rate
- An HDFS cluster has two types of nodes operating in a master−worker pattern: a namenode (the master) and a number of datanodes (workers).
-Without the namenode, the filesystem cannot be used. In fact, if the machine running the namenode were obliterated, all the files on the filesystem would be lost since there would be no way of knowing how to reconstruct the files from the blocks on the datanodes. For this reason, it is important to make the namenode resilient to failure, and Hadoop provides two mechanisms for this
- HDFS federation, introduced in the 2.x release series, allows a cluster to scale by adding namenodes, each of which manages a portion of the filesystem namespace.
- Hadoop 2 remedied this situation by adding support for HDFS high availability (HA). In this implementation, there are a pair of namenodes in an active-standby configura‐ tion. In the event of the failure of the active namenode, the standby takes over its duties to continue servicing client requests without a significant interruption
- The transition from the active namenode to the standby is managed by a new entity in the system called the failover controller
- Hadoop provides many interfaces to its filesystems, and it generally uses the URI scheme to pick the correct filesystem instance to communicate with
- When copying data into HDFS, it’s important to consider cluster balance. HDFS works best when the file blocks are evenly spread across the cluster
