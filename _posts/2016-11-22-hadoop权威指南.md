---
layout: post
title: "hadoop权威指南"       # Title of the post
subtitle:
date:       2016-11-22 12:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 大数据
---

# chapter 0
[在Mac OSX Yosemite上安装Hadoop](http://www.jianshu.com/p/3aebdba32363)

# chapter 1 Meet Hadoop
- For all its strengths,MapReduce is fundamentally a batch processing system,and is not suitable for interactive analysis,so it's best for offline use
- HBase provides both online read/write access of individual rows and batch operations for reading and writing data in bulk,making it a good solution for building applications on
- YARN is a cluster resource management system,which allows any distributed program(not just MapReduce) to run on data in a Hadoop cluster
- MapReduce is a good fit for problems that need to analyze the whole dataset in a batch fashion,particularly for ad hoc analysis.An RDBMS is good for point queries or updates,where the dataset has been indexed to deliver low-latency retrieval and  update times of a relatively small amount of data.MapReduce suits applications where the data is written once and read many times,whereas a relational database is good for datasets that are continually updated.
- Hadoop works well on unstructured or semi-structured data because it is designed to interpret the data at processing time (so called schema-on-read). This provides flexibility and avoids the costly data loading phase of an RDBMS, since in Hadoop it is just a file copy

# chapter 2 MapReduce
- MapReduce is a programming model for data processing
- MapReduce works by breaking the processing into two phases: the map phase and the reduce phase.Each phase has key-value pairs as input and output,the types of which may be chosen by the programmer.The programmer also specifies two functions:the map function and the reduce function
- The output from the map function is processed by the MapReduce framework before being sent to the reduce function.This processing sorts and groups the key-value pairs by key
- The input types of the reduce function must match the output types of the map function
- Hadoop runs the job by dividing it into tasks, of which there are two types: map tasks and reduce tasks. The tasks are scheduled using YARN and run on nodes in the cluster. If a task fails, it will be automatically rescheduled to run on a different node
- Hadoop divides the input to a MapReduce job into fixed-size pieces called input splits, or just splits. Hadoop creates one map task for each split, which runs the user-defined map function for each record in the split.
- On the other hand, if splits are too small, the overhead of managing the splits and map task creation begins to dominate the total job execution time. For most jobs, a good split size tends to be the size of an HDFS block, which is 128 MB by default, although this can be changed for the cluster (for all newly created files) or specified when each file is created.
- Hadoop does its best to run the map task on a node where the input data resides in HDFS, because it doesn’t use valuable cluster bandwidth. This is called the data locality optimization
- It should now be clear why the optimal split size is the same as the block size: it is the largest size of input that can be guaranteed to be stored on a single node. If the split spanned two blocks, it would be unlikely that any HDFS node stored both blocks, so some of the split would have to be transferred across the network to the node running the map task, which is clearly less efficient than running the whole map task using local data.
- Map tasks write their output to the local disk, not to HDFS. Why is this? Map output is intermediate output: it’s processed by reduce tasks to produce the final output, and once the job is complete, the map output can be thrown away. So, storing it in HDFS with replication would be overkill.
- The output of the reduce is normally stored in HDFS for reliability
- Many MapReduce jobs are limited by the bandwidth available on the cluster, so it pays to minimize the data transferred between map and reduce tasks. Hadoop allows the user to specify a combiner function to be run on the map output, and the combiner function’s output forms the input to the reduce function
- The combiner function doesn’t replace the reduce function.But it can help cut down the amount of data shuffled between the mappers and the reducers, and for this reason alone it is always worth considering whether you can use a combiner function in your MapReduce job
- Hadoop Streaming uses Unix standard streams as the interface between Hadoop and your program, so you can use any language that can read standard input and write to standard output to write your MapReduce program

# chapter 3 HDFS
- Applications that require low-latency access to data, in the tens of milliseconds range, will not work well with HDFS. Remember, HDFS is optimized for delivering a high throughput of data, and this may be at the expense of latency.HBase is currently a better choice for low-latency access
- Files in HDFS may be written to by a single writer. Writes are always made at the end of the file, in append-only fashion. There is no support for multiple writers or for modifications at arbitrary offsets in the file
- HDFS blocks are large compared to disk blocks, and the reason is to minimize the cost of seeks. If the block is large enough, the time it takes to transfer the data from the disk can be significantly longer than the time to seek to the start of the block. Thus, transferring a large file made of multiple blocks operates at the disk transfer rate
- An HDFS cluster has two types of nodes operating in a master−worker pattern: a namenode (the master) and a number of datanodes (workers).
-Without the namenode, the filesystem cannot be used. In fact, if the machine running the namenode were obliterated, all the files on the filesystem would be lost since there would be no way of knowing how to reconstruct the files from the blocks on the datanodes. For this reason, it is important to make the namenode resilient to failure, and Hadoop provides two mechanisms for this
- HDFS federation, introduced in the 2.x release series, allows a cluster to scale by adding namenodes, each of which manages a portion of the filesystem namespace.
- Hadoop 2 remedied this situation by adding support for HDFS high availability (HA). In this implementation, there are a pair of namenodes in an active-standby configuration. In the event of the failure of the active namenode, the standby takes over its duties to continue servicing client requests without a significant interruption
- The transition from the active namenode to the standby is managed by a new entity in the system called the failover controller
- Hadoop provides many interfaces to its filesystems, and it generally uses the URI scheme to pick the correct filesystem instance to communicate with
- When copying data into HDFS, it’s important to consider cluster balance. HDFS works best when the file blocks are evenly spread across the cluster
- 分布式文件系统不会有缓存
- 分布式文件系统简化了API，不会有单机文件系统那么多的操作
- HDFS吞吐量较大，响应时间较长，适用于大文件，不适用于小文件
- 多次读取，一次写入，写入只能在文件末尾
- 块大小为64MB，使得传输时间是寻道时间的1%

# chapter 4 YARN
- Apache YARN (Yet Another Resource Negotiator) is Hadoop’s cluster resource management system
- YARN provides its core services via two types of long-running daemon: a resource manager (one per cluster) to manage the use of resources across the cluster, and node managers running on all the nodes in the cluster to launch and monitor containers. A container executes an application-specific process with a constrained set of resources (memory, CPU, and so on)
- A YARN application can make resource requests at any time while it is running
- It is the job of the YARN scheduler to allocate resources to applications according to some defined policy
- Three schedulers are available in YARN: the FIFO, Capacity, and Fair Schedulers
- FIFO 调度方式对短作业不公平，因为长作业会长时间占有
- The Capacity Scheduler allows sharing of a Hadoop cluster along organizational lines, whereby each organization is allocated a certain capacity of the overall cluster. Each organization is set up with a dedicated queue that is configured to use a given fraction of the cluster capacity. Queues may be further divided in hierarchical fashion, allowing each organization to share its cluster allowance between different groups of users within the organization. Within a queue, applications are scheduled using FIFO scheduling.
- The Fair Scheduler attempts to allocate resources so that all running applications get the same share of resources
- When using delay scheduling, the scheduler doesn’t simply use the first scheduling opportunity it receives, but waits for up to a given maximum number of scheduling opportunities to occur before loosening the locality constraint and taking the next scheduling opportunity

# chapter 5 Hadoop I/O
- The usual way of detecting corrupted data is by computing a checksum for the data when it first enters the system, and again whenever it is transmitted across a channel that is unreliable and hence capable of corrupting the data. The data is deemed to be corrupt if the newly generated checksum doesn’t exactly match the original
- Datanodes are responsible for verifying the data they receive before storing the data and its checksum.This applies to data that they receive from clients and from other datanodes during replication
- When clients read data from datanodes, they verify checksums as well, comparing them with the ones stored at the datanodes
- Because HDFS stores replicas of blocks, it can “heal” corrupted blocks by copying one of the good replicas to produce a new, uncorrupt replica.The way this works is that if a client detects an error when reading a block, it reports the bad block and the datanode it was trying to read from to the namenode before throwing a ChecksumException. The namenode marks the block replica as corrupt so it doesn’t direct any more clients to it or try to copy this replica to another datanode. It then schedules a copy of the block to be replicated on another datanode, so its replication factor is back at the expected level. Once this has happened, the corrupt replica is deleted
- File compression brings two major benefits: it reduces the space needed to store files, and it speeds up data transfer across the network or to or from disk
- All compression algorithms exhibit a space/time trade-off: faster compression and de‐ compression speeds usually come at the expense of smaller space savings.
- splitting is whether you can seek to any point in the stream and start reading from some point further on. Splittable compression formats are especially suitable for Map‐ Reduce
- When considering how to compress data that will be processed by MapReduce, it is important to understand whether the compression format supports splitting
- For large files, you should not use a compression format that does not support splitting on the whole file, because you lose locality and make MapReduce applications very inefficient.
- Even if your MapReduce application reads and writes uncompressed data, it may benefit from compressing the intermediate output of the map phase.The map output is written to disk and transferred across the network to the reducer nodes, so by using a fast compressor such as LZO, LZ4, or Snappy, you can get performance gains simply because the volume of data to transfer is reduced
- Serialization is the process of turning structured objects into a byte stream for trans‐ mission over a network or for writing to persistent storage. Deserialization is the reverse process of turning a byte stream back into a series of structured objects
- How do you choose between a fixed-length and a variable-length encoding? Fixed- length encodings are good when the distribution of values is fairly uniform across the whole value space, such as when using a (well-designed) hash function. Most numeric variables tend to have nonuniform distributions, though, and on average, the variable- length encoding will save space. Another advantage of variable-length encodings is that you can switch from VIntWritable to VLongWritable, because their encodings are actually the same. So, by choosing a variable-length representation, you have room to grow without committing to an 8-byte long representation from the beginning
- A MapFile is a sorted SequenceFile with an index to permit lookups by key
- In general, column-oriented formats work well when queries access only a small number of columns in the table. Conversely, row- oriented formats are appropriate when a large number of columns of a single row are needed for processing at the same time.

# chapter 6 Developing a MapReduce Application
- System properties take priority over properties defined in resource files
- When the processing gets more complex, this complexity is generally manifested by having more MapReduce jobs, rather than having more complex map and reduce functions. In other words, as a rule of thumb, think about adding more jobs, rather than adding complexity to jobs
- Apache Oozie is a system for running workflows of dependent jobs. It is composed of two main parts: a workflow engine that stores and runs workflows composed of different types of Hadoop jobs (MapReduce, Pig, Hive, and so on), and a coordinator engine that runs workflow jobs based on predefined schedules and data availability
- In Oozie parlance, a workflow is a DAG of action nodes and control-flow nodes.An action node performs a workflow task, such as moving files in HDFS; running a MapReduce, Streaming, Pig, or Hive job; performing a Sqoop import; or running an arbitrary shell script or Java program. A control-flow node governs the workflow exe‐ cution between actions by allowing such constructs as conditional logic (so different execution branches may be followed depending on the result of an earlier action node) or parallel execution. When the workflow completes, Oozie can make an HTTP callback to the client to inform it of the workflow status. It is also possible to receive callbacks every time the workflow enters or exits an action node

# chapter 7 How MapReduce Works
- The Streaming task communicates with the process (which may be written in any language) using standard input and output streams. During execution of the task, the Java process passes input key-value pairs to the external process, which runs it through the user-defined map or reduce function and passes the output key-value pairs back to the Java process. From the node manager’s point of view, it is as if the child process ran the map or reduce code itself.
- Node managers may be blacklisted if the number of failures for the application is high, even if the node manager itself has not failed. Blacklisting is done by the application master, and for MapReduce the application master will try to reschedule tasks on different nodes if more than three tasks fail on a node manager.
- To achieve high availability (HA), it is necessary to run a pair of resource managers in an active-standby configuration. If the active resource manager fails, then the standby can take over without a significant interruption to the client
- MapReduce makes the guarantee that the input to every reducer is sorted by key. The process by which the system performs the sort—and transfers the map outputs to the reducers as inputs—is known as the shuffle.
- On the map side, the best performance can be obtained by avoiding multiple spills to disk; one is optimal
- On the reduce side, the best performance is obtained when the intermediate data can reside entirely in memory.
- Hadoop doesn’t try to diagnose and fix slow-running tasks; instead, it tries to detect when a task is running slower than expected and launches another equivalent task as a backup. This is termed speculative execution of tasks.
- Rather, the scheduler tracks the progress of all tasks of the same type (map and reduce) in a job, and only launches speculative duplicates for the small proportion that are running significantly slower than the average. When a task completes successfully, any duplicate tasks that are running are killed since they are no longer needed
- Speculative execution is an optimization, and not a feature to make jobs run more reliably. If there are bugs that sometimes cause a task to hang or slow down, relying on speculative execution to avoid these problems is unwise and won’t work reliably, since the same bugs are likely to affect the speculative task.
- Hadoop MapReduce uses a commit protocol to ensure that jobs and tasks either succeed or fail cleanly.

# chapter 8 MapReduce Types and Formats
- The default partitioner is HashPartitioner, which hashes a record’s key to determine which partition the record belongs in. Each partition is processed by a reduce task, so the number of partitions is equal to the number of reduce tasks for the job
- an input split is a chunk of the input that is processed by a single map. Each map processes a single split. Each split is divided into records, and the map processes each record—a key-value pair—in turn
- Notice that a split doesn’t contain the input data; it is just a reference to the data. The storage locations are used by the MapReduce system to place map tasks as close to the split’s data as possible, and the size is used to order the splits so that the largest get processed first, in an attempt to minimize the job runtime
- The split size is normally the size of an HDFS block
- Hadoop works better with a small number of large files than a large number of small files.

# chapter 9 MapReduce Features
- Counters are a useful channel for gathering statistics about the job: for quality control or for application-level statistics. They are also useful for problem diagnosis
- Task counters gather information about tasks over the course of their execution, and the results are aggregated over all the tasks in a job
- Job counters are maintained by the application master, so they don’t need to be sent across the network, unlike all other counters, including user-defined ones. They measure job-level statistics, not values that change while a task is running
- A map-side join between large inputs works by performing the join before the data reaches the map function. For this to work, though, the inputs to each map must be partitioned and sorted in a particular way. Each input dataset must be divided into the same number of partitions, and it must be sorted by the same key (the join key) in each source. All the records for a particular key must reside in the same partition. This may sound like a strict requirement (and it is), but it actually fits the description of the output of a MapReduce job
- A map-side join can be used to join the outputs of several jobs that had the same number of reducers, the same keys, and output files that are not splittable
- A reduce-side join is more general than a map-side join, in that the input datasets don’t have to be structured in any particular way, but it is less efficient because both datasets have to go through the MapReduce shuffle. The basic idea is that the mapper tags each record with its source and uses the join key as the map output key, so that the records with the same key are brought together in the reducer
- Side data can be defined as extra read-only data needed by a job to process the main dataset. The challenge is to make side data available to all the map or reduce tasks (which are spread across the cluster) in a convenient and efficient fashion.

# chapter 10 Setting Up a Hadoop Cluster
- HDFS clusters do not benefit from using RAID (redundant array of independent disks) for datanode storage (although RAID is recommended for the namenode’s disks, to protect against corruption of its metadata). The redundancy that RAID provides is not needed, since HDFS handles it by replication between nodes.
- The namenode has high memory requirements, as it holds file and block metadata for the entire namespace in memory
- Aside from simple resource requirements, the main reason to run masters on separate machines is for high availability
- Rack awareness: However, for multirack clusters, you need to map nodes to racks. This allows Hadoop to prefer within-rack transfers (where there is more bandwidth available) to off-rack transfers when placing MapReduce tasks on nodes. HDFS will also be able to place replicas more intelligently to trade off performance and resilience.
- Network locations such as nodes and racks are represented in a tree, which reflects the network “distance” between locations

# chapter 17 Hive
- Hive is a framework for data warehousing on top of Hadoop
- In normal use, Hive runs on your workstation and converts your SQL query into a series of jobs for execution on a Hadoop cluster.
- The shell is the primary way that we will interact with Hive, by issuing commands in HiveQL. HiveQL is Hive’s query language, a dialect of SQL. It is heavily influenced by MySQL, so if you are familiar with MySQL, you should feel at home using Hive.
- The ROW FORMAT clause, however, is particular to HiveQL. This declaration is saying that each row in the data file is tab-delimited text
- By itself, SET will list all the properties (and their values) set by Hive. Note that the list will not include Hadoop defaults, unless they have been explicitly overridden in one of the ways covered in this section. Use SET -v to list all the properties in the system, including Hadoop defaults.
- In a traditional database, a table’s schema is enforced at data load time. If the data being loaded doesn’t conform to the schema, then it is rejected. This design is sometimes called schema on write because the data is checked against the schema when it is written into the database.Hive, on the other hand, doesn’t verify the data when it is loaded, but rather when a query is issued. This is called schema on read
- There are trade-offs between the two approaches. Schema on read makes for a very fast initial load, since the data does not have to be read, parsed, and serialized to disk in the database’s internal format. The load operation is just a file copy or move. It is more flexible, too: consider having two schemas for the same underlying data, depending on the analysis being performed
- Schema on write makes query time performance faster because the database can index columns and perform compression on the data. The trade-off, however, is that it takes longer to load data into the database. Furthermore, there are many scenarios where the schema is not known at load time, so there are no indexes to apply, because the queries have not been formulated yet. These scenarios are where Hive shines.
- HDFS does not provide in-place file updates, so changes resulting from inserts, updates, and deletes are stored in small delta files. Delta files are periodically merged into the base table files by MapReduce jobs that are run in the background by the metastore.
- a STRUCT is a record type that encapsulates a set of named fields. A UNION specifies a choice of data types; values must match exactly one of these types
- You can retrieve a list of functions from the Hive shell by typing SHOW FUNCTIONS.To get brief usage instructions for a particular function, use the DESCRIBE command
- You can perform explicit type conversion using CAST
- When you create a table in Hive, by default Hive will manage the data, which means that Hive moves the data into its warehouse directory. Alternatively, you may create an external table, which tells Hive to refer to the data that is at an existing location outside the warehouse directory
- As a rule of thumb, if you are doing all your processing with Hive, then use managed tables, but if you wish to use Hive and other tools on the same dataset, then use external tables
- Hive organizes tables into partitions—a way of dividing a table into coarse-grained parts based on the value of a partition column, such as a date. Using partitions can make it faster to do queries on slices of the data.
- Tables or partitions may be subdivided further into buckets to give extra structure to the data that may be used for more efficient queries
- Partitions are defined at table creation time using the PARTITIONED BY clause,7 which takes a list of column definitions.At the filesystem level, partitions are simply nested subdirectories of the table directory
- We can ask Hive for the partitions in a table using SHOW PARTITIONS
- You can use partition columns in SELECT statements in the usual way. Hive performs input pruning to scan only the relevant partitions
- There are two reasons why you might want to organize your tables (or partitions) into buckets. The first is to enable more efficient queries. Bucketing imposes extra structure on the table, which Hive can take advantage of when performing certain queries. In particular, a join of two tables that are bucketed on the same columns—which include the join columns—can be efficiently implemented as a map-side join.The second reason to bucket a table is to make sampling more efficient. When working with large datasets, it is very convenient to try out queries on a fraction of your dataset while you are in the process of developing or refining them.
- The data within a bucket may additionally be sorted by one or more columns. This allows even more efficient map-side joins, since the join of each bucket becomes an efficient merge sort
- We can see the same thing by sampling the table using the TABLESAMPLE clause, which restricts the query to a fraction of the buckets in the table rather than the whole table
- There are two dimensions that govern table storage in Hive: the row format and the file format. The row format dictates how rows, and the fields in a particular row, are stored.The file format dictates the container format for fields in a row
- Binary formats can be divided into two categories: row-oriented formats and column- oriented formats. Generally speaking, column-oriented formats work well when queries access only a small number of columns in the table, whereas row-oriented formats are appropriate when a large number of columns of a single row are needed for processing at the same time.
- In HiveQL, you can turn the INSERT statement around and start with the FROM clause for the same effect.The reason for this syntax becomes clear when you see that it’s possible to have multiple INSERT clauses in the same query. This so-called multitable insert is more efficient than multiple INSERT statements because the source table needs to be scanned only once to produce the multiple disjoint outputs.
- It’s often very convenient to store the output of a Hive query in a new table, perhaps because it is too large to be dumped to the console or because there are further processing steps to carry out on the result.The new table’s column definitions are derived from the columns retrieved by the SELECT clause.
- The DROP TABLE statement deletes the data and metadata for a table. In the case of external tables, only the metadata is deleted; the data is left untouched.If you want to delete all the data in a table but keep the table definition, use TRUNCATE TABLE
- In a similar vein, if you want to create a new, empty table with the same schema as another table, then use the LIKE keyword
- Sorting data in Hive can be achieved by using a standard ORDER BY clause. ORDER BY performs a parallel total sort of the input.When a globally sorted result is not required—and in many cases it isn’t—you can use Hive’s nonstandard extension, SORT BY, instead. SORT BY produces a sorted file per reducer.
- In some cases, you want to control which reducer a particular row goes to—typically so you can perform some subsequent aggregation. This is what Hive’s DISTRIBUTE BY clause does
- Using an approach like Hadoop Streaming, the TRANSFORM, MAP, and REDUCE clauses make it possible to invoke an external script or program from Hive.
- Hive only supports equijoins, which means that only equality can be used in the join predicate, which here matches on the id column in both tables.
- You can see how many MapReduce jobs Hive will use for any particular query by prefixing it with the EXPLAIN keyword
- If one table is small enough to fit in memory, Hive can load it into memory to perform the join in each of the mappers. This is called a map join.
- Views are included in the output of the SHOW TABLES command, and you can see more details about a particular view, including the query used to define it, by issuing the DESCRIBE EXTENDED view_name command
- There are three types of UDF in Hive: (regular) UDFs, user-defined aggregate functions (UDAFs), and user-defined table-generating functions (UDTFs). They differ in the number of rows that they accept as input and produce as output:A UDF operates on a single row and produces a single row as its output. Most functions, such as mathematical functions and string functions, are of this type;A UDAF works on multiple input rows and creates a single output row.Aggregate functions include such functions as COUNT and MAX;A UDTF operates on a single row and produces multiple rows—a table—as output
- It’s also possible to create a function for the duration of the Hive session, so it is not persisted in the metastore, using the TEMPORARY keyword
- An aggregate function is more difficult to write than a regular UDF. Values are aggregated in chunks (potentially across many tasks), so the implementation has to be capable of combining partial aggregations into a final result.

# chapter 19 Spark
- Spark is best known for its ability to keep large working datasets in memory between jobs. This capability allows Spark to outperform the equivalent MapReduce workflow (by an order of magnitude or more in some cases1), where datasets are always loaded from disk. Two styles of application that benefit greatly from Spark’s processing model are iterative algorithms (where a function is applied to a dataset repeatedly until an exit condition is met) and interactive analysis (where a user issues a series of ad hoc ex‐ ploratory queries on a dataset).
- Resilient Distributed Dataset, abbreviated to RDD, which is the central abstraction in Spark: a read-only collection of objects that is partitioned across multiple machines in a cluster. In a typical Spark program, one or more RDDs are loaded as input and through a series of transformations are turned into a set of target RDDs, which have an action performed on them (such as computing a result or writing them to persistent storage).The term “resilient” in “Resilient Distributed Dataset” refers to the fact that Spark can automatically reconstruct a lost par‐ tition by recomputing it from the RDDs that it was computed from.
- Loading an RDD or performing a transformation on one does not trigger any data processing; it merely creates a plan for performing a computation. The computation is only triggered when an action is performed on an RDD.
- This uses the map() method on RDD to apply a function to every element in the RDD.
- The filter() method on RDD takes a predicate, a function that returns a Boolean
- The saveAsTextFile() method also triggers a Spark job. The main difference is that no value is returned, and instead the RDD is computed and its partitions are written to files in the output directory
- A Spark job is more general than a MapReduce job, though, since it is made up of an arbitrary directed acyclic graph (DAG) of stages, each of which is roughly equivalent to a map or reduce phase in MapReduce
- Stages are split into tasks by the Spark runtime and are run in parallel on partitions of an RDD spread across the cluster—just like tasks in MapReduce.
- A job always runs in the context of an application (represented by a SparkContext instance) that serves to group RDDs and shared variables. An application can run more than one job, in series or in parallel, and provides the mechanism for a job to access an RDD that was cached by a previous job in the same application. An interactive Spark session, such as a spark-shell session, is just an instance of an application.
- There are three ways of creating RDDs: from an in-memory collection of objects (known as parallelizing a collection), using a dataset from external storage (such as HDFS), or transforming an existing RDD.
- Spark provides two categories of operations on RDDs: transformations and actions. A transformation generates a new RDD from an existing one, while an action triggers a computation on an RDD and does something with the results—either returning them to the user, or saving them to external storage.Actions have an immediate effect, but transformations do not—they are lazy, in the sense that they don’t perform any work until an action is performed on the transformed RDD
- The three main transformations for aggregating RDDs of pairs by their keys are reduc eByKey(), foldByKey(), and aggregateByKey().The simplest is reduceByKey(), which repeatedly applies a binary function to values in pairs until a single value is produced.
- Calling cache() does not cache the RDD in memory straightaway. Instead, it marks the RDD with a flag indicating it should be cached when the Spark job is run
- Compare this to MapReduce, where to perform another calculation the input dataset has to be loaded from disk again. Even if an intermediate dataset can be used as input (such as a cleaned-up dataset with invalid rows and unnecessary fields removed), there is no getting away from the fact that it must be loaded from disk, which is slow. Spark will cache datasets in a cross-cluster in-memory cache, which means that any computation performed on those datasets will be very fast.
- Spark offers different types of persistence behavior that may be selected by calling persist() with an argument to specify the StorageLevel.
- There are two aspects of serialization to consider in Spark: serialization of data and serialization of functions (or closures).
- At the highest level, there are two independent entities: the driver, which hosts the application (SparkContext) and schedules tasks for a job; and the executors, which are exclusive to the application, run for the duration of the application, and execute the application’s tasks.
- A Spark job is submitted automatically when an action (such as count()) is performed on an RDD. Internally, this causes runJob() to be called on the SparkContext (step 1 in Figure 19-1), which passes the call on to the scheduler that runs as a part of the driver (step 2). The scheduler is made up of two parts: a DAG scheduler that breaks down the job into a DAG of stages, and a task scheduler that is responsible for submitting the tasks from each stage to the cluster.
- As the name suggests, shuffle map tasks are like the map-side part of the shuffle in MapReduce. Each shuffle map task runs a computation on one RDD partition and, based on a partitioning function, writes its output to a new set of partitions, which are then fetched in a later stage.Shuffle map tasks run in all stages except the final stage.
- Result tasks run in the final stage that returns the result to the user’s program (such as the result of a count()). Each result task runs a computation on its RDD partition, then sends the result back to the driver, and the driver assembles the results from each partition into a final result
- The DAG scheduler is responsible for splitting a stage into tasks for submission to the task scheduler.
- Each task is given a placement preference by the DAG scheduler to allow the task scheduler to take advantage of data locality.
- Spark offers two deploy modes for running on YARN: YARN client mode, where the driver runs in the client, and YARN cluster mode, where the driver runs on the cluster in the YARN application master.
- YARN client mode is required for programs that have any interactive component, such as spark-shell or pyspark. Client mode is also useful when building Spark programs, since any debugging output is immediately visible
- YARN cluster mode, on the other hand, is appropriate for production jobs, since the entire application runs on the cluster, which makes it much easier to retain logfiles (including those from the driver program) for later inspection. YARN will also retry the application if the application master fails
- RDDs:Immutable collection partitioned across cluster that can be rebuilt if a partition is lost;Created by transforming data in stable storage using data flow operators (map, filter, group-by, ...);Can be cached across parallel operations.
- SparkContext is the object that manages the connection to the clusters in Spark and coordinates running processes on the clusters themselves. SparkContext connects to cluster managers, which manage the actual executors that run the specific computations. Here’s a diagram from the Spark documentation to better visualize the architecture:
![GA](/img/SparkContext.png)

- The SparkContext object is usually referenced as the variable sc
- pyspark中take(n) will return the first n elements of the RDD
- The key idea to understand when working with Spark is data pipelining. Every operation or calculation in Spark is essentially a series of steps that can be chained together and run in succession to form a pipeline. Each step in the pipeline returns either a Python value (e.g. Integer), a Python data structure (e.g. Dictionary) or an RDD object.
