---
layout: post
title: "hadoop权威指南"       # Title of the post
subtitle:
date:       2016-11-22 12:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 大数据
---

# chapter 0
[在Mac OSX Yosemite上安装Hadoop](http://www.jianshu.com/p/3aebdba32363)

# chapter 1 Meet Hadoop
- For all its strengths,MapReduce is fundamentally a batch processing system,and is not suitable for interactive analysis,so it's best for offline use
- HBase provides both online read/write access of individual rows and batch operations for reading and writing data in bulk,making it a good solution for building applications on
- YARN is a cluster resource management system,which allows any distributed program(not just MapReduce) to run on data in a Hadoop cluster
- MapReduce is a good fit for problems that need to analyze the whole dataset in a batch fashion,particularly for ad hoc analysis.An RDBMS is good for point queries or updates,where the dataset has been indexed to deliver low-latency retrieval and  update times of a relatively small amount of data.MapReduce suits applications where the data is written once and read many times,whereas a relational database is good for datasets that are continually updated.
- Hadoop works well on unstructured or semi-structured data because it is designed to interpret the data at processing time (so called schema-on-read). This provides flexibility and avoids the costly data loading phase of an RDBMS, since in Hadoop it is just a file copy

# chapter 2 MapReduce
- MapReduce is a programming model for data processing
- MapReduce works by breaking the processing into two phases: the map phase and the reduce phase.Each phase has key-value pairs as input and output,the types of which may be chosen by the programmer.The programmer also specifies two functions:the map function and the reduce function
- The output from the map function is processed by the MapReduce framework before being sent to the reduce function.This processing sorts and groups the key-value pairs by key
- The input types of the reduce function must match the output types of the map function
- Hadoop runs the job by dividing it into tasks, of which there are two types: map tasks and reduce tasks. The tasks are scheduled using YARN and run on nodes in the cluster. If a task fails, it will be automatically rescheduled to run on a different node
- Hadoop divides the input to a MapReduce job into fixed-size pieces called input splits, or just splits. Hadoop creates one map task for each split, which runs the user-defined map function for each record in the split.
- On the other hand, if splits are too small, the overhead of managing the splits and map task creation begins to dominate the total job execution time. For most jobs, a good split size tends to be the size of an HDFS block, which is 128 MB by default, although this can be changed for the cluster (for all newly created files) or specified when each file is created.
- Hadoop does its best to run the map task on a node where the input data resides in HDFS, because it doesn’t use valuable cluster bandwidth. This is called the data locality optimization
- It should now be clear why the optimal split size is the same as the block size: it is the largest size of input that can be guaranteed to be stored on a single node. If the split spanned two blocks, it would be unlikely that any HDFS node stored both blocks, so some of the split would have to be transferred across the network to the node running the map task, which is clearly less efficient than running the whole map task using local data.
- Map tasks write their output to the local disk, not to HDFS. Why is this? Map output is intermediate output: it’s processed by reduce tasks to produce the final output, and once the job is complete, the map output can be thrown away. So, storing it in HDFS with replication would be overkill.
- The output of the reduce is normally stored in HDFS for reliability
- Many MapReduce jobs are limited by the bandwidth available on the cluster, so it pays to minimize the data transferred between map and reduce tasks. Hadoop allows the user to specify a combiner function to be run on the map output, and the combiner function’s output forms the input to the reduce function
- The combiner function doesn’t replace the reduce function.But it can help cut down the amount of data shuffled between the mappers and the reducers, and for this reason alone it is always worth considering whether you can use a combiner function in your MapReduce job
- Hadoop Streaming uses Unix standard streams as the interface between Hadoop and your program, so you can use any language that can read standard input and write to standard output to write your MapReduce program

# chapter 3 HDFS
- Applications that require low-latency access to data, in the tens of milliseconds range, will not work well with HDFS. Remember, HDFS is optimized for delivering a high throughput of data, and this may be at the expense of latency.HBase is currently a better choice for low-latency access
- Files in HDFS may be written to by a single writer. Writes are always made at the end of the file, in append-only fashion. There is no support for multiple writers or for modifications at arbitrary offsets in the file
- HDFS blocks are large compared to disk blocks, and the reason is to minimize the cost of seeks. If the block is large enough, the time it takes to transfer the data from the disk can be significantly longer than the time to seek to the start of the block. Thus, transferring a large file made of multiple blocks operates at the disk transfer rate
- An HDFS cluster has two types of nodes operating in a master−worker pattern: a namenode (the master) and a number of datanodes (workers).
-Without the namenode, the filesystem cannot be used. In fact, if the machine running the namenode were obliterated, all the files on the filesystem would be lost since there would be no way of knowing how to reconstruct the files from the blocks on the datanodes. For this reason, it is important to make the namenode resilient to failure, and Hadoop provides two mechanisms for this
- HDFS federation, introduced in the 2.x release series, allows a cluster to scale by adding namenodes, each of which manages a portion of the filesystem namespace.
- Hadoop 2 remedied this situation by adding support for HDFS high availability (HA). In this implementation, there are a pair of namenodes in an active-standby configuration. In the event of the failure of the active namenode, the standby takes over its duties to continue servicing client requests without a significant interruption
- The transition from the active namenode to the standby is managed by a new entity in the system called the failover controller
- Hadoop provides many interfaces to its filesystems, and it generally uses the URI scheme to pick the correct filesystem instance to communicate with
- When copying data into HDFS, it’s important to consider cluster balance. HDFS works best when the file blocks are evenly spread across the cluster
- 分布式文件系统不会有缓存
- 分布式文件系统简化了API，不会有单机文件系统那么多的操作
- HDFS吞吐量较大，响应时间较长，适用于大文件，不适用于小文件
- 多次读取，一次写入，写入只能在文件末尾
- 块大小为64MB，使得传输时间是寻道时间的1%

# chapter 4 YARN
- Apache YARN (Yet Another Resource Negotiator) is Hadoop’s cluster resource management system
- YARN provides its core services via two types of long-running daemon: a resource manager (one per cluster) to manage the use of resources across the cluster, and node managers running on all the nodes in the cluster to launch and monitor containers. A container executes an application-specific process with a constrained set of resources (memory, CPU, and so on)
- A YARN application can make resource requests at any time while it is running
- It is the job of the YARN scheduler to allocate resources to applications according to some defined policy
- Three schedulers are available in YARN: the FIFO, Capacity, and Fair Schedulers
- FIFO 调度方式对短作业不公平，因为长作业会长时间占有
- The Capacity Scheduler allows sharing of a Hadoop cluster along organizational lines, whereby each organization is allocated a certain capacity of the overall cluster. Each organization is set up with a dedicated queue that is configured to use a given fraction of the cluster capacity. Queues may be further divided in hierarchical fashion, allowing each organization to share its cluster allowance between different groups of users within the organization. Within a queue, applications are scheduled using FIFO scheduling.
- The Fair Scheduler attempts to allocate resources so that all running applications get the same share of resources
- When using delay scheduling, the scheduler doesn’t simply use the first scheduling opportunity it receives, but waits for up to a given maximum number of scheduling opportunities to occur before loosening the locality constraint and taking the next scheduling opportunity

# chapter 5 Hadoop I/O
- The usual way of detecting corrupted data is by computing a checksum for the data when it first enters the system, and again whenever it is transmitted across a channel that is unreliable and hence capable of corrupting the data. The data is deemed to be corrupt if the newly generated checksum doesn’t exactly match the original
- Datanodes are responsible for verifying the data they receive before storing the data and its checksum.This applies to data that they receive from clients and from other datanodes during replication
- When clients read data from datanodes, they verify checksums as well, comparing them with the ones stored at the datanodes
- Because HDFS stores replicas of blocks, it can “heal” corrupted blocks by copying one of the good replicas to produce a new, uncorrupt replica.The way this works is that if a client detects an error when reading a block, it reports the bad block and the datanode it was trying to read from to the namenode before throwing a ChecksumException. The namenode marks the block replica as corrupt so it doesn’t direct any more clients to it or try to copy this replica to another datanode. It then schedules a copy of the block to be replicated on another datanode, so its replication factor is back at the expected level. Once this has happened, the corrupt replica is deleted
- File compression brings two major benefits: it reduces the space needed to store files, and it speeds up data transfer across the network or to or from disk
- All compression algorithms exhibit a space/time trade-off: faster compression and de‐ compression speeds usually come at the expense of smaller space savings.
- splitting is whether you can seek to any point in the stream and start reading from some point further on. Splittable compression formats are especially suitable for Map‐ Reduce
- When considering how to compress data that will be processed by MapReduce, it is important to understand whether the compression format supports splitting
- For large files, you should not use a compression format that does not support splitting on the whole file, because you lose locality and make MapReduce applications very inefficient.
- Even if your MapReduce application reads and writes uncompressed data, it may benefit from compressing the intermediate output of the map phase.The map output is written to disk and transferred across the network to the reducer nodes, so by using a fast compressor such as LZO, LZ4, or Snappy, you can get performance gains simply because the volume of data to transfer is reduced
- Serialization is the process of turning structured objects into a byte stream for trans‐ mission over a network or for writing to persistent storage. Deserialization is the reverse process of turning a byte stream back into a series of structured objects
- How do you choose between a fixed-length and a variable-length encoding? Fixed- length encodings are good when the distribution of values is fairly uniform across the whole value space, such as when using a (well-designed) hash function. Most numeric variables tend to have nonuniform distributions, though, and on average, the variable- length encoding will save space. Another advantage of variable-length encodings is that you can switch from VIntWritable to VLongWritable, because their encodings are actually the same. So, by choosing a variable-length representation, you have room to grow without committing to an 8-byte long representation from the beginning
- A MapFile is a sorted SequenceFile with an index to permit lookups by key
- In general, column-oriented formats work well when queries access only a small number of columns in the table. Conversely, row- oriented formats are appropriate when a large number of columns of a single row are needed for processing at the same time.

# chapter 6 Developing a MapReduce Application
- System properties take priority over properties defined in resource files
- When the processing gets more complex, this complexity is generally manifested by having more MapReduce jobs, rather than having more complex map and reduce functions. In other words, as a rule of thumb, think about adding more jobs, rather than adding complexity to jobs
- Apache Oozie is a system for running workflows of dependent jobs. It is composed of two main parts: a workflow engine that stores and runs workflows composed of different types of Hadoop jobs (MapReduce, Pig, Hive, and so on), and a coordinator engine that runs workflow jobs based on predefined schedules and data availability
- In Oozie parlance, a workflow is a DAG of action nodes and control-flow nodes.An action node performs a workflow task, such as moving files in HDFS; running a MapReduce, Streaming, Pig, or Hive job; performing a Sqoop import; or running an arbitrary shell script or Java program. A control-flow node governs the workflow exe‐ cution between actions by allowing such constructs as conditional logic (so different execution branches may be followed depending on the result of an earlier action node) or parallel execution. When the workflow completes, Oozie can make an HTTP callback to the client to inform it of the workflow status. It is also possible to receive callbacks every time the workflow enters or exits an action node

# chapter 7 How MapReduce Works
- The Streaming task communicates with the process (which may be written in any language) using standard input and output streams. During execution of the task, the Java process passes input key-value pairs to the external process, which runs it through the user-defined map or reduce function and passes the output key-value pairs back to the Java process. From the node manager’s point of view, it is as if the child process ran the map or reduce code itself.
- Node managers may be blacklisted if the number of failures for the application is high, even if the node manager itself has not failed. Blacklisting is done by the application master, and for MapReduce the application master will try to reschedule tasks on different nodes if more than three tasks fail on a node manager.
- To achieve high availability (HA), it is necessary to run a pair of resource managers in an active-standby configuration. If the active resource manager fails, then the standby can take over without a significant interruption to the client
- MapReduce makes the guarantee that the input to every reducer is sorted by key. The process by which the system performs the sort—and transfers the map outputs to the reducers as inputs—is known as the shuffle.
- On the map side, the best performance can be obtained by avoiding multiple spills to disk; one is optimal
- On the reduce side, the best performance is obtained when the intermediate data can reside entirely in memory.
- Hadoop doesn’t try to diagnose and fix slow-running tasks; instead, it tries to detect when a task is running slower than expected and launches another equivalent task as a backup. This is termed speculative execution of tasks.
- Rather, the scheduler tracks the progress of all tasks of the same type (map and reduce) in a job, and only launches speculative duplicates for the small proportion that are running significantly slower than the average. When a task completes successfully, any duplicate tasks that are running are killed since they are no longer needed
- Speculative execution is an optimization, and not a feature to make jobs run more reliably. If there are bugs that sometimes cause a task to hang or slow down, relying on speculative execution to avoid these problems is unwise and won’t work reliably, since the same bugs are likely to affect the speculative task.
- Hadoop MapReduce uses a commit protocol to ensure that jobs and tasks either succeed or fail cleanly.
