---
layout: post
title: "hadoop权威指南"       # Title of the post
subtitle:
modified: 2016-11-22                 # Date
date:       2016-11-22 12:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 大数据
---

# chapter 0
[在Mac OSX Yosemite上安装Hadoop](http://www.jianshu.com/p/3aebdba32363)

# chapter 1 Meet Hadoop
- For all its strengths,MapReduce is fundamentally a batch processing system,and is not suitable for interactive analysis,so it's best for offline use
- HBase provides both online read/write access of individual rows and batch operations for reading and writing data in bulk,making it a good solution for building applications on
- YARN is a cluster resource management system,which allows any distributed program(not just MapReduce) to run on data in a Hadoop cluster
- MapReduce is a good fit for problems that need to analyze the whole dataset in a batch fashion,particularly for ad hoc analysis.An RDBMS is good for point queries or updates,where the dataset has been indexed to deliver low-latency retrieval and  update times of a relatively small amount of data.MapReduce suits applications where the data is written once and read many times,whereas a relational database is good for datasets that are continually updated.
- Hadoop works well on unstructured or semi-structured data because it is designed to interpret the data at processing time (so called schema-on-read). This provides flexibility and avoids the costly data loading phase of an RDBMS, since in Hadoop it is just a file copy

# chapter 2 MapReduce

- MapReduce is a programming model for data processing
- MapReduce works by breaking the processing into two phases: the map phase and the reduce phase.Each phase has key-value pairs as input and output,the types of which may be chosen by the programmer.The programmer also specifies two functions:the map function and the reduce function
- The output from the map function is processed by the MapReduce framework before being sent to the reduce function.This processing sorts and groups the key-value pairs by key
- The input types of the reduce function must match the output types of the map function
- Hadoop runs the job by dividing it into tasks, of which there are two types: map tasks and reduce tasks. The tasks are scheduled using YARN and run on nodes in the cluster. If a task fails, it will be automatically rescheduled to run on a different node
- Hadoop divides the input to a MapReduce job into fixed-size pieces called input splits, or just splits. Hadoop creates one map task for each split, which runs the user-defined map function for each record in the split.
- On the other hand, if splits are too small, the overhead of managing the splits and map task creation begins to dominate the total job execution time. For most jobs, a good split size tends to be the size of an HDFS block, which is 128 MB by default, although this can be changed for the cluster (for all newly created files) or specified when each file is created.
- Hadoop does its best to run the map task on a node where the input data resides in HDFS, because it doesn’t use valuable cluster bandwidth. This is called the data locality optimization
- It should now be clear why the optimal split size is the same as the block size: it is the largest size of input that can be guaranteed to be stored on a single node. If the split spanned two blocks, it would be unlikely that any HDFS node stored both blocks, so some of the split would have to be transferred across the network to the node running the map task, which is clearly less efficient than running the whole map task using local data.
- Map tasks write their output to the local disk, not to HDFS. Why is this? Map output is intermediate output: it’s processed by reduce tasks to produce the final output, and once the job is complete, the map output can be thrown away. So, storing it in HDFS with replication would be overkill.
- The output of the reduce is normally stored in HDFS for reliability
- Many MapReduce jobs are limited by the bandwidth available on the cluster, so it pays to minimize the data transferred between map and reduce tasks. Hadoop allows the user to specify a combiner function to be run on the map output, and the combiner function’s output forms the input to the reduce function
- The combiner function doesn’t replace the reduce function.But it can help cut down the amount of data shuffled between the mappers and the reducers, and for this reason alone it is always worth considering whether you can use a combiner function in your MapReduce job
- Hadoop Streaming uses Unix standard streams as the interface between Hadoop and your program, so you can use any language that can read standard input and write to standard output to write your MapReduce program
