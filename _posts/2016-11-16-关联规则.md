---
layout: post
title: "关联规则"       # Title of the post
description: 关联规则       # Description of the post, used for Facebook Opengraph & Twitter
headline: 关联规则      # Will appear in bold letters on top of the post
modified: 2016-11-16                 # Date
category: 机器学习
tags: [关联规则]
image:
comments: true
mathjax:
---
# 关联规则

## 1.Support、Confidence、Lift
- Support is an indication of how frequently the item-set appears in the database.
- Confidence is an indication of how often the rule has been found to be true
- Lift is a simple correlation measure that is given as follows. The occurrence of itemset A is independent of the occurrence of itemset B if ; otherwise, itemsets A and B are dependent and correlated as events. This definition can easily be extended to more than two itemsets.
If the resulting value is less than 1, then the occurrence of A is negatively correlated with the occurrence of B, meaning that the occurrence of one likely leads to the absence of the other one. If the resulting value is greater than 1, then A and B are positively correlated, meaning that the occurrence of one implies the occurrence of the other. If the resulting value is equal to 1, then A and B are independent and there is no correlation between them

## 2.Apriori
- Apriori employs an iterative approach known as a level-wise search, where k-itemsets are used to explore (k + 1)-itemsets. First, the set of frequent 1-itemsets is found by scanning the database to accumulate the count for each item, and collecting those items that satisfy minimum support. The resulting set is denoted by L1. Next, L1 is used to find L2, the set of frequent 2-itemsets, which is used to find L3, and so on, until no more frequent k-itemsets can be found. The finding of each Lk requires one full scan of the database
To improve the efficiency of the level-wise generation of frequent itemsets, an important property called the Apriori property is used to reduce the search space
>Apriori property: All nonempty subsets of a frequent itemset must also be frequent

## 3.FP-growth
- First, it compresses the database representing frequent items into a frequent pattern tree, or FP-tree, which retains the itemset association information. It then divides the compressed database into a set of conditional databases (a special kind of projected database), each associated with one frequent item or “pattern fragment,” and mines each database separately. For each “pattern fragment,” only its associated data sets need to be examined. Therefore, this approach may substantially reduce the size of the data sets to be searched, along with the “growth” of patterns being examined

>The FP-growth method transforms the problem of finding long frequent patterns into searching for shorter ones in much smaller conditional databases recursively and then concatenating the suffix. It uses the least frequent items as a suffix, offering good selectivity. The method substantially reduces the search costs
