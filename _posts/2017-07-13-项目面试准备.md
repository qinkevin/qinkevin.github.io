---
layout:     post
title:      "项目面试准备"
subtitle:
date:       2017-7-13 11:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 其他
---
# 百度实习

## 规则
- 不一致性检测（一个bssid对应多个ssid）、异常检测（一个cuid对应过少或过多ip）、与异常实体有较多关联（登陆过模拟器的账户）
- 优点：性能高、易于理解和分析、开发相对简单
- 缺点：一刀切，容易被薅羊毛的人嗅探到；规则冲突问题
## 图计算
- Label propagation、PageRank、Triangle count
- LPA算法的终止条件要求所有node的label一定是它的邻居label中出现次数最多的
- LPA不需要太多计算资源，但是并不一定会收敛且可能不准确
- 主要参数为迭代次数
## 机器学习
- 目的：识别风险设备
- 数据量：7千万，负样本70万
- 数据探索性分析：统计分析、异常值（未处理，本身可能就是信号）、缺失值（大量）
- 特征工程：设备基本信息（是否模拟器、是否root设备、是否改码工具、是否注入调试等）、规则特征（设备所在bssid下root设备占比、模拟器占比、设备所在ip下root设备占比、模拟器占比）、图结构特征（设备所属团伙的拓扑特征、团伙的实体特征）、设备活跃特征（设备连接的bssid数、周、月）、app安装情况、账户数、组合特征、one-hot encoder 共两百多个特征，加上GBDT造的新特征300，共五百多特征
- 标签：使用过去30天是否被规则引擎命中
- 模型： xgboost、xgboost+LR、gcForest
- 最终效果： AUC 0.7、准确率 0.98、召回率 0.2。
- 注意点：GBDT和LR使用的是不同的数据,GBDT树的深度为3，树的个数为30,每棵树的最大叶子节点数10；过采样(smote)，类别不平衡问题;

# 融360贷款预测比赛
- 目的：预测用户是否会逾期还款
- 数据：用户的基本属性（用户id、性别、职业、教育、婚姻状态、户口类型）、银行流水记录（用户id、时间戳、交易类型、交易金额、工资收入标记，一个用户对应多条记录，所以采用对每种交易类型进行聚合，交易金额取均值方差总和最大值最小值，时间戳取中位数，次数和金额。同理也可根据是否工资收入进行聚合，用户持卡数）、用户浏览记录（用户id、时间戳、浏览行为数据、浏览子行为编号，计算每个用户每个子行为的次数）、信用卡账单记录（用户id、账单时间戳、银行id、上期账单金额、上期还款金额、信用卡额度、本期账单余额、本期账单最低还款额、消费笔数、本期账单金额、调整金额、循环利息、可用金额、预借现金额度、还款状态。上期是否还清、上期是否超额、本期账单是否有余额、本期账单金额与上期账单金额比、每笔消费平均金额、本期最低还款额与本期账单金额比值、可用额度与本期账单金额比值）、放款时间（用户id和放款时间）、顾客是否发生逾期的记录.总数据7万，训练数据5万5千，测试数据1万5千，正负样本比7：1
- 数据探索性分析：统计分析、异常值、缺失值、低频合并（离散）、对数变换（连续）
- 特征工程：基础特征、组合特征、是否缺失特征（是否有银行记录、信用卡账单记录）、放款前后对比、特征选择（将rf和gbdt的平均重要度作为最终参考指标，筛选掉得分低的特征）
- 模型：均有尝试 KNN、SVM、LR、RF、GBDT、XGBOOST、Neural Network.SVM太慢、GBDT、XGBOOST（一千棵树，深度为5）、RF最好
- 调参：网格调参、贝叶斯调参，对重要的参数设置搜索空间，交叉验证看效果、学习曲线看过拟合还是欠拟合
- 模型融合：Stacking
- 最终效果：AUC ，KS值（tpr和fpr差值的最大值）,从0.4到0.43
- 错误分析：贷款时间，从而区分放款前和放款后
注意：
- 将不同方法的平均重要度作为最终参考指标，筛选掉得分低的特征。
- 为了解某个模型在犯什么错误，我们可以观察被模型误判的样本，总结它们的共同特征，我们就可以再训练一个效果更好的模型
- 如果数据出现Label不均衡情况，可以使用Stratified K-fold，这样得到的Train Set和Test Set的Label比例是大致相同。
- 在实现Stacking时，要注意的一点是，避免标签泄漏(Label Leak)。在训练次学习器时，需要上一层学习器对Train Data的测试结果作为特征。如果我们在Train Data上训练，然后在Train Data上预测，就会造成Label Leak。为了避免Label Leak，需要对每个学习器使用K-fold，将K个模型对Valid Set的预测结果拼起来，作为下一层学习器的输入。可以将K个模型对Test Data的预测结果求平均，也可以用所有的Train Data重新训练一个新模型来预测Test Data.
-  对于取值较多（如几十万）的类别特征（ID特征），直接进行OneHotEncoder编码会导致特征矩阵非常巨大，影响模型效果。可以使用如下的方式进行处理：统计每个取值在样本中出现的频率，取 Top N 的取值进行 One-hot 编码，剩下的类别分到“其他“类目下，其中 N 需要根据模型效果进行调优
- 对于稀疏型特征（如文本特征，One-hot的ID类特征），我们一般使用线性模型，譬如 Linear Regression 或者 Logistic Regression。Random Forest 和 GBDT 等树模型不太适用于稀疏的特征，但可以先对特征进行降维（如PCA，SVD/LSA等），再使用这些特征。稀疏特征直接输入 DNN 会导致网络 weight 较多，不利于优化，也可以考虑先降维，或者对 ID 类特征使用 Embedding 的方式；
- 对于稠密型特征，推荐使用 XGBoost 进行建模，简单易用效果好；
- 数据中既有稀疏特征，又有稠密特征，可以考虑使用线性模型对稀疏特征进行建模，将其输出与稠密特征一起再输入 XGBoost/DNN 建模
#信用卡评分
- 目的：贷前评分卡，量化客户违约风险，违约概率（转换为打分）
- 数据：give me some credict的公开贷款数据集，训练集15万，测试集10万，特征11个（是否会逾期90天、信用卡余额占总的信用卡额度的比例、年龄、最近两年出现逾期30-59天的次数、最近两年出现逾期60-89天的次数、每月消费占每月总收入的比重、每月收入、贷款和信用额度、逾期90天以上的次数、家庭成员数）
- 预处理：缺失值较多的去掉、缺失值补0
- 特征工程：分组、WOE（分组中好的样本比例占总体中好的样本比例/分组中好的样本比例占总体中好的样本比例，然后取对数）
- 模型：通常的做法是用qcut先将变量划分为30个箱，然后根据woe和badrate对相邻箱进行合并，同时考虑业务逻辑进行边界点的选择。在经验法则里IV小于0.05的变量预测能力都是非常差的，在业务逻辑允许的情况下，可以直接剔除该变量。通过逐步回归（StepWise Logistic Regression）来确定最终变量，做特征选择，根据F统计量
- 最终效果 AUC 0.7 KS 0.3
- 大量人工特征，将连续特征离散化，并对离散化的特征进行One-Hot编码，最后对特征进行二阶或者三阶的特征组合，目的是为了得到非线性的特征。连续变量切分点如何选取？离散化为多少份合理？选择哪些特征交叉？多少阶交叉，二阶，三阶或更多？一般都是按照经验，不断尝试一些组合，然后根据线下评估选适当参数
#文本聚类
