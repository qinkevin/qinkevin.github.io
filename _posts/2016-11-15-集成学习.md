---
layout: post
title: "集成学习"       # Title of the post
subtitle:   
date:       2016-11-19 12:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 机器学习
---
# 集成学习

- 集成学习通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性能。这对“弱学习器”尤为明显

>弱学习器常指泛化性能略优于随机猜测的学习器；例如在二分类问题上精度略高于50%的分类器

- 要获得好的集成，个体学习器应“好而不同”，即个体学习器要有一定的“准确性”，即学习器不能太坏，并且要有“多样性”，即学习器间具有差异，但两者往往存在冲突。

>Different Learning Algorithms ：DT, SVM, NN, KNN …
>Different Training Processes :Different Parameters、Different Training Sets、Different Feature Sets


- 根据个体学习器的生成方式，集成学习方法大致可分为两大类，即个体学习器间存在强依赖关系、必须串行生成的序列化方法，以及个体学习器间不存在强依赖关系、可同时生成的并行化方法；前者的代表是Boosting,后者的代表是Bagging 和 Random Forests
- 集成方法被广泛使用；不需要特征正则化；可以学习高阶交叉项；可以大规模使用

## 1.Boosting
- Boosting 的工作机制是：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合

> - 和Bagging 不同，Boosting 是基于分类器的表现投票，而不是每个分类器的权重都相同
> - bagging的有放回抽样是为了保证每次相互独立
> - 从偏差-方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成

## 2.AdaBoost
- 对Boosting方法来说，有两个问题需要回答：一是在每一轮如何改变训练数据的权值或概率分布；二是如何将弱分类器组合成一个强分类器。关于第一个问题，AdaBoost的做法是，提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注。至于第二个问题，即弱分类器的组合，AdaBoost采取加权多数表决的方法。具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类错误率大的弱分类器的权值，使其在表决中起较小的作用
- AdaBoost是用基学习器的线性组合来最小化指数损失函数。初始化权重相同，然后计算预测值与真实值的误差，基于误差更新权重（误差越小，权值越大），直到达到实现指定的T个基学习器。最后将T个基学习器加权结合，权重为每次基于误差所得，最后看符号即可求出分类
- 若指数损失函数最小化，则分类错误率也将最小化；这说明指数损失函数是分类任务原本0/1损失函数的一致的替代损失函数。由于这个替代函数有更好的数学性质，因此我们用它替代0/1损失函数作为优化目标
- 重采样法即在每一轮学习中，根据样本分布对训练集重新进行采样，再用重采样而得的样本集对基学习器进行训练
- Boosting算法在训练的每一轮都要检查当前生成的基学习器是否满足基本条件（即检查当前基分类器是否比随机猜测好），一旦条件不满足，则当前基学习器即被抛弃，且学习过程停止

Advantages:
- Simple and easy to implement
- Almost no parameters to tune
- Proven upper bounds on training set
- Immune to overfitting（不容易过拟合）

Disadvantages
- Suboptimal a values
- Steepest descent
- Sensitive to noise(对噪声比较敏感)

## 3.RegionBoost
- AdaBoost assigns fixed weights to models
- However, different models emphasize different regions.
- The weights of models should be input-dependent.
- Given an input, only invoke appropriate models.
- Train a competency predictor for each model.
- Estimate whether the model is likely to make a right decision.
- Use this information as the weight.

## 4.Bagging
- Bagging 基于 bootstrap samling(有放回抽样)采样出T个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合。在对预测输出进行结合时，Bagging 通常对分类任务使用简单投票法，对回归任务使用简单平均法
- 从偏差-方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显
- 当基学习器对数据扰动敏感时，Bagging会发挥较大作用
- bootstrap抽样法样本次数不同，有的样本出现很多次，有的样本可能不会出现

## 5.Random Forests
- 随机森林也是采用有放回抽样的方式，大约只会用到2/3的数据训练，剩余数据作为测试（bootstrap sampling）
- 随机森林也是通过投票的方式进行选择
- 随机森林是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一颗决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类
- 随机森林在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体来说，传统决策树在选择划分属性时是在当前结点的属性集合（假定有d个属性）中选择一个最优属性；而在随机森林中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最有数学用于划分。一般情况下，推荐值k=log2d.
- 随机森林对Bagging只做了小改动，但是与Bagging中基学习器的“多样性”仅通过样本扰动（通过对初始训练集采样）而来不同，随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动（从初始属性集中抽取出若干个属性子集，再基于每个属性子集训练一个基学习器），这使得最终集成的泛化性能可通过个体学习器之间差异度的增加而进一步提升。
- 随机森林有两个参数，一个是森林中决策树的个数，一个是每棵树上使用的（随机选取的）特征变量个数

>随机森林不需要剪枝，也不会出现过拟合。因为随机森林每次划分只需要考虑更少的属性，所以在大型数据中非常有效

>随机森林是一类专门为决策树分类器设计的组合方法。它组合多棵决策树作出的预测，其中每棵树都是基于随机变量的一个独立集合的值产生的。随机化有助于减少决策树之间的相关性，从而改善组合分类器的泛化误差

>每一个决策树都可以进行“事后剪枝”以避免过拟合，但对于随机森林模型来说，这完全没有必要。随机森林的一大特性就是对过拟合的免疫性：模型本身就吸收了数据中的特质方差，因此即使不做单个决策树的“事后剪枝”，随机森林模型也不会过拟合

>随机森林性能好，容易并行，不用做特征选择，训练速度快

>随机森林非常随机，可能需要很多树才能收敛

### 5.1 随机森林之特征选择

在随机森林中某个特征X的重要性的计算方法如下：

1. 对于随机森林中的每一颗决策树,使用相应的OOB(袋外数据)数据来计算它的袋外数据误差,记为errOOB1.
2. 随机地对袋外数据OOB所有样本的特征X加入噪声干扰(就可以随机的改变样本在特征X处的值),再次计算它的袋外数据误差,记为errOOB2.
3. 假设随机森林中有Ntree棵树,那么对于特征X的重要性=∑(errOOB2-errOOB1)/Ntree,之所以可以用这个表达式来作为相应特征的重要性的度量值是因为：若给某个特征随机加入噪声之后,袋外的准确率大幅度降低,则说明这个特征对于样本的分类结果影响很大,也就是说它的重要程度比较高。

## 6.Bagging, boosting and stacking

bagging和stacking中的基模型为强模型（偏差低方差高），boosting中的基模型为弱模型。

Random Forest的子模型都拥有较低的偏差，整体模型的训练过程旨在降低方差，故其需要较少的子模型（n_estimators默认值为10）且子模型不为弱模型（max_depth的默认值为None），同时，降低子模型间的相关度可以起到减少整体模型的方差的效果（max_features的默认值为auto）。另一方面，Gradient Tree Boosting的子模型都拥有较低的方差，整体模型的训练过程旨在降低偏差，故其需要较多的子模型（n_estimators默认值为100）且子模型为弱模型（max_depth的默认值为3），但是降低子模型间的相关度不能显著减少整体模型的方差

All three are so-called "meta-algorithms": approaches to combine several machine learning techniques into one predictive model in order to decrease the variance (bagging), bias (boosting) or improving the predictive force (stacking alias ensemble)

Every algorithm consists of two steps:
- Producing a distribution of simple ML models on subsets of the original data.
- Combining the distribution into one "aggregated" model.

Here is a short description of all three methods:
- Bagging (stands for Bootstrap Aggregation) is the way decrease the variance of your prediction by generating additional data for training from your original dataset using combinations with repetitions to produce multisets of the same cardinality/size as your original data. By increasing the size of your training set you can't improve the model predictive force, but just decrease the variance, narrowly tuning the prediction to expected outcome.
- Boosting is a two-step approach, where one first uses subsets of the original data to produce a series of averagely performing models and then "boosts" their performance by combining them together using a particular cost function (=majority vote). Unlike bagging, in the classical boosting the subset creation is not random and depends upon the performance of the previous models: every new subsets contains the elements that were (likely to be) misclassified by previous models.
- Stacking is a similar to boosting: you also apply several models to your original data. The difference here is, however, that you don't have just an empirical formula for your weight function, rather you introduce a meta-level and use another model/approach to estimate the input together with outputs of every model to estimate the weights or, in other words, to determine what models perform well and what badly given these input data.

Here is a comparison table:

![comparison table](/img/RFfqb.png)

## 7.boosting和bagging的差别：
 
bagging中的模型是强模型，偏差低，方差高。目标是降低方差。在bagging中，每个模型的bias和variance近似相同，但是互相相关性不太高，因此一般不能降低Bias，而一定程度上能降低variance。典型的bagging是random forest。
 
boosting中每个模型是弱模型，偏差高，方差低。目标是通过平均降低偏差。boosting的基本思想就是用贪心法最小化损失函数，显然能降低偏差，但是通常模型的相关性很强，因此不能显著降低variance,所以必须使用弱模型。典型的Boosting是adaboost，另外一个常用的并行Boosting算法是GBDT（gradient boosting decision tree）。这一类算法通常不容易出现过拟合。
 
过拟合的模型，通常variance比较大，这时应该用bagging对其进行修正。
欠拟合的模型，通常Bias比较大，这时应该可以用boosting进行修正。使用boosting时， 每一个模型可以简单一些。

## 8.结合策略
学习器结合可能会从三个方面带来好处：首先，从统计的方面来看，由于学习任务的假设空间往往很大，可能有多个假设在训练集上达到同等性能，此时若使用单学习器可能因误选而导致泛化性能不佳，结合多个学习器则会减少这一风险；第二，从计算的方面来看，学习算法往往会陷入局部极小，有的局部极小点所对应的泛化性能可能很糟糕，而通过多次运行之后进行结合，可降低陷入糟糕局部极小点的风险；第三，从表示的方面来看，某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，此时若使用单学习器则肯定无效，而通过结合多个学习器，由于相应的假设空间有所扩大，有可能学得更好的近似
- 平均法：包括简单平均、加权平均。
- 投票法：多数投票法、加权投票法
- Stacking先从初始数据集训练出初级学习器，然后“生成”一个新数据集用于训练次级学习器。在这个新数据集中，初级学习器的输出被当做样例输入特征，而初始样本的标记仍被当做样例标记

## 9.多样性
- 个体学习器准确性越高、多样性越大，则集成越好
- 多样性度量是用于度量集成中个体分类器的多样性。即估算个体学习器的多样化程度。典型做法是考虑个体分类器的两两相似/不相似性。
- 多样性增强，一般思路是在学习过程中引入随机性，常用做法主要是对数据样本、输入属性、输出表示、算法参数进行扰动。数据扰动通常是基于采样法。输入属性扰动从初始属性集中抽取出若干个属性子集，再基于每个属性子集训练一个基学习器。算法参数扰动，通过随机设置不同的参数，往往可产生差别较大的个体学习器，可以和交叉验证相结合
## 10.参考文献
- [机器学习](https://book.douban.com/subject/26708119/)
- [Data Mining](https://book.douban.com/subject/6533777/)
- [统计学习方法](https://book.douban.com/subject/10590856/)
