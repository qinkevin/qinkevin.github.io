---
layout: post
title: "文本挖掘"       # Title of the post
subtitle:
date:       2016-12-02 22:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 机器学习
---

# 文本挖掘
- 首先大家都有这样一种认知假设，即意思相近的词语，它在文本中出现的上下文也是相似的，也就是说，相似的词语拥有相似的语境。因此，我们可以利用一个词语的上下文，如一个词语与其它词语共同出现的次数，这样一个次数组成的向量，来表示这个词语。当然，如果句子特别长，我们可以限定窗口，只取该单词前后 n 个单词的词共现次数来表示这个单词
- 与之前一般的共现计数不同，word2vec 作为现在主流的词嵌入算法，主要是通过预测一个窗口长度为 c 的窗口内每个单词的周边单词概率，来作为这个单词的词向量。通过这种方式，把单词映射到一个高维向量空间，借此可以计算单词之间的距离，即计算语义相似性
- 在 word2vec 中使用最重要的两个模型分别是 CBOW 和 Skip-gram 模型，前者是利用词的上下文预测当前的单词，后者则是利用当前词来预测上下文
- CBOW 全称是 Continuous Bag-of-Words Model，即连续的词袋，因为它用连续空间来表示词，而且这些词的先后顺序并不重要
- 单词-文档矩阵是表达单词与文档之间所具有的一种包含关系的概念模型。“倒排索引”是实现单词到文档映射关系的最佳实现方式.通过倒排索引，可以根据单词快速获取包含这个单词的文档列表
- 在单词对应的倒排列表中不仅可以记录了文档编号，还可以记载单词频率信息（TF），即这个单词在某个文档中的出现次数，之所以要记录这个信息，是因为词频信息在搜索结果排序时，计算查询和文档相似度是很重要的一个计算因子，所以将其记录在倒排列表中，以方便后续排序时进行分值计算。
- 还可以记载每个单词对应的“文档频率信息”。“文档频率信息”代表了在文档集合中有多少个文档包含某个单词，之所以要记录这个信息，其原因与单词频率信息一样，这个信息在搜索结果排序计算中是非常重要的一个因子
- 单词词典是倒排索引中非常重要的组成部分，它用来维护文档集合中出现过的所有单词的相关信息，同时用来记载某个单词对应的倒排列表在倒排文件中的位置信息。在支持搜索时，根据用户的查询词，去单词词典里查询，就能够获得相应的倒排列表，并以此作为后续排序的基础。对于一个规模很大的文档集合来说，可能包含几十万甚至上百万的不同单词，能否快速定位某个单词，这直接影响搜索时的响应速度，所以需要高效的数据结构来对单词词典进行构建和查找，常用的数据结构包括哈希加链表结构和树形词典结构。
- 使用朴素贝叶斯进行文本分类会失去词语之间的顺序信息
- 常见的做法是将文本转换成『文档-词项矩阵』。矩阵中的元素，可以使用词频，或者TF-IDF值等

## 1.TF-IDF算法
- 词频(TF)表示某个词在文章中出现的次数，考虑到文章次数的不同进行标准化，所以TF=某个词在文章中出现的次数/文章的总词数
- 逆文档频率(IDF)的主要思想是如果某个词比较少见，但是它在这篇文章中多次出现，那么它很可能就反映了这篇文章的特性，正是我们所需要的关键词。
- IDF=log(语料库的文档总数/(包含该词的文档数+1)).如果一个词越常见，那么分母就越大，逆文档频率就越小越接近0。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）。log表示对得到的值取对数
- 知道了"词频"（TF）和"逆文档频率"（IDF）以后，将这两个值相乘，就得到了一个词的TF-IDF值。某个词对文章的重要性越高，它的TF-IDF值就越大。所以，排在最前面的几个词，就是这篇文章的关键词
- 可以看到，TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比


## 2.主题模型
- 传统判断两个文档相似性的方法是通过查看两个文档共同出现的单词的多少，如TF-IDF等，这种方法没有考虑到文字背后的语义关联，可能在两个文档共同出现的单词很少甚至没有，但两个文档是相似的.所以在判断文档相关性的时候需要考虑到文档的语义，而语义挖掘的利器是主题模型，LDA就是其中一种比较有效的模型
- 在主题模型中，主题表示一个概念、一个方面，表现为一系列相关的单词，是这些单词的条件概率。形象来说，主题就是一个桶，里面装了出现概率较高的单词，这些单词与这个主题有很强的相关性。
- 首先，可以用生成模型来看文档和主题这两件事。所谓生成模型，就是说，我们认为一篇文章的每个词都是通过“以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语”这样一个过程得到的.那么，如果我们要生成一篇文档，它里面的每个词语出现的概率为：
![主题模型公式](/img/主题模型.PNG)
这个概率公式可以用矩阵表示：
![主题模型矩阵](/img/主题模型矩阵表示.PNG)
- 其中”文档-词语”矩阵表示每个文档中每个单词的词频，即出现的概率；”主题-词语”矩阵表示每个主题中每个单词的出现概率；”文档-主题”矩阵表示每个文档中每个主题出现的概率。
- 给定一系列文档，通过对文档进行分词，计算各个文档中每个单词的词频就可以得到左边这边”文档-词语”矩阵。主题模型就是通过左边这个矩阵进行训练，学习出右边两个矩阵

主题模型有两种：pLSA（Probabilistic Latent Semantic Analysis）和LDA（Latent Dirichlet Allocation）

### 2.1 LDA模型

### 2.2 pLSA模型

- LSA潜在语义分析的目的，就是要找出词(terms)在文档和查询中真正的含义，也就是潜在语义，从而解决上节所描述的问题。具体说来就是对一个大型的文档集合使用一个合理的维度建模，并将词和文档都表示到该空间，比如有2000个文档，包含7000个索引词，LSA使用一个维度为100的向量空间将文档和词表示到该空间，进而在该空间进行信息检索。而将文档表示到此空间的过程就是SVD奇异值分解和降维的过程。降维是LSA分析中最重要的一步，通过降维，去除了文档中的“噪音”，也就是无关信息（比如词的误用或不相关的词偶尔出现在一起），语义结构逐渐呈现。相比传统向量空间，潜在语义空间的维度更小，语义关系更明确
- LSA的步骤如下：

>1. 分析文档集合，建立Term-Document矩阵。
>2. 对Term-Document矩阵进行奇异值分解。
>3. 对SVD分解后的矩阵进行降维，也就是奇异值分解一节所提到的低阶近似。
>4. 使用降维后的矩阵构建潜在语义空间，或重建Term-Document矩阵。



## 参考文献
- [用 Word2vec 轻松处理新金融风控场景中的文本类数据](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650720823&idx=4&sn=2ed3964e94e3076e060e48a4708faa2a&chksm=871b0e49b06c875fd2ead27692e8c2b35ac38724adc0215183a434283f4f869fde11e7036c5e#rd)
- [机器学习系列-word2vec篇](https://zhuanlan.zhihu.com/p/23733638)
- [搜索引擎-倒排索引基础知识](http://blog.csdn.net/hguisu/article/details/7962350)
- [TF-IDF与余弦相似性的应用（一）：自动提取关键词](http://www.ruanyifeng.com/blog/2013/03/tf-idf.html)
- [使用scikit-learn进行文本分类](http://yphuang.github.io/blog/2016/04/15/Working-with-Text-Data-using-sklearn/)
