---
layout:     post
title:      "认识数据"
subtitle:   
date:       2016-11-19 12:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 机器学习
---

# 认识数据

## 1.Data Objects and Attribute Types

- Because nominal attribute values do not have any meaningful order about them and are not quantitative, it makes no sense to find the mean (average) value or median (middle) value for such an attribute, given a set of objects. One thing that is of interest, however, is the attribute's most commonly occurring value. This value, known as the mode, is one of the measures of central tendency
- The central tendency of an ordinal attribute can be represented by its mode and its median (the middle value in an ordered sequence), but the mean cannot be defined

## 2.Basic Statistical Descriptions of Data

- To offset the effect caused by a small number of extreme values, we can instead use the trimmed mean, which is the mean obtained after chopping off values at the high and low extremes
- A quantile–quantile plot, or q-q plot, graphs the quantiles of one univariate distribution against the corresponding quantiles of another. It is a powerful visualization tool in that it allows the user to view whether there is a shift in going from one distribution to another
- 对于名义变量，使用卡方检验；对于数值属性，使用相关系数和协方差来进行相关分析
- 整理数据必须首先备份原始数据，方便回头检查
- 若手头数据非常混乱，就应该大胆地排序，尤其是在记录量很大的情况下，要一次性看清所有的数据往往很难，而按照不同的域对数据进行排序则能够以直观的方式为数据分组，从而发现重复现象或其他疑义
- 傅里叶变换能够识别时间序列数据中的基本频率
- 余弦相似度不考虑两个数据对象的量值。当量值是重要的时，欧几里得距离可能是一种更好的选择
- PCA能够更好地处理稀疏数据，而小波变换更适合高维数据
- 因子分析与主成分分析的区别：主成分分析侧重“变异量”，通过转换原始变量为新的组合变量使得数据的“变异量”最大，从而能把样本个体之间的差异最大化，但得出来的主成分往往从业务场景的角度难以解释；因子分析更重视相关变量的“共变异量”，组合的是相关性较强的原始变量，目的是找到在背后起作用的少量关键因子，因子分析的结果往往更容易用业务知识去加以解释
- 数据归约策略包括维归约、数量归约和数据压缩。维归约减少所考虑的随机变量或属性的个数。数量归约用替代的、较小的数据表示形式替换原数据。数据压缩使用变换，以便得到原数据的归约或“压缩”表示。如果原数据能够从压缩后的数据重构，而不损失信息，则该数据归约称为无所的。如果我们只能近似重构原数据，则该数据归约称为有损的。
- 解释变量之间的关联时要特别小心，因为观察到的联系可能受其他混淆因素的影响，这些因素，即没有包括在分析中的隐藏变量。在某些情况下，隐藏的变量可能会导致观察到的一对变量之间的联系小时或逆转方向，这种现象就是所谓的辛普森悖论
- 奇异值分解(SVD)的重要性是每个矩阵都可以表示成秩为1矩阵的以奇异值为权重的加权和
- 卡方分布主要用于检查实际效果与期望结果之间何时存在显著差别。卡方分布有两个主要用途：第一是用于检查拟合优度，也就是可以检验一组给定的数据与指定分布的吻合程度。另一个用途是检验两个变量的独立性，通过这个方法可以检查变量之间是否存在某种关联
- 雅克比指数表示两个集合的相似性，它是两个集合中共同的项数除以并集中不同项总数的结果。如果两个集合的交集和并集相等，比值为1.如果两个集合完全不同，那么比值为0.其余就是中间值了
- 从简单处着手永远是个好办法，建模时在简单和准确之间有一个权衡
- 探索性数据分析的基本工具是图、表和汇总统计量。一般来说，探索性数据分析是一种系统性分析数据的方法，它展示了所有变量的分布情况（利用箱线图）、时间序列数据和变换变量，利用散点矩阵图展示了变量两两之间的关系，并且得到了所有的汇总统计量。换句话说，就是要计算均值、最小值、最大值、上下四分位数和确定异常值。在探索性数据分析中图形只是帮助你理解数据，与可视化不同
- 在特征选择中，被过滤掉的两个“不重要”变量单独来看可能确实都不重要的，但是放在一起可能会非常重要。变量之间的相互作用往往会将某些不重要的变量联系并组合成一个比较重要的特征
- 当某个变量是分类型变量并包含较多类别时，其在模型中的应用要引起分析人员的额外关注。比如说“邮政编码”这样的分类变量可能有成百上千个不同的类别，但是在建模过程中，这样多累的分类变量基本是无用的。需要通过组合的方式加以简化。
- 因果关系模型并不是一套完全不同于预测模型的统计方法。恰好相反，它其实是根植于传统预测模型的框架内的。但是，你的思路和目标就不再是优化模型以提高预测的准确性了，而是尽力分离出变量之间的因果关系。
- 当一个变量同时影响到“实验”本身，以及“实验”的结果时，它就是一个干扰因子
- 干扰因子的选择对实验结果有着直接影响，选择不同的干扰因子，得出的结果可能有天壤之别
- 确立因果关系的黄金法则是使用随机化实验，去除掉干扰因子的影响
- 这种分组因素对因果关系推断的方向性影响称作辛普森悖论
- 应当牢记，当使用基于距离的算法时，我们必须尝试将数据缩放，这样较不重要的特征不会因为自身较大的范围而主导目标函数。此外，具有不同度量单位的特征也应该进行缩放，这样给每个特征具有相同的初始权重，最终我们会得到更好的预测模型。
- 特征缩放和标准化中二选一是个令人困惑的选择，你必须对数据和要使用的学习模型有更深入的理解，才能做出决定。对于初学者，你可以两种方法都尝试下并通过交叉验证精度来做出选择
- 比较常用的图表有：
* 查看目标变量的分布。当分布不平衡时，根据评分标准和具体模型的使用不同，可能会严重影响性能。
* 对 Numerical Variable，可以用 Box Plot 来直观地查看它的分布。
* 对于坐标类数据，可以用 Scatter Plot 来查看它们的分布趋势和是否有离群点的存在。
* 对于分类问题，将数据根据 Label 的不同着不同的颜色绘制出来，这对 Feature 的构造很有帮助。
* 绘制变量之间两两的分布和相关度图表。
