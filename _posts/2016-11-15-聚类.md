---
layout: post
title: "聚类"       # Title of the post
subtitle:  
date:       2016-11-19 12:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 机器学习
---

# 聚类

>聚类过程仅能自动形成簇结构，簇所对应的概念语义需由使用者来把握和命名
>软聚类中边界点可以属于多个组

## 1.原型聚类

### 1.1 k-means
First, it randomly selects k of the objects in D, each of which initially represents a cluster mean or center. For each of the remaining objects, an object is assigned to the cluster to which it is the most similar, based on the Euclidean distance between the object and the cluster mean. The k-means algorithm then iteratively improves the within-cluster variation. For each cluster, it computes the new mean using the objects assigned to the cluster in the previous iteration. All the objects are then reassigned using the updated means as the new cluster centers. The iterations continue until the assignment is stable, that is, the clusters formed in the current round are the same as those formed in the previous round

> - 对于名义变量，可以使用众数代替平均值
> - 由于使用平均值，所以易受到噪声和异常值的影响
> - 可能会到达局部最优，所以可以多次初始化尝试
> - 做聚类不要随便标准化，因为可能量纲是很重要的
> - k-means最大的缺点在于可解释性，但是逻辑简单，运行速度快

### 1.2 k-medoids
由于k-means算法中心点的选取易收到异常值的影响，所以k-medoids限制中心点必须是真实点，而不是计算出来的位置，然后计算其他点与该点的距离最小

### 1.3 k-means 与 k-medoids 比较
The k-medoids method is more robust than k-means in the presence of noise and outliers because a medoid is less influenced by outliers or other extreme values than a mean. However, the complexity of each iteration in the k-medoids algorithm is O(k(n − k)2). For large values of n and k, such computation becomes very costly, and much more costly than the k-means method. Both methods require the user to specify k, the number of clusters.

### 1.4 轮廓系数
轮廓系数（Silhouette Coefficient）结合了聚类的凝聚度（Cohesion）和分离度（Separation），用于评估聚类的效果。该值处于-1~1之间，值越大，表示聚类效果越好。具体计算方法如下：

>1.对于第i个元素x_i，计算x_i与其同一个簇内的所有其他元素距离的平均值，记作a_i，用于量化簇内的凝聚度。
>2.选取x_i外的一个簇b，计算x_i与b中所有点的平均距离，遍历所有其他簇，找到最近的这个平均距离,记作b_i，用于量化簇之间分离度。
>3.对于元素x_i，轮廓系数s_i = (b_i – a_i)/max(a_i,b_i)
>4.计算所有x的轮廓系数，求出平均值即为当前聚类的整体轮廓系数

从上面的公式，不难发现若s_i小于0，说明x_i与其簇内元素的平均距离小于最近的其他簇，表示聚类效果不好。如果a_i趋于0，或者b_i足够大，那么s_i趋近与1，说明聚类效果比较好。

### 1.5 K值选取
在实际应用中，由于Kmean一般作为数据预处理，或者用于辅助分类贴标签。所以k一般不会设置很大。可以通过枚举，令k从2到一个固定值如10，在每个k值上重复运行数次kmeans(避免局部最优解)，并计算当前k的平均轮廓系数，最后选取轮廓系数最大的值对应的k作为最终的集群数目。

### 1.6 kmeans最佳实践

>1.随机选取训练数据中的k个点作为起始点
>2.当k值选定后，随机计算n次，取得到最小开销函数值的k作为最终聚类结果，避免随机引起的局部最优解
>3.手肘法选取k值：绘制出k--开销函数闪点图(横轴是簇数，纵轴是簇内误差)，看到有明显拐点（如下）的地方，设为k值，可以结合轮廓系数。
>4.k值有时候需要根据应用场景选取，而不能完全的依据评估参数选取。

## 2.层次聚类
层次聚类试图在不同层次对数据集进行划分，从而形成树形的聚类结构

### 2.1 Agglomerative Hierarchical Clustering
是一种自底向上聚合策略的层次聚类算法。它先将数据集中的每个样本看做一个初始聚类簇，然后在算法运行的每一步中找出距离最近的两个聚类簇进行合并，该过程不断重复，直至达到预设的聚类簇个数

>也可以自顶向下分拆，但是由于分拆的复杂度太高，一般用自底向上聚合策略

### 2.2 BIRCH
BIRCH是一种聚类算法，它最大的特点是能利用有限的内存资源完成对大数据集的高质量的聚类，同时通过单遍扫描数据集最小化I/O代价
具体可参考[博客](http://blog.csdn.net/qll125596718/article/details/6895291)

## 3.密度聚类
此类算法假定聚类结构能通过样本分布的紧密程度确定。通常情形下，密度聚类算法从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇以获得最终的聚类结果

### 3.1 DBSCAN
Initially, all objects in a given data set D are marked as “unvisited.” DBSCAN randomly selects an unvisited object p, marks p as “visited,” and checks whether the ε-neighborhood of p contains at least MinPts objects. If not, p is marked as a noise point. Otherwise, a new cluster C is created for p, and all the objects in the ε-neighborhood of p are added to a candidate set, N. DBSCAN iteratively adds to C those objects in N that do not belong to any cluster. In this process, for an object p′ in N that carries the label “unvisited,” DBSCAN marks it as “visited” and checks its ε-neighborhood. If the ε-neighborhood of p′ has at least MinPts objects, those objects in the ε-neighborhood of p′ are added to N. DBSCAN continues adding objects to C until C can no longer be expanded, that is, N is empty. At this time, cluster C is completed, and thus is output.
To find the next cluster, DBSCAN randomly selects an unvisited object from the remaining ones. The clustering process continues until all objects are visited

## 4.性能度量
希望聚类结果的“簇内相似度”高且“簇间相似度”低。
聚类性能度量大致有两类。一类是将聚类结果与某个“参考模型”进行比较，称为“外部指标”；另一类是直接考察聚类结果而不利用任何参考模型，称为“内部指标”

## 5.参考文献
- [机器学习](https://book.douban.com/subject/26708119/)
- [Data Mining](https://book.douban.com/subject/6533777/)
