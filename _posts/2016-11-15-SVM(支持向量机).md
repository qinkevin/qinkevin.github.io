---
layout: post
title: "SVM（支持向量机）"       # Title of the post
subtitle:  
date:       2016-11-19 12:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 机器学习
---

# SVM(支持向量机)

当训练数据线性可分时，通过硬间隔最大化，学习一个线性的分类器，即线性可分支持向量机，又称为硬间隔支持向量机；当训练数据近似线性可分时，通过软间隔最大化，也学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机；当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机

> Binary classification schemes, such as support vector machines, can be adapted to handle multiclass classification. This involves constructing an ensemble of binary classifiers. Error-correcting codes can be used to increase the accuracy of the ensemble

## 1.线性可分支持向量机与硬间隔最大化
一般地，当训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开。感知机利用误分类最小的策略，求得分离超平面，不过这时的解有无穷多个。线性可分支持向量机利用间隔最大化求最优分离超平面，这时解是唯一的。
- **函数间隔**：$$ y(w.x+b) $$
- **几何间隔**：对函数间隔进行规范化，在函数间隔上除以||w||
- 如果超平面参数w和b成比例地改变（超平面没有改变），函数间隔也按此比例改变，而几何间隔不变
- 支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面
- 间隔最大化的直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类。也就是说，不仅将正负实例点分开，而且对最难分的实例点（离超平面最近的点）也有足够大的确信度将它们分开。这样的超平面应该对未知的新实例有很好的分类预测能力
- 在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为**支持向量**
- **间隔（margin）**即支持向量所在的两条平行于分离超平面之间的距离，等于$$ \dfrac{2}{||w||}$$
> 在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用，而其他实例点并不起作用。如果移动支持向量将改变所求的解；但是如果在间隔边界以外移动其他实例点，甚至去掉这些点，则解是不会改变的。由于支持向量在确定分离超平面中起着决定性作用，所以将这种分类模型称为支持向量机。所以此种分类器的复杂性是由支持向量的数量而非数据的数量决定，所以支持向量机并不容易过拟合。
- 使用对偶算法的优点：一是对偶问题往往更容易求解；二是自然引入核函数，进而推广到非线性分类问题
- 对偶算法的分类决策函数只依赖于输入x和训练样本输入的内积

## 2.线性支持向量机与软间隔最大化
- 通常情况下，样本点并非完全线性可分，可能存在一些异常点。如果坚持要硬间隔的话可能会导致噪声过拟合
- 线性不可分意味着某些样本点不能满足函数间隔大于等于1的约束条件
- 通过增加**松弛变量**解决，目标函数增加了松弛变量的惩罚项，约束条件相应改变
- 软间隔支持向量或者在间隔边界上，或者在间隔边界与分类超平面之间，或者在分离超平面误分一侧
- hinge损失函数：max(0,1-z)
- 等价于最小化hinge损失函数与w的正则项

## 3.非线性支持向量机与核函数
- 用线性分类方法求解非线性分类问题分为两步：首先使用一个变换将原空间的数据映射到新空间；然后在新空间里用线性分类学习方法从训练数据中学习分类模型
- 核技巧的想法是在学习与预测中只定义核函数，而不显式地定义映射函数
- 常用的核函数有线性核、多项式核函数、高斯核函数（RBF核）、Sigmoid核
- 非线性 SVM 意味着算法计算的边界不再是直线。它的优点是可以捕获数据之间更复杂的关系，而无需人为地进行困难的数据转换；缺点是训练时间长得多，因为它的计算量更大。
- 在SVM的对偶问题中，存在内积项，在特征空间的内积等于在原始空间通过核函数计算的结果，这就是核函数的作用
- 只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用，即正定核函数
- 在核函数K(x, z)给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机。学习是隐式地在特征空间进行的，不需要显式地定义特征空间和映射函数。这样的技巧称为核技巧，它是巧妙地利用线性分类学习方法与核函数解决非线性问题的技术。在实际应用中，往往依赖领域知识直接选择核函数，核函数选择的有效性需要通过实验验证
- 对于高斯核函数可以调节sigma，sigma决定了多快趋向于0，测量了两个向量之间的距离，采用的是欧氏距离。large sigma: higher bias, lower variance.small sigma: lower bias, higher variance.所以还是可能陷入过拟合，尤其是当sigma过小时。映射到无限多维的空间，缺点是难解释，比较慢，可能过拟合
- 多项式核函数当P较小的时使用，过大会计算很复杂，一般先使用线性核函数

## 4.SMO算法
- 支持向量机的学习问题可以形式化为求解凸二次规划问题，这样的凸二次规划问题具有全局最优解。但是当训练样本容量很大时，这些算法往往变得很低效，以至于无法使用。因为SVM的目标函数中变量为拉格朗日乘子，一个变量对应一个样本点，因此变量的总数等于变量训练容量。
- SMO算法是一种启发式算法，其基本思路是：如果所有变量的解都满足此最优化问题的KKT条件，那么这个最优化问题的解就得到了。因为KKT条件是该最优化问题的充分必要条件。否则，选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题。这个二次规划问题关于这两个变量的解应该更接近原始二次规划问题的解，因为这会使得原始二次规划问题的目标函数值变得更小。重要的是，这时子问题可以通过解析方法求解，这样就可以大大提高整个算法的计算速度。子问题有两个变量，一个是违反KKT条件最严重的那一个，另一个由约束条件自动确定。如此，SMO算法将原问题不断分解为子问题并对子问题求解，进而达到求解原问题的目的。子问题的两个变量中只有一个是自由变量。
- 整个SMO算法包括两个部分；求解两个变量二次规划的解析方法和选择变量的启发式方法。
- SMO算法在每个子问题中选择两个变量优化，其中至少一个变量是违反KKT条件的。SMO称选择第1个变量的过程为外层循环。外层循环在训练样本中选取违反KKT条件最严重的样本点，并将其对应的变量作为第1个变量。SMO称选择第2个变量的过程为内层循环。假设在外层循环中已经找到第1个变量，现在要在内层循环中找第2个变量。第2个变量选择的标准是希望能使它又足够大的变化

## 5.SVR
- 对于样本(x,y),传统回归模型通常直接基于模型输出f(x)与真实输出y之间的差别来计算损失，当且仅当f(x)与y完全相同时，损失才为零。与此不同，支持向量回归假设我们能容忍f(x)与y之间最多有一定的偏差，即仅当f(x)与y之间的差别绝对值大于一定值时才计算损失。

## 6.优缺点
优点：
- 使用核函数可以向高维空间进行映射
- 使用核函数可以解决非线性的分类
- 分类思想很简单，就是将样本与决策面的间隔最大化
- 分类效果较好

缺点：
- 对大规模数据训练比较困难，因为它是用二次规划来求解的,效率太差了
- 无法直接支持多分类，但是可以使用间接的方法来做

## 6.参考文献
- [机器学习](https://book.douban.com/subject/26708119/)
- [Data Mining](https://book.douban.com/subject/6533777/)
- [统计学习方法](https://book.douban.com/subject/10590856/)
- [理解SVM的三层境界](http://blog.csdn.net/v_july_v/article/details/7624837)
