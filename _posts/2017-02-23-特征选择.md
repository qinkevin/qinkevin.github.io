---
layout: post
title: "特征选择"       # Title of the post
subtitle:
date:       2017-02-23 15:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 机器学习
---

特征选择的目标是寻找最优特征子集。特征选择能剔除不相关(irrelevant)或冗余(redundant )的特征，从而达到减少特征个数，提高模型精确度，减少运行时间的目的。另一方面，选取出真正相关的特征简化模型，协助理解数据产生的过程。
特征选择-产生过程和生成特征子集方法
完全搜索(Complete)
- 广度优先搜索( Breadth First Search )：广度优先遍历特征子空间。枚举所有组合，穷举搜索，实用性不高。
- 分支限界搜索( Branch and Bound )：穷举基础上加入分支限界。例如：剪掉某些不可能搜索出比当前最优解更优的分支。
- 其他，如定向搜索 (Beam Search )，最优优先搜索 ( Best First Search )等
启发式搜索(Heuristic)
- 序列前向选择( SFS ， Sequential Forward Selection )：从空集开始，每次加入一个选最优。
- 序列后向选择( SBS ， Sequential Backward Selection )：从全集开始，每次减少一个选最优。
- 增L去R选择算法 ( LRS ， Plus-L Minus-R Selection )：从空集开始，每次加入L个，减去R个，选最优（L>R)或者从全集开始，每次减去R个，增加L个，选最优(L<R)。
- 其他如双向搜索( BDS ， Bidirectional Search )，序列浮动选择( Sequential Floating Selection )等
随机搜索(Random)
- 随机产生序列选择算法(RGSS， Random Generation plus Sequential Selection)：随机产生一个特征子集，然后在该子集上执行SFS与SBS算法。
- 模拟退火算法( SA， Simulated Annealing )：以一定的概率来接受一个比当前解要差的解，而且这个概率随着时间推移逐渐降低
- 遗传算法( GA， Genetic Algorithms )：通过交叉、突变等操作繁殖出下一代特征子集，并且评分越高的特征子集被选中参加繁殖的概率越高。

特征选择是指选择获得相应模型和算法最好性能的特征集
- 1.计算每一个特征与响应变量的相关性：工程上常用的手段有计算皮尔逊系数和互信息系数
- 2.构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征
- 3.通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验
- 4.训练能够对特征打分的预选模型：RandomForest和Logistic Regression等都能对模型的特征打分，通过打分获得相关性后再训练最终模型；
- 5.通过特征组合后再来选择特征：如对用户id和用户特征最组合来获得较大的特征集再来选择特征，这种做法在推荐系统和广告系统中比较常见，这也是所谓亿级甚至十亿级特征的主要来源，原因是用户数据比较稀疏，组合特征能够同时兼顾全局模型和个性化模型
- 6.基于树的模型会使用基尼不纯度等进行选择（其中randomForest以及xgboost里的方法可以判断features的Importance）
- 7.PCA等方法可以生成指定数量的新features
- 8.擅对features进行visualization或correlation的分析

当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：

- 特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。
- 特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。
