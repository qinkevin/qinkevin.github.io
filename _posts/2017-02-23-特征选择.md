---
layout: post
title: "特征选择"       # Title of the post
subtitle:
date:       2017-02-23 15:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 机器学习
---

特征选择的目标是寻找最优特征子集。特征选择能剔除不相关(irrelevant)或冗余(redundant )的特征，从而达到减少特征个数，提高模型精确度，减少运行时间的目的。另一方面，选取出真正相关的特征简化模型，协助理解数据产生的过程。

1. 特征选择-产生过程和生成特征子集方法

完全搜索(Complete)
- 广度优先搜索( Breadth First Search )：广度优先遍历特征子空间。枚举所有组合，穷举搜索，实用性不高。
- 分支限界搜索( Branch and Bound )：穷举基础上加入分支限界。例如：剪掉某些不可能搜索出比当前最优解更优的分支。
- 其他，如定向搜索 (Beam Search )，最优优先搜索 ( Best First Search )等

启发式搜索(Heuristic)
- 序列前向选择( SFS ， Sequential Forward Selection )：从空集开始，每次加入一个选最优。
- 序列后向选择( SBS ， Sequential Backward Selection )：从全集开始，每次减少一个选最优。
- 增L去R选择算法 ( LRS ， Plus-L Minus-R Selection )：从空集开始，每次加入L个，减去R个，选最优（L>R)或者从全集开始，每次减去R个，增加L个，选最优(L<R)。
- 其他如双向搜索( BDS ， Bidirectional Search )，序列浮动选择( Sequential Floating Selection )等

随机搜索(Random)
- 随机产生序列选择算法(RGSS， Random Generation plus Sequential Selection)：随机产生一个特征子集，然后在该子集上执行SFS与SBS算法。
- 模拟退火算法( SA， Simulated Annealing )：以一定的概率来接受一个比当前解要差的解，而且这个概率随着时间推移逐渐降低
- 遗传算法( GA， Genetic Algorithms )：通过交叉、突变等操作繁殖出下一代特征子集，并且评分越高的特征子集被选中参加繁殖的概率越高。

2. 特征选择是指选择获得相应模型和算法最好性能的特征集
- 计算每一个特征与响应变量的相关性：工程上常用的手段有计算皮尔逊系数和互信息系数
- 构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征
- 通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验
- 训练能够对特征打分的预选模型：RandomForest和Logistic Regression等都能对模型的特征打分，通过打分获得相关性后再训练最终模型；
- 通过特征组合后再来选择特征：如对用户id和用户特征最组合来获得较大的特征集再来选择特征，这种做法在推荐系统和广告系统中比较常见，这也是所谓亿级甚至十亿级特征的主要来源，原因是用户数据比较稀疏，组合特征能够同时兼顾全局模型和个性化模型
- 基于树的模型会使用基尼不纯度等进行选择
- PCA等方法可以生成指定数量的新features
- 传统用前进或者后退法的逐步回归来筛选特征或者对特征重要性排序，对于特征数量不多的情况还是适用的。
- 方差选择法计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征
- 卡方检验 经典的卡方检验是检验定性自变量对定性因变量的相关性
- 互信息法 互信息法经典的互信息也是评价定性自变量对定性因变量的相关性的

3. 当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：

- 特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。
- 特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。

4. 常见的特征选择方法大致可分为三类：过滤式（filter）、 包裹式(wrapper)和嵌入式(embedding)
- 过滤式方法先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关。这相当于先用特征选择过程对初始特征进行“过滤”，再用过滤后的特征来训练模型
- 一般而言，由于包裹式特征选择方法直接针对给定学习器进行优化，因此从最终学习器性能来看，包裹式特征选择比过滤式特征选择更好，但另一方面，由于在特征选择过程中需多次训练学习器，因此包裹式特征选择的计算开销通常比过滤式特征选择大得多
- 嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，即在学习器训练过程中自动地进行了特征选择。如L1正则化
