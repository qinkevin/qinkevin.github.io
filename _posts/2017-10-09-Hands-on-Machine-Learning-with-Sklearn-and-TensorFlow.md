---
layout:     post
title:      "Hands-on-Machine-Learning-with-Sklearn-and-TensorFlow"
subtitle:
date:       2017-10-09 11:00:00
author:     "随机漫步的傻瓜"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    -  机器学习
---
- cross-validation allows you to get not only an estimate of the performance of your model, but also a measure of how precise this estimate is(ie.,its standard deviation)
- you should save every model you experiment with, so you can come back easily to any model you want.Make sure you save both the hyperparameter and the trained parameter, as well as the cross-validation scores and perhaps the actual predictions as well.This will allow you to easily compare scores across model types, and compare the types of errors they make.You can easily save Scikit-Learn models by using Python's pickle module,or using sklearn.externals.joblib, which is more efficient at serializing large NumPy arrays
- When using Gradient Descent, you should ensure that all features have a similar scale(e.g.,using Scikit-Learn's StandardScaler class),or else it will take much longer to converge
- Therefore randomness is good to escape from local optima, but bad because it means that the algorithm can never settle at the minimum.One solution to this dilemma is to gradually reduce the learning rate.The steps start out large(which helps make quick progress and escape local minima),then get smaller and smaller, allowing the algorithm to settle at the global minimum.This process is called simulated annealing, because it resembles the process of annealing in metallurgy where molten metal is slowly cooled down.The function that determines the learning rate at each iteration is called the learning schedule.If the learning rate is reduced too quickly, you may get stuck in a local minimum, or even end up frozen halfway to the minimum.If the learning rate is reduced too slowly, you may jump around the minimum for a long time and end up with a suboptimal solution if you halt training too early
- It is important to scale the data(e.g.,using a StandardScaler) before performing Ridge Regression,as it is sensitive to the scale of the input features.This is true of most regularized models
- SVMs are particularly well suited for classification of complex but small-or-medium-sized datasets
- SVMs are sensitive to the feature scales
- A common approach to find the right hyperparameter values is to use grid search.It is often faster to first do a very coarse grid search ,then a finer grid search around the best values found.Having a good sense of what each hyperparameter actually does can also help you search in the right part of the hyperparameter space
- One of the many qualities of Decision Trees is that they require very little data preparation.In particular,they don't require feature scaling or centering at all
- More generally,the main issue with Decision Trees is that they are very sensitive to small variations in the training data
- Extra-Trees: When you are growing a tree in a Random Forest, at each node only a random subset of the features is considered for splitting (as discussed earlier). It is possible to make trees even more random by also using random thresholds for each feature rather than searching for the best possible thresholds (like regular Decision Trees do)
- In order to find the optimal number of trees, you can use early stopping. A simple way to implement this is to use the staged_predict() method: it returns an iterator over the predictions made by the ensemble at each stage of train‐ ing (with one tree, two trees, etc.)
- PCA assumes that the dataset is centered around the origin. As we will see, Scikit-Learn’s PCA classes take care of centering the data for you. However, if you implement PCA yourself (as in the pre‐ ceding example), or if you use other libraries, don’t forget to center the data first
- As kPCA is an unsupervised learning algorithm, there is no obvious performance measure to help you select the best kernel and hyperparameter values. However, dimensionality reduction is often a preparation step for a supervised learning task (e.g., classification), so you can simply use grid search to select the kernel and hyper‐ parameters that lead to the best performance on that task.
- t-Distributed Stochastic Neighbor Embedding (t-SNE) reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used for visualization, in particular to visualize clusters of instances in high-dimensional space
- Linear Discriminant Analysis (LDA) is actually a classification algorithm, but during training it learns the most discriminative axes between the classes, and these axes can then be used to define a hyperplane onto which to project the data. The benefit is that the projection will keep classes as far apart as possible, so LDA is a good technique to reduce dimensionality before running another classification algorithm such as an SVM classifier.
- In Jupyter (or in a Python shell), it is common to run the same commands more than once while you are experimenting. As a result, you may end up with a default graph containing many duplicate nodes. One solution is to restart the Jupyter kernel (or the Python shell), but a more convenient solution is to just reset the default graph by running tf.reset_default_graph().
- placeholder nodes are special because they don’t actually perform any computation, they just output the data you tell them to output at runtime. They are typically used to pass the training data to TensorFlow during training
- tanh 函数：Just like the logistic function it is S-shaped, continuous, and differentiable, but its output value ranges from –1 to 1 (instead of 0 to 1 in the case of the logistic function), which tends to make each layer’s output more or less normalized (i.e., centered around 0) at the beginning of training. This often helps speed up convergence.
- It is important to initialize connection weights randomly for all hidden layers to avoid any symmetries that the Gradient Descent algorithm would be unable to break
- For many problems, you can just begin with a single hidden layer and you will get reasonable results. It has actually been shown that an MLP with just one hidden layer can model even the most complex functions provided it has enough neurons. For a long time, these facts convinced researchers that there was no need to investigate any deeper neural networks. But they overlooked the fact that deep networks have a much higher parameter efficiency than shallow ones: they can model complex functions using exponentially fewer neurons than shallow nets, making them much faster to train.
- In summary, for many problems you can start with just one or two hidden layers and it will work just fine. For more complex problems, you can gradually ramp up the number of hidden layers, until you start overfitting the training set. Very complex tasks, such as large image classification or speech recognition, typically require networks with dozens of layers (or even hundreds, but not fully connected ones, as we will see in Chapter 13), and they need a huge amount of training data. However, you will rarely have to train such networks from scratch: it is much more common to reuse parts of a pretrained state-of-the-art network that performs a similar task. Training will be a lot faster and require much less data
- For the output layer, the softmax activation function is generally a good choice for classification tasks (when the classes are mutually exclusive). For regression tasks, you can simply use no activation function at all.
- In most cases you can use the ReLU activation function in the hidden layers
- The one drawback of Momentum optimization is that it adds yet another hyperparameter to tune. However, the momentum value of 0.9 usually works well in practice and almost always goes faster than Gradient Descent.
- Dropout:at every training step, every neuron (including the input neurons but excluding the output neurons) has a probability p of being tem‐ porarily “dropped out,” meaning it will be entirely ignored during this training step, but it may be active during the next step
- If you observe that the model is overfitting, you can increase the dropout rate (i.e., reduce the keep_prob hyperparameter). Conversely, you should try decreasing the dropout rate (i.e., increasing keep_prob) if the model underfits the training set. It can also help to increase the dropout rate for large layers, and reduce it for small ones.
- One last regularization technique, data augmentation, consists of generating new training instances from existing ones, artificially boosting the size of the training set. This will reduce overfitting, making this a regularization technique
